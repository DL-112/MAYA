{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6d0560-411e-4802-8ec3-77586ad5ef65",
      "metadata": {
        "id": "8e6d0560-411e-4802-8ec3-77586ad5ef65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db12a1c0-76a9-4d3f-8e9a-1ab81e39f3ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"GPU is not available\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDhKDxcHucCt",
        "outputId": "90561f05-6211-487f-9846-ad889be8ad52"
      },
      "id": "FDhKDxcHucCt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "text_data = \"\"\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Books/ZLN(2)11.2.25'\n",
        "\n",
        "def visit_files_in_directory(root_dir):\n",
        "    global text_data\n",
        "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            filepath = os.path.join(dirpath, filename)\n",
        "            try:\n",
        "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                    text_data += f.read()\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file {filepath}: {e}\")\n",
        "\n",
        "# Call the function to visit all files\n",
        "visit_files_in_directory(folder_path)\n",
        "\n",
        "# Print the results\n",
        "print(\"Total number of characters:\", len(text_data))\n",
        "print(text_data[:99])  # Print first 99 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLxX4XZwuWrY",
        "outputId": "19a91f1c-c864-41cf-ca75-744ca2d375d1"
      },
      "id": "xLxX4XZwuWrY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 7459593\n",
            "﻿\n",
            " \n",
            "ပထမံ ဆရာသိန်း\n",
            "ရွှေဥဒေါင်း\n",
            "ပုံနှိပ်မှတ်တမ်း\n",
            " \n",
            "စာမူခွင့်ပြုချက်အမှတ် [၃၇၇ / ၉၆(၇)]\n",
            "မျက်နှာဖုံးခွင\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a137143-45b9-45e0-b8a6-098f9c5e33dd",
      "metadata": {
        "id": "3a137143-45b9-45e0-b8a6-098f9c5e33dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e612362-eb50-4eae-bb32-dbccd4ee1af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ကြည့် ပါ\n"
          ]
        }
      ],
      "source": [
        "def create_break_pattern():\n",
        "    \"\"\"Creates and returns the regular expression pattern for Myanmar syllable breaking.\"\"\"\n",
        "    my_consonant = r\"က-အ၎\"\n",
        "    en_char = r\"a-zA-Z0-9\"\n",
        "    other_char = r\"ဣဤဥဦဧဩဪဿ၌၍၏၀-၉၊။!-/:-@[-`{-~\\s“”—‘’…><\"\n",
        "    subscript_symbol = r'္'\n",
        "    a_that = r'်'\n",
        "\n",
        "    # Regular expression pattern for Myanmar syllable breaking\n",
        "    return re.compile(\n",
        "        r\"((?<!\" + subscript_symbol + r\")[\" + my_consonant + r\"]\"\n",
        "        r\"(?![\"\n",
        "        + a_that + subscript_symbol + r\"])\"\n",
        "        + r\"|[\" + en_char + other_char + r\"])\"\n",
        "    )\n",
        "\n",
        "def process_string(input_string):\n",
        "    # Function to check if the character is valid (contains '်', '့' and exactly one consonant)\n",
        "    def is_valid_char(char):\n",
        "        return '်' in char and '့' in char and len(char) == 3\n",
        "\n",
        "    # Function to rearrange the Unicode order of specific marks\n",
        "    def rearrange_unicode_order(word):\n",
        "        splited = [c for c in word]\n",
        "        splited[1], splited[2] = splited[2], splited[1]\n",
        "        return (\"\").join(splited)\n",
        "\n",
        "    # Process the string\n",
        "    updated_string = []\n",
        "    for word in input_string.split():\n",
        "        if is_valid_char(word) and updated_string:\n",
        "            updated_string[-1] += rearrange_unicode_order(word)  # Merge with the previous word\n",
        "        else:\n",
        "            updated_string.append(word)  # Otherwise, append as a new word\n",
        "\n",
        "    return ' '.join(updated_string)\n",
        "\n",
        "# main function to use sylbreak.py\n",
        "def break_syllables(line):\n",
        "    \"\"\"Applies syllable breaking rules to a line.\"\"\"\n",
        "    break_pattern = create_break_pattern()\n",
        "    separator = ' '\n",
        "    line = re.sub(r'\\s+', ' ', line.strip())\n",
        "    segmented_line = break_pattern.sub(separator + r\"\\1\", line)\n",
        "\n",
        "    # Remove the leading delimiter if it exists\n",
        "    if segmented_line.startswith(separator):\n",
        "        segmented_line = segmented_line[len(separator):]\n",
        "\n",
        "    # Replace delimiter+space+delimiter with a single space\n",
        "    double_delimiter = separator + \" \" + separator\n",
        "    segmented_line = segmented_line.replace(double_delimiter, \" \")\n",
        "\n",
        "    segmented_line = process_string(segmented_line)\n",
        "\n",
        "    return segmented_line\n",
        "\n",
        "\n",
        "\n",
        "test_text = \"ကြည့်ပါ\"\n",
        "tokenized_text = break_syllables(test_text)\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59947efd-9a14-44e0-a300-fef8f1e6d699",
      "metadata": {
        "id": "59947efd-9a14-44e0-a300-fef8f1e6d699",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4ae55d-38ba-4f3d-841e-511479f5d1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3631\n"
          ]
        }
      ],
      "source": [
        "def load_vocab_from_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        vocab = json.load(f)\n",
        "    return vocab\n",
        "\n",
        "vocab_dict = load_vocab_from_json(\"/content/drive/MyDrive/vocabs.json\")\n",
        "print(len(vocab_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce07ce46-3e78-4c54-9128-201c98c06ccd",
      "metadata": {
        "id": "ce07ce46-3e78-4c54-9128-201c98c06ccd"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG = {\n",
        "    \"vocab_size\": 3631,   # Vocabulary size\n",
        "    \"context_length\": 128, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 512,        # Embedding dimension\n",
        "    \"n_heads\": 8,         # Number of attention heads\n",
        "    \"n_layers\": 8,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21d0ab63-2849-4e76-b3ea-a6cee7eda0fe",
      "metadata": {
        "id": "21d0ab63-2849-4e76-b3ea-a6cee7eda0fe"
      },
      "outputs": [],
      "source": [
        "def text_to_token_ids(text):\n",
        "    # Step 1: Replace spaces with \"<_>\"\n",
        "    text_with_tokens = text.replace(\" \", \"<_>\")\n",
        "    unknowns = []\n",
        "\n",
        "    # Step 2: Break into syllables while keeping \"<_>\" intact\n",
        "    syllables_string = break_syllables(text_with_tokens)\n",
        "\n",
        "    # Step 3: Split into syllables (keeping \"<_>\" as a unit)\n",
        "    syllables_list = syllables_string.split(\" \")\n",
        "\n",
        "    # Step 4: Convert syllables to token IDs\n",
        "    encoded_ids = []\n",
        "    i = 0\n",
        "    while i < len(syllables_list):\n",
        "        syllable = syllables_list[i]\n",
        "\n",
        "        # Detect \"<_>\" token pattern\n",
        "        if syllable == \"<\" and i + 2 < len(syllables_list) and syllables_list[i + 1] == \"_\" and syllables_list[i + 2] == \">\":\n",
        "            encoded_ids.append(vocab_dict[\"<_>\"])\n",
        "            i += 3  # Skip '<', '_', and '>'\n",
        "        elif syllable in vocab_dict:\n",
        "            encoded_ids.append(vocab_dict[syllable])\n",
        "            i += 1\n",
        "        else:\n",
        "            unknowns.append(syllable)\n",
        "            encoded_ids.append(vocab_dict.get(\"<UNK>\", 0))  # Default to 0 if \"<UNK>\" isn't in vocab\n",
        "            i += 1\n",
        "\n",
        "    # Step 5: Convert to tensor with batch dimension\n",
        "    print(set(unknowns))\n",
        "    print(len(unknowns))\n",
        "    print(len(set(unknowns)))\n",
        "    return torch.tensor(encoded_ids).unsqueeze(0)\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids):\n",
        "    # Step 1: Remove batch dimension\n",
        "    flat_ids = token_ids.squeeze(0).tolist()  # Convert tensor to list\n",
        "\n",
        "    # Step 2: Create a reverse mapping (ID -> word)\n",
        "    reverse_vocab_dict = {v: k for k, v in vocab_dict.items()}\n",
        "\n",
        "    # Step 3: Find the ID for \"<_>\" (space token)\n",
        "    space_id = vocab_dict.get(\"<_>\", None)\n",
        "\n",
        "    # Step 4: Decode each token ID\n",
        "    decoded_words = []\n",
        "    for token_id in flat_ids:\n",
        "        if token_id == space_id:\n",
        "            decoded_words.append(\" \")  # Replace \"<_>\" with space\n",
        "        elif token_id in reverse_vocab_dict:\n",
        "            decoded_words.append(reverse_vocab_dict[token_id])  # Convert ID to syllable\n",
        "        else:\n",
        "            decoded_words.append(\"<UNK>\")  # Handle unknown tokens\n",
        "\n",
        "    # Step 5: Join the decoded words into a string\n",
        "    return \"\".join(decoded_words)  # Keep spacing intact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c3af690-efb9-4cce-9194-dbb91948e6df",
      "metadata": {
        "id": "1c3af690-efb9-4cce-9194-dbb91948e6df"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7925bcbd-d4ea-4b0b-9112-65f45a4c3b69",
      "metadata": {
        "id": "7925bcbd-d4ea-4b0b-9112-65f45a4c3b69"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05f0a70a-35c7-4c86-9cae-3de4edb3ecb7",
      "metadata": {
        "id": "05f0a70a-35c7-4c86-9cae-3de4edb3ecb7"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # 2*4*768\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "        # 2*4*768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a1a5dca-a00e-4d2a-be03-fff988b1a69d",
      "metadata": {
        "id": "2a1a5dca-a00e-4d2a-be03-fff988b1a69d"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "model = GPTModel(GPT_CONFIG)\n",
        "model.eval();  # Disable dropout during inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32d25aa7-7ae0-4e25-a365-848fa4316b05",
      "metadata": {
        "id": "32d25aa7-7ae0-4e25-a365-848fa4316b05"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "316f2d13-7963-422a-9570-75d7c51f77d4",
      "metadata": {
        "id": "316f2d13-7963-422a-9570-75d7c51f77d4"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "\n",
        "    ###Input batch:\n",
        " ###tensor([[6109, 3626, 6100,  345],\n",
        "        ##[6109, 1110, 6622,  257]])\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc24e02c-4112-48e6-9a27-2ec35d62cc7c",
      "metadata": {
        "id": "dc24e02c-4112-48e6-9a27-2ec35d62cc7c"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a37c4f-4624-4267-9dbc-2dac738c5578",
      "metadata": {
        "id": "81a37c4f-4624-4267-9dbc-2dac738c5578"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "699168a4-cdd0-45a8-96b5-e7aa2caf7fce",
      "metadata": {
        "id": "699168a4-cdd0-45a8-96b5-e7aa2caf7fce"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "\n",
        "            if global_step % 25 == 0:\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f237da04-5420-4093-9ff0-9ea89b0367ce",
      "metadata": {
        "id": "f237da04-5420-4093-9ff0-9ea89b0367ce"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Pre-train\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3135b0e-6bbb-4794-b459-575f1645e375",
      "metadata": {
        "id": "e3135b0e-6bbb-4794-b459-575f1645e375"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = text_to_token_ids(txt)\n",
        "        token_ids = token_ids.squeeze(0)\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(input_chunk.clone().detach())\n",
        "            self.target_ids.append(target_chunk.clone().detach())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "815337d6-4fbb-42cc-be77-f7230d7046e0",
      "metadata": {
        "id": "815337d6-4fbb-42cc-be77-f7230d7046e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9da1bd2-e3ab-4793-d21c-ea4731664c76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'နူတ်', 'ကုန္ဒ', 'ဗျုး', 'တ\\u200c', 'ဝစ္ဆေ', 'ဇိင်း', 'တော့ည်', 'ခံ\\ufeff', 'ပြဟ်', 'ဒူလ်', 'တ့', 'ကိုး\\ufeff', 'ဇွပ်', '၀ူး', 'လိင်း', 'ဒစ္စ', 'ယိန်', 'မို့စ်', 'မြှင်', 'ဆောတ်', 'ဇာ့တ်', 'နေ\\ufeff', 'ဟောလ်', 'ဂ္ဂိုင်', 'ကျွန့်', 'ချုန်', 'ဂျင်း', 'နက်ဖ်', 'တဲ့\\ufeff', 'ဗွိုင်း', 'ကြိတ်\\ufeff', 'နား\\ufeff', 'လျာက်', 'တိက်္က', 'ဒီး\\ufeff', 'ဝတ်\\ufeff', 'စောင့်င့်', 'ကုဏ္ဍ', 'ဿာန်', 'ခန္ဒာ', 'nç', 'လို့\\ufeff', '၀ုိင်း', '>ဋ္ဌာ', 'ဟုမ်းစ်', 'နည်း\\ufeff', 'ဟာ\\u200c', 'လျှောင်', 'မြွင့်', 'ဝန်\\ufeff', 'နွိုက်', '၊ည်', 'ရက်စ်', 'ဖွင်', 'နာည်း', 'ဗြန်း', '-ိလ်', 'ဥ\\u200c\\u200c', 'စာင့်', 'နဲ့\\ufeff', 'မီ\\ufeff', 'ညီ\\u200c', 'အိမ်\\ufeff', 'မာ\\u200c', 'ဝ\\u200c', 'ဒက္ကာ', 'သန်္နိဋ္ဌာန်', 'ရှိန့်', 'ဘော့ဗ်', 'ဇီစ်', 'ကူ့', 'ဝုဖ်', 'ရွှီးတ်', 'ရေွး', 'ရစ်ခ်ျ', 'လောက်\\ufeff', 'အို့တ်', 'ဆော့ဖ်', 'အောက်စ်', 'ဟတ်ခ်', 'မြန်\\ufeff', 'ဆေ', 'လင်္ဂ', 'ပေါတ်', 'သမ်္ဗာန်', 'ကျာ်', 'ဒမ်', 'ဆွေး\\u200c', '၀ှက်', 'ဒွတ္တ', 'မစ္စက်', 'တော့\\ufeff', 'ဂွတ်', 'ရှုး', 'မား\\ufeff', 'သန္ဒိ', 'ဆုံုး', 'ဘီးလ်', 'ဤတ်', 'ပြိုင််', 'အဇ္စျ', 'ကို\\ufeff', 'U\\ufeff', 'ပျောက်\\u200c', 'ဥ်း\\u200c', 'နိုုင်', '၀င်္ကန္တ', '၇က်က်', 'ကြိုင်း', 'ဗို\\ufeff', 'နတ္ထ', 'ဂျော့', 'ဒန္တီ', 'မ\\ufeff', 'အယ်လ်ပ်', 'ကောက်စ်', 'ဂျေး', 'သုဒ္ဓါ', 'အိတ်စ်', 'မယ်လ်', 'ဆယ်လ်', 'ပြီ\\ufeff', 'ဖေ့', 'မျြွှေ', 'မက်စ်', 'တိ\\ufeff', 'ဆွဲာ', 'နေးး', 'ချောက်\\ufeff', '>မ်', 'ယျဉ်', 'ကို့လ်ဗ်', '၀\\ufeff', 'ရှေ့း', 'တည်\\u200c', 'ဘိန်', '5', 'ကျွီ', 'ကြေွ', 'ချို\\ufeff', 'လာင်း', 'ယမ်း\\ufeff', 'ဘစ်', 'ဆေ့ဖ်', 'ကတ်စ်', 'လြာ', '>ဒ္မေ', 'ရက်\\u200c', 'ဒါရ်', 'သက်\\ufeff', 'တိိုင်', 'သောင်း\\ufeff', 'ဂျော', 'ညဏ်', 'ဂန္ဒီ', 'ရှဒ်', 'လျံ', '“ဒ်', 'ဆေွ့', 'ကြား\\ufeff', 'လက္ခာ', 'ကိလ္လိ', 'ဂျော့ဗ်', 'ရေး\\u200c', '\\ufeff\\ufeff', 'ဂျွင်', 'ကြွြွ', 'ကုန်\\ufeff', 'ကိယ်', 'နွိုင်း', 'ဂွေ', 'ရစ္ဆာ', 'ထဲန်', 'ချာင်း', 'ဂေါ့ဂ်', 'ဖို့စ်', 'ယောတ်', 'ကွယ့်', 'ကေျး', 'တေွး', 'ခုံးစ်', '၀ံ့', 'တွိင်', 'အိန်', 'ခြုံ\\u200c', 'ခြံု', 'သင်္ဃန်း', 'ရှား\\ufeff', 'နော်ဲ', '•\\ufeff', 'ခန်္ဓာ', 'ဒွီ', 'ဂျိမ်း', 'ပေး\\u200c', 'ပြောင်း\\u200c', 'ပုပ္ပါး', 'မန်\\ufeff', 'ဆုုိ', 'တိုတ်', 'ဘော\\u200c', 'ဒွင်', 'ကျော့်', 'ချုႋင်း', '၇န်း', 'ရဲန်', 'ကည့်', 'တုတ်း', 'န်း', '၀င်္က', 'အဗ္ဗ', 'မင်ယ်', 'မှာ\\ufeff', 'ခုံး\\ufeff', 'ဒုပ်', 'သိင်္ဂြိုဟ်', 'ခု\\ufeff', 'ဒြေ', 'ချားလ်စ်', 'ကြိုတ်', 'ဝင်\\u200c', 'ဒဗ္ဗ', 'တောည်', 'ငယ့်', 'ဗယ်', '‘က်', 'ရွင်', 'ဂျူန်း', '၀ီ', 'ည့့်', 'လှန်\\u200c', 'ဗန္ဓု', '၀ိုင်', 'ဆြွို', 'ယက္ခ', 'ဘော\\u200c\\u200c', 'ဒတ္တေ', 'စာ\\ufeff', 'မှု့', 'ဣန္ဒိ', 'လိုတ်', '>\\u200c\\u200c', 'လစ်ပ်', 'ကျံု့', '၈း', '၈ါ်', 'ပီတ်', 'ဍတ်', 'ဝေါ့', 'တော်ြ', 'ကြွြ', 'သွား\\ufeff', 'အိဗ်', 'နိုင်\\ufeff', '(ခ်', 'လျှေုာက်', 'မော့လ်', 'လစ်ဖ်', 'ရား\\u200c', 'ငြိပ်', 'သယ့်', 'ပို\\u200c', 'ကြုံ့', 'မဂ်္ဂ', 'နှစ်\\u200c', 'မော့ထ်', 'ဖ\\u200c', 'ဝိဇ်္ဇာ', 'လျူး', 'ရွှေ\\ufeff', 'ဆေ့ပ်', 'ဗီးလ်', 'တြု', 'ရန်\\ufeff', 'ရုန်း\\u200c', 'ညဥ့်', '“က်', 'တန်\\ufeff', 'ဂွိုင်', '8', 'မောလ်', '>ည်', 'ကျ\\ufeff', 'မç', 'ထုပ္ပတ္တိ', 'လွမ်းဂ်', '”\\ufeff', 'စဉ်\\ufeff', 'မျတ်', 'ရဝ်', 'ဒိ်္ဓ', '၍\\ufeff', 'မှုး', 'လိပ်\\u200c', ')\\ufeff', 'ညဉ်', 'အော်လ်', 'လေဉ်', 'ဇမ်္ဗူ', 'တပ္ပ', 'မှ\\ufeff', 'ကက္ကင်း', 'လှှ', 'ရစ်ဖ်', 'ကြွ်', 'တုု', '>ြွေ', '>ဏ်', 'ဗျမ်', 'မှူ', 'လော့ပ်', 'မြင်\\ufeff', 'ခ့ံ', 'တေွ့', 'သြွေော', 'ခိုး\\ufeff', '>လ်တ်', '>န်း', 'ပတ်သ်', '`', 'လေှ', 'ဒုင်', 'ဘွန်', 'တိန်္န', 'ဗြိစ္စ', '>–', 'ဓည့်', 'ဝါစ်', 'လျှံ့', '°°', 'ထောတ်', 'ပဉ်္စာ', 'ဝ\\ufeff', 'တြပ္ပ', 'လှို့က်', '၀ုန်း', 'ကြမ္မ', '၀ိုင်း', 'လွိုက်', 'ဟိန္ဒ', 'ညွ', '၀ု', 'အံ့့', 'သ\\ufeff', 'ရြီ့း', 'မျေ', 'တိက်', 'ဘုံး', 'လာ\\ufeff', 'အဉ်း', '။\\ufeff', 'မှပ်', 'ကမ္မန္တော', 'အဲလ်ဘ်', 'ဘံ', 'ပေးလ်စ်', 'သူ့\\u200c', 'ပက္ကု', 'သည််', 'သတ္တာ', 'စွက်\\ufeff', 'င့်\\ufeff', 'နေ့\\ufeff', 'ကန်ုး', '၀ါး', 'အက်ဒ်', '၅ç', 'တယ်\\ufeff', 'သပ်\\ufeff', 'သျှင်', 'ခေတ်\\u200c', 'ယွန်းမ်', 'ဇန်း', 'နေတ္တိ', 'လို\\u200c', 'ငြွ့်', 'ချန္ဒ', 'ဖိုဒ်', 'ဩတ္တ', 'သဏ္ဍာ', 'ဂိ်္ဂ', 'ဂက်စ်', 'အင်မ်', 'ဝယ်\\ufeff', 'စဝ်', 'သတ်\\ufeff', 'လေး\\ufeff', 'ဒီ့', 'တော်\\u200c', 'နာ\\ufeff', 'မေှာင်', 'ယုန်း', 'ဘွက်', '>ခ်စ်', 'တင်\\ufeff', 'အ\\u200c\\u200c\\u200c', 'ဗွပ်', 'မွာ', 'မတ်စ်', 'မီးင်္သ', 'ညွင်း', 'ဖေ့စ်', 'လွူပ်', 'သဒ်္ဒါ', 'ဆက်ဗ်', '၀ှီ', 'တွေဲ', 'ဂေါ့ဒ်', 'နဲ့့', 'ချန်း', 'မြှေးာက်', 'ရး', 'ကြွေ်', 'ပဒ္ဓေ', 'ဗုဒ္ခ', 'ငေွ', 'အစ်စ်', 'ဗုဒ်္ဓ', 'ရိုုက်', 'ထုက်', 'ဂွတ္တ', 'စော\\ufeff', 'မာတ်', 'စည်း\\u200c', 'သစ်က်', 'စေ\\ufeff', 'ယပ်စ်', 'မက်ဖ်', 'တွေ့\\ufeff', 'ရန်စ်', '၂ç', 'ရှိုင်', 'ဆွီ', 'Pည်း', 'ကဲလ်', 'ဇန္န', 'ကြာင့်', 'နမ္မ', 'တွေက်', 'ဂျင့်', 'ပုံ\\ufeff', 'ခီး', 'န့်ခ်', 'ကြီး\\ufeff', 'ဘာက်', 'ပေါ့င်း', 'ပျော်ြ', 'အဇ္စျတ္တ', 'ဟျ', 'မွား', 'ကုဉ္စ', 'ဉာည့်', 'ရော့တ်', 'လွိုင်', 'စျာန်', 'ရာ\\ufeff', 'လပ်ဖ်', 'လွုပ်', 'လေ\\ufeff', 'ဘာ့', 'နှစ်\\ufeff', 'ယယ့်', 'နူန်း', 'လိုက်ခ်', 'နိင်', 'ယောက်\\ufeff', 'ပေး\\ufeff', 'ဆွစ်', 'နွောင့်', 'ကျ်ာ', 'လိမ္ဗော်', 'လော့ဗ်', 'ချ\\ufeff', 'ကျူပ်', 'အေင်', 'ငယ်\\ufeff', 'ကြို\\ufeff', 'ဒစ်ခ်ျ', 'ဥာဏ်', 'ရပ္ပ', 'မောင့်', 'ဗင်း', 'အန္တု', 'ဒင်းန်', 'ပါ်', 'ဖေါ်', 'ဖြစ်\\ufeff', 'ထြွေို', 'ရေွ့', 'သိုယ်', 'အုံ\\ufeff', 'ဝတ္ထိ', 'က\\u200c', '•', 'ဥက်္က', 'လောဝ်', 'စင်္ကြန်', 'ဖလ်', 'ကာ\\ufeff', 'င့်\\u200c', 'ကျောမ်', 'ပွင်', 'မှုံ\\ufeff\\ufeff', 'သွ', '၀ပ်', 'အက်စ်', '၇န်', 'အော့ဖ်', 'အေမ်', 'ကောင့်', '>ြွ', 'ဆောင်\\ufeff', 'နက်ခ်', 'ချွ', 'နေ်္ဓ', 'ဆေ့', 'စု\\u200c', 'တည်\\ufeff', 'မိတ္ထိ', 'စွဖ်', 'ဘော\\u200c\\u200c\\u200c', 'စဉ်း\\ufeff', 'နှုန်', 'ကန္ဒတ္တ', '0', 'င်္ဝါး', 'သင်္ကပ္ပော', 'ဝို', 'ဉဏ်', 'က\\ufeff', 'ပြယ့်', '။´´', 'ရိုစ်', 'လို့ါ', 'ဝိုဏ်း', 'ပဲ\\ufeff', 'ရှုံး\\u200c', 'ရွှံ', 'လိမ္မော', 'မယ်ယ်', 'ရင်္ဂ', 'ရှး', 'လျှေက်', 'ထန်\\ufeff', 'ဒစ်ရ်', 'လက်္ခ', 'သယ်လ်', 'ဂြုႋဟ်', 'မေ်္မာ်', 'ကိုုယ်', 'နှိပ်\\ufeff', 'သောလ္ဘက်', 'ပျယ်', 'သိီ', 'အာလ်', 'ဝေါသ်', 'ဟိန်', 'ဖို့\\ufeff', 'ဂို့ဗ်', 'မက္ကာ', 'ဓာတ်\\ufeff', 'တက်္က', '>မ်း', 'ဟုမ္မ', 'ဗြံုး', 'ရက်င့်', '>င့်', 'အယ်လ်', 'လေဋ္ဋု', 'ဝီလ်', 'အဇ္ဇ', 'ကော့ဗ်', 'ဂါရ်', 'လိက်', 'ကင်မ့်', 'နှိ', 'ဗြဟ်္မ', 'ဝဲလ်', 'တာလ်', 'ပြံုး', 'အွန်း', 'ဘောင်္သား', 'သြွာ', 'ယှင်', 'ယမ္ပိ', 'ဒါက်', 'ဝေးလ်', 'ဖဲလ်', 'ပျော်\\ufeff', 'ဂိ်္ဂလ်', 'ဒါဏ်', 'ညိမ့်', 'ကွိုင်း', '၊း', 'ဝါ့', 'ရော\\ufeff', 'ဥ်\\u200c', 'အိဖ်', 'သော်\\ufeff', 'ဖေွ', 'ဗို', 'ပဗ္ဗ', 'ယေက်', 'ညီ\\ufeff', 'အြွုံ', 'ဖြး', 'ယူး\\u200c', 'င့့်', 'ဂိုဏ်', 'အမ့်', '၀ါ', 'လျှောတ်', 'ဒေါ့ဖ်', '>ဖ်', 'မစ္စစ်', 'ရိုး\\ufeff', 'ဖွန်', 'ပါ\\ufeff', 'စျာ', '>ာ', '၉း', 'ကျမ္ဘ', '°', 'ကိစ္စလ်', 'ပြံု', 'ကုမ္ဘ', 'မြှော', 'ချက်ဇ်', 'ဘီလ်', 'ကုမ္ဘာဏ်', 'ဟဂ္ဂ', 'ရေး\\ufeff', 'အက်ဖ်', 'E\\ufeff', '>စ္ဆာ', 'တံ\\ufeff', 'ဆိုင်\\u200c', 'ဓိပ္ပါယ်', 'ဆိုး\\ufeff', 'နှ့ံ', 'ယယ်လ်', 'အဲ့လ်', 'ဧည်', 'ပလ်္လင်', 'ချစ်ေ', 'ဦ', 'ရာန်', 'မယ်လ်င်း', 'ပေါလ်', 'ရက္ကာ', '၀မ်း', 'ထိုုင်', 'မူ\\ufeff', 'ပြစ်\\ufeff', 'ဗိုင်', 'ဗုဒ္ဓိ', 'ဂျက်ဖ်', 'ရမ့်', 'ဘုန်', 'သဗ္ဗေ', 'ဦး\\u200c', 'ပုတ္တ', 'နူမ်', 'ဂျဇ်', 'ဘွန်း', 'ခဲ့\\ufeff', 'ယှဉ်း', 'ဟစ်လ်', 'လတ်\\ufeff', 'ဝါ\\ufeff', 'မော်လ်', 'ဓော', 'မည်\\u200c', 'ဘော့စ်', 'ယိပ်', 'ဘေး\\ufeff', 'များ\\ufeff', 'ရောမ်', 'ည့်\\ufeff', '>ေ', 'တင်း\\ufeff', 'နစ္စ', 'ထိန်းန်', 'တောင်\\ufeff', 'ဖြသ်', 'ရပ်\\ufeff', 'ကိုးလ်', 'ရော့စ်', '>\\u200c\\u200c\\u200c', 'ကန္ဒ', 'သမ်္ဗန်', 'စုမ်', 'နှို', 'သမ္မဉ်', '၀န်', '1', 'ကျႌ', 'ရိမ်ှု', 'လော့ခ်', 'တို့\\ufeff', 'ယွမ်း', 'အက်္ခ', 'လာ\\u200c', 'မြွတ်', 'နှီး\\u200c', 'ဘဒ်္ဒ', '>လ်', 'ဝေ့ဗ်', 'ဟု\\ufeff', 'ဦး', 'ဘုသ်', 'ထင်က်', '“\\u200c', 'ဖြေး\\u200c', '၀င့်', 'ရွုတ်', 'ပစ္စုပ္ပာန်', 'နက်္ခတ်', 'ဇာ\\u200c', 'ဖြတ်\\ufeff', 'အ\\u200c', 'လက်ခ်ျ', 'မိုက်ခ်', 'နာ့စ်', 'မှန်း\\ufeff', 'မာည်', 'နှင့်လ္ဘက်', 'သဒ်္ဓါ', 'အ့ံ', 'ချီလ်', 'တေွေ', 'အတ်စ်', 'လေိန်', 'ဘာာ', 'န့်\\u200c\\u200c\\u200c', 'ဟောင်းလ်', 'ဝက်စ်', 'တည်း\\ufeff', 'ကန္ဒတ္ထ', 'ရစ်\\ufeff', 'ဟော့ဖ်', '9', 'အုက်ဒ်', 'နွ', 'ယုံယ့်', 'မေွး', 'ဂျမ်', 'ယူ\\ufeff', 'အူး', 'ဖစ်စ်', 'ဟုက်', 'ဗဒ္ဓ', 'အဂ်္ဂ', 'ဣန္ဒြ', 'နိန်', 'သုတ္တ', 'ကောဏ္ဍ', 'ဗက်စ်', 'သေက်္ခ', 'သက္ခ', '>ြစ်', 'စာတ်', '၄ç', '>ီ', 'မီလ်', 'ရွှီး', 'ချီ\\u200c', 'ကျံုး', 'ည်', 'ခြိမ်း\\u200c', 'နက္ခ', 'လျက်ဇ်', '၁ç', 'ရှိ႕', 'ဝန်ဂ်', 'ငြမ်', '၇ှင်', 'ကျွိ', 'ကဒ်', 'ကော\\ufeff', 'ထောင်\\ufeff', 'ငြိက်', '၀ိဇ်္ဇာ', 'စဥ့်', 'ယက်စ်', 'ခိုက်\\ufeff', 'ရား\\ufeff', 'မွန့်', 'မ့််', 'ဂွန်း', 'လူ့ြ့', 'ဘလ္လိ', 'ထားသ်', 'စာ်္ဆန်', 'မြွောက်', 'ပီ့', 'တ\\u200c\\u200c', 'ကျွန််', 'သိ\\ufeff', 'တော့ါ', 'ပိုးလ်', 'ရှို', 'တီ့', 'အဓ္မ', 'ဆီးယ့်', '>ခ်', 'မမ်း', 'လုုပ်', 'ဒြောင်', 'ဘုံလ်', 'နွယ့်', 'ပုည္က', 'ယောင်္ကျား', 'သိဒ္ဓတ္ထ', 'မတ္တာ', 'မာ့စ်', 'တ်', 'နြွို့', 'တန္ဒြေ', 'ကာ့', 'န့့်', 'င့ါ', 'ဟပ်ဖ်', 'ချြေား', 'ဒြွေ်', 'စမ္မာ', 'အန့်', 'နှား', 'နတ္တူ', 'မ့', 'တိုး\\ufeff', 'တတ္တ', 'ဘို့စ်', 'သူ့\\ufeff', 'သလ်္လာ', 'ဝှစ်ခ်ျ', 'ဖန်းဂ်', 'ပြေး\\ufeff', 'တိဗ်', 'တိုင်း\\ufeff', 'မန္', 'နြေ', 'ဥ်း', 'ကျ်ား', 'မတ်ခ်', 'လျင်\\ufeff', 'ရည်\\ufeff', 'ငေါင်', 'လှိ', 'လျို့', 'ဆင်\\ufeff', 'နန္ဒိ', 'ခွဲြ့', 'ဖတ်စ်', 'ကင်မ်', 'ဥမ်္မာ', 'တွိစ်', 'တော့ပ်', 'ဥ်', '၍်', 'လာ့', 'မဂ္ဂီ', 'ကြ\\ufeff', 'ဖီးလ်', 'စုံ့', 'ကိုုယ့်', 'ဂျံုး', 'လောက်စ်', 'လှည်', 'လမ်း\\ufeff', '၈တ်', 'အိဏ်', 'ညှိတ်', 'ဒီးက်', 'ဆွိတ်', 'ပြည်\\ufeff', 'င့်ု', 'ပဒ္မေ', 'ယက္ကန်း', 'ဟတ်ခ်ပ်', 'ရွှေး', '၀ဲ', 'တိုး××', 'လဲဖ်', '၁ည်', 'လျှာက်', 'ဝေါလ်', 'ပစ္စုပ္ပါန်', 'c⁴', '(စ်', 'ထက်\\ufeff', 'အိန္ဒ', 'ဒုဒ်', 'ဆွေဖ်', 'ချိ်န်', 'ဝံံ့', '၀န်း', 'လိုဟ်', 'ကောင်း\\u200c', '၃\\ufeff', 'ကျွန်ု်ပ့်', 'ဒဘ်', 'ကျွန်ု်ပ်', 'င့််', 'ဖိန့်', 'တွက်\\ufeff', 'ပါး\\ufeff', 'နီ့', 'စန္သီ', 'ငုံ့တ်', 'ဿာဒ်', 'တပ်စ်', 'ဘို့း', 'ကု\\u200c\\u200c', 'ဗဲ', 'လတ်စ်', 'မာမ်', 'အစ်ဂ်', 'ခင်း\\ufeff', 'ဉ့််', 'ဂျီင်', 'ညြွေ့်', 'လမ်္ပာယ်', '၊\\ufeff', 'ပြန်\\ufeff', 'ခွါ', '6', 'ည့်လ်', 'တွေင်', '>ောင့်', '>န်', 'ရဲလ်', 'ဆောလ်စ်', 'ရောင်\\ufeff', 'တန္ဒ', 'ဒတ်စ်', 'ပေျာက်', 'ရှု့', 'လက္က', '—–', 'ချွမ်', 'ရစ်္ဆာန်', 'လွှမ်ား', 'ယောက်္ခ', 'ဆွေက်', 'ပါင်း', 'ရှိလ္ဘက်', 'ကို့လ်ဘ်', 'ရမ္မာ', 'င့်မ်', 'မိဒ်', 'ဣန္ဒေ', 'ဂျန်း', 'ကိိုင်', 'ဂေျး', 'ယာ့ဒ်', 'စိတ္တာ', 'ဖို့ည်း', 'ရွိုက်စ်', 'ဓတ်', 'ဒစ်စ်', 'ဂျား', 'ပာတ်', 'ဂင်း', 'ဒက်ခ်', 'နတ္တိ', 'မြံု', 'ရစ်ဇ်', 'နွယ်\\ufeff', 'မွည်း', 'ရုံ\\ufeff', 'ရီ့', 'ပို\\ufeff', 'မက်ဒ်', 'ညေ', 'ကြ်ာ', 'ကျွန်\\ufeff', '>က်', 'အိက်', 'ချိတ်\\ufeff', 'နစ်ခ်', 'ရိုုင်း', 'သည်\\ufeff', 'ခဲ့ည်', 'ဖမ်', 'ဗုန်', 'ဘတ္တ', 'နော့သ်', 'ယှစ်', '.\\ufeff', 'ဆာ့ဘ်', 'အ\\ufeff', 'ဆွန်', 'ချိုတ်', 'မောဒ်', 'ဝေ\\ufeff', 'နျ', 'ဆီက်', 'လက္မ', 'ကမ်္မဋ္ဌာန်း', 'ဆန်\\ufeff', 'တိုင်\\ufeff', '>န္ဒာ', 'မျိုးလ်', 'ပတ်ဖ်', 'ဤ်', 'ချိုန်', 'ပေါ့း', 'နှုိး', 'မည်\\ufeff', 'မက္ကင်', 'လး', 'ဂျုန်း', 'လ\\ufeff', 'ငတ်တ်', 'လိ်ု', '၀တ္တ', 'ဗစ္စ', 'ခြတ်', 'ချယ်လ်', '2', 'သန်း\\u200c', '၀ဋ်', '၌\\ufeff', 'ဖောက်ခ်', 'လို\\ufeff', '·', 'ဆူလ်', 'နစ်စ်', 'တက်ဒ်', '>လ္ဘက်', 'မုဒ်္ဒ', 'တိုုက်', 'သမ္ဘိန္န', 'လော့စ်', 'အြွေ', 'လူး\\ufeff', 'ပျောက်\\ufeff', 'ဘော်လ်', 'ဆဲ့', 'တိုင်လ်', '၀ိဇ္ဇာ', 'လက်\\ufeff', 'ဖေ့်စ်', 'တြေ', '၎င်းင်း', 'င့်ေ', '၏\\ufeff', 'လုံ\\ufeff', '>စ်', 'Y\\ufeff', 'သက်္က', 'ပဲါ', 'လို\\ufeffက်', 'ဂျွိုင်း', 'ဟဲဒ်', 'လ္ဘက်', 'လီ့', 'ဆာဖ်', 'လွပ်', 'ကြွြွ်', 'ည့ံ', 'ညိမ်း', 'ပူး\\u200c\\u200c\\u200c', 'ရောစ်', 'မဲ့\\ufeff', 'ရြွေ', 'စမ္ပါ', 'ဟဖ်', 'ဘုန်း\\ufeff', '>်', 'ငြွေို', 'လာက့်', 'စေွ့', 'ထား\\ufeff', 'စက္ခ', 'ဟာ့', 'ပြဒ်', 'ယှပ်', 'ရေ့ခ်', '၀ç', '>း', 'လာတ့်', 'ဖျော', 'သေ့\\ufeff', 'ဆောက်ျား', 'အောင်\\ufeff', 'ဘောက်စ်', 'ကွစ်ဘ်', 'ရိက်္ခာ', 'အိပ်ခ်ျ', 'ညံ့\\ufeff', 'ဇေး', 'စိ်တ်', 'ထဲ\\u200c', 'ပါပ်', 'သိက်္ခာ', 'သမ္ပာ', 'သြွူ', 'ကြာင်း', 'ဋိတ်', 'ရမ္ပ', 'များြွေ', 'ဘယ်\\ufeff', 'နှမ်', 'ဆေွ', 'မြှစ်', 'ကပ်စ်', 'ကာ့ဒ်', 'စိက်', 'ဇီ့', 'ဒါးလ်', 'မစ်သ်', 'ပေ့ါ', 'ကားြွ', 'ပတ်\\ufeff', 'လှည့်ည့်', 'မျ', 'စတ်', 'ဖော်\\ufeff', 'မေးလ်', 'အတ္ထု', 'ဇား', 'င့်စ်', 'ပေါ်\\u200c\\u200c', 'ပြွ်', 'ယွန့်', 'နြေ်္ဒ', 'ဇဏ်', 'ရစ်သ်', 'ကာင်း', 'ဂို့', 'ရန်း\\ufeff', 'ဂြို', 'ယန်းဂ်', 'ဥ္ဇင်း', 'လဇ်္ဇီ', 'မာ့ခ်စ်', '>န့်', 'ရိုုး', 'ကား\\ufeff', 'S\\ufeff', 'တိုင်းမ်', 'ဆုံ\\u200c', 'လိ်ုက်', 'ကိုယ်\\u200c', 'မှည်', 'ကျွပ်', 'မှိုး', 'ပြံး', 'ချင်လ်', 'ချုပ်\\u200c', 'သူ\\ufeff', 'ပယ်လ်', 'တ\\u200c\\u200c\\u200c\\u200c', 'ဝုဒ်', 'ဘဂ္ဂ', 'သာ\\ufeff', 'ကဲက်', 'လောင်လ္ဘက်', 'လိမ်္မာ', '>ယ်', 'ဘို့ဒ်', 'ချပ်စ်', 'တွစ်', 'လင်းစ်', 'ကြွို', 'ဝို့လ်ဖ်', 'ဝက်ဘ်', '၀ိုး', 'ထဲ့', 'အတ်ဂ်', 'ပင်\\ufeff', 'ညိာ', 'မို့\\ufeff', 'ဖွင်း', 'ပင်း\\ufeff', 'တော့ခ်', 'အဇ်္စျ', 'ချူန်း', 'ဆို\\u200c', 'ရွိ', 'လစ္စ', 'ဖို့ဒ်', 'ကန္နီ', 'ရိုး\\u200c', 'ဒုက္ခီ', 'နှယ့်', 'ပို့တ်', 'ဒေ့', 'လေန္တေ', 'စိ\\ufeff', 'ချ်', 'ဂျု', 'ကတ်ဒ်', 'ဟဲလ်', 'နုက်', 'ဆက်\\ufeff', 'တွေပ်', 'ဥ္စင်း', 'ဝိက်\\ufeff', 'တွင်\\u200c', 'ခေျာ', 'လိုက်\\ufeff', 'ကင်းစ်', 'ကောက်\\ufeff', 'စေ်္ဆ', '\\u200c', 'ရစ်ပ်', 'ပိန့်', 'အ\\u200c\\u200c', '၀ယ်', 'တဇ္ု', 'ကွိုင်', 'မျုး', 'ဝဎ္ဍု', 'ဓိန့်', 'ဆိုုး', 'ရက်ဂ်', 'ဟွတ်', 'မုပ္ပါ', 'အဇ္ဈယ်', 'ကိုင်\\ufeff', 'ခုက္ကူ', 'သွမ်', 'တာဗ်', 'စောင်းလ္ဘက်', 'ရွယ်\\ufeff', 'ဖော့စ်', '…\\ufeff', 'အေး\\ufeff', 'လက်\\u200c', 'ဒက္ခီ', 'ဂွါ', 'ပေါ်\\u200c', 'ဗွိုင်', 'ကို့', 'ပြီါ', 'နိုိင်', 'မိန်း\\ufeff', 'အောင်\\u200c', 'ဆွာ', 'ဂျုံး', 'ငြွေ့်', 'မည်း\\ufeff', 'ရျင်', '>ယ့်', 'ဥံု', '3', 'မားလ်', 'ပွိုင်', 'ကေျာ်', 'ကျာ်း', 'ခတ််', 'ဟက်စ်', 'ထိုု', 'ကျွု', 'ဟဂ်္ဂ', 'မစ်ဒ်', 'ကိုပ်', '×××××××××××', 'ဒေါတ်', 'စူလ်', 'လျည်', 'ခဝ်', 'ကြိး', 'ဗြိမ်း', 'ကမ္ဗော', '>ု', 'ရ\\ufeff', 'တြွေ်', 'တောင်းန်', 'ဂက်', '>ဒ်ဏ်', 'လည်း\\ufeff', 'ဆိုဒ်', 'နင်္က', '“င်း', 'ဇက်က်', 'ကန်း\\ufeff', '>င်း', 'မြွေျား', 'စွာ\\ufeff', 'ကျွမ်', 'စား\\ufeff', 'အလ္လိ', 'ငှာ\\ufeff', 'ပြုိင်', 'ဖြစ်\\u200c', 'စင်္ကာ', 'တာ\\ufeff', 'ရွှေန်း', 'ဘေ့စ်', 'အိပ်စ်', 'ရင်\\ufeff', 'ရိုးလ်', 'လဖ်', 'နြေ္ဒ', '>°°', 'ကု\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c', 'ပန့်', 'ပုဂ်္ဂ', 'တော်\\ufeff', 'ဝ့ံ', '4', 'ကျမ္မာ', 'ရပ်စ်', 'တ\\ufeffင်', 'န့့်', 'ဆုဖ်', 'နေ့••', 'ငံ\\u200c', '၀ိ', 'လည်\\ufeff', 'ငှါး', 'တာ့ခ်', 'ထူး\\ufeff', 'ဖြို့', 'ဂျို့', 'ဟန်းစ်', 'မောဂ္ဂ', 'ခမ်', '>ဒ်', 'ဘက်ခ်', 'ဒါ\\ufeff', 'ရာ\\u200c', 'ဟွန်', 'ခါး\\ufeff', 'များလ်', '>မ္ပ', 'ဇဉ်း', 'ငြုပ်', '>\\ufeff', 'လေး\\u200c', 'လိင်္ဂ', 'ဟောဒ်', 'ရင်း\\ufeff', 'နို\\ufeff', 'တဒ်', 'ခဲ\\ufeff', 'ဥဒ္ဒံ', 'ဖေ\\ufeff', 'ဒွတ္တံ', 'ဆင်ရ်ွိင်', 'နဒ်္ဒီ', 'အင်္ကျ', 'ရွတ်ခ်', 'ဓမ်္မ', 'ဗပ်', 'မြွို', 'ပြံ', 'ဆိက်', 'ခေါ်ည်', 'ဗျီး', 'မြှု', 'အော့ဘ်', 'သွားည်', 'လား\\ufeff', 'ကျင်္ား', 'ဂျွိုက်စ်', 'တွင်\\ufeff', 'ဗယ်လ်', 'ပြး', '\\ufeffႏွယ်', 'မှှ', 'ကေ့', 'ဘပ်စ်', 'စီ့', '>▬▬', 'မျင်', 'မိခ်', 'ဦး\\ufeff', 'သလ်', 'ကွစ်', 'I\\ufeff', 'စိဉ္ဇ', 'လျောင်', 'ဒီးစ်', 'ဆစ်လ်', '>က်ဒ်', 'ဘုက်', 'ဂုမ်္ဘဏ်', 'လှုတ်', 'န့ံ', '၀င်', 'ခြင်\\ufeff', 'သစ်\\u200c', 'လိဖ်', 'စမ်း\\ufeff', 'မယ်\\ufeff', 'ပုပ္ပား', 'ဃာ့', '>ဉ်္စ', 'နယ်လ်', 'မျှု', 'မိပ္ပာယ်', 'ဆုက်', 'နှစ်\\u200c\\u200c\\u200c', 'ကိ်ုယ့်', 'မိိ', 'ဖင်းဒ်', 'နဒ္ဒီ', 'ကျုန်း', 'ဂဏ္ဍီ', 'ကုမ္မ', '>ပ်', 'ကို့စ်', 'ဂိန်', 'ဝိန်း', 'ဆိုင်တ်', 'ပိပ္ပ', 'ရှိ\\u200c', 'ညှန်', 'မံ\\ufeff', 'ရော့ဗ်', 'လူ\\u200c\\u200c', 'သေွး', 'ဖက်\\ufeff', 'ကၠဴ', 'ဂျွမ်', 'ဂမ္ဘီ', 'ဟာ့စ်', 'နက်စ်', '>́', 'လျှက်', 'တပ်\\ufeff', 'သို့\\ufeff', 'လုပ်\\ufeff', '>တ်', '၀င်း', 'တိို့', 'ဋိက္က', 'ပေါ့\\ufeff', 'ကျက်\\ufeff', 'ဟာ\\ufeff', '>ည်း', 'လျှင်\\ufeff', 'ပေ\\ufeff', 'ဗင့်', 'လောတ်', 'ဂျီဟ်', 'ပုဂ်္ဂိုလ်', 'ပဲလ်', 'ဘုတ်ခ်', 'ဒဲလ်', 'ဂျိတ်', 'ပြေင်း', 'သပ္ပါယ်', 'ကး', 'ရပ်ဖ်', 'ဗောက်စ်', 'ဂျော့ခ်', 'နောက်\\ufeff', 'ဆွေ\\u200c', '>တ္တူ', 'ပတ်ခ်', 'ပါ့ခ်', '7', 'ဘတ်ဂ်', 'ရေ့ဖ်', '၈ျယ်', 'မိ\\ufeff', 'တုတ်္ထ', 'ဆာလ်', 'x\\ufeff', 'ကုုန်း', 'ကြုိး', 'ကားလ်', 'ချိစ်', 'အတ္တာ', 'ဝင်\\ufeffး', 'ဂွဒ်', 'မျှော်\\ufeff', 'ဂယ်', 'မ\\u200c', 'ရီ\\ufeff', 'နုတ္ထိုရ်', 'လာည်', 'ကျွှန်', 'တို\\ufeff', 'လို့က်', 'လက္ကန့်', 'သူ\\u200c', 'ဂါး\\ufeff', 'ကျွုန်ုပ်', 'ဒင်္ဂါ', '၀တ်', 'ကယ်\\ufeff', 'မျှ\\ufeff', 'ဂိုလ်', 'ကမ္ဗည်း', 'က်', 'စွာ့', 'မာ\\u200c\\u200c', 'တစ်\\ufeff', 'ရက်ဇ်', 'နါး', 'သော\\ufeff', 'ချီးစ်', 'တြီး', 'ခုက်', 'ချံုး', '>င်', 'ဥိး', 'ရုဒ်', 'နှစ်ူ', 'သမ္ဘ', 'ဒါ့စ်', 'လုံး\\ufeff', 'ကေ့စ်', 'မှာ\\u200c', 'ဒုန်', 'ကြောင်း\\ufeff', 'ဟျု', '၀တ္ထု', 'ပွင်း', 'သြီး', '>°', 'ဂီးလ်', 'မင်\\ufeff', 'လွှာ\\ufeff', 'ဌက်', 'ကျမ္မ', 'ဆစ်ခ်', 'ဒီက်', '-\\ufeff', 'သြော်', 'ထို\\ufeff', 'ပတ်စ်', '>\\u200c', 'ဝင်္ကံ', 'သပ္ပ', 'ဏိဇ္ဇီ', 'မန္တာန်', 'ဒီစ္စ', 'ခန္ဓ', 'နြွ်', 'ပို့စ်', 'ပဉ်္ဇင်း', 'စိတ်\\u200c\\u200c\\u200c', 'သြား', 'ထံ\\ufeff', 'ပေါ်\\ufeff', 'ဘက်ပ်', 'ဇည့်', 'သူး', 'လွှစ်', 'ခေျာ့', 'ယ့်\\ufeff', 'ပွား\\u200c', 'လိ့', '>ိက်', 'မိင့်', 'သား\\ufeff', 'ဝှိ', 'ကိပ်', 'ဂေါရ်', 'ဇိုင်', 'ချက်\\ufeff', 'မာလ်', 'ရွှ', 'သွီ', 'ဂျိုင်', 'ိန်း', 'လိခ်', 'ဒီင့်', 'ရဋ္ဌော', 'သေး\\ufeff', 'သက်\\u200c', 'ချွင်း', '’\\ufeff', 'ရည့်', 'ပြီး\\ufeff', 'စိိ', 'ကမ္တာ', 'သန်း\\ufeff', 'ငိ', 'သိန္နီ', 'ရက်\\ufeff', 'များ\\u200c', 'ဘက်\\ufeff', 'လော့ဒ်', 'ဟက်ဒ်', 'ဒုဋ္ဌ', 'ဒုက်္ခ', 'ခွမ်း', 'ထွက်\\ufeff', 'ဥ\\u200c', '၀က်', 'ပူးလ်', 'သင်္ခန်း', 'ဂုမ္ဘာန်', 'ဆာန်', 'ဝိန္ဒ', 'ကျွန်ုပ်\\ufeff', 'ဂယ်လ်', 'နော့်', '××××××××××', 'ညာ်', 'ဈာန့်', '>ါ', 'ောက်', 'တေွ', '>ဋ္ဌ', 'ဝဲ\\u200c', 'တေ်္တ', 'ဒြွေီ', 'ညိတ်\\ufeff', 'မုန်း\\ufeff', 'ဖြစ်ာ', 'နု်', 'နွုတ်', 'ခြေ\\u200c', '”ုပ်', 'ဒေန်', 'တွေ\\ufeff', 'ရှ်', 'ကိစ္ရယ်', 'ရွိုင်း', 'ဟောက်န်း', 'ဂတ်စ်', 'ရွေး\\ufeff', 'ဘိခ်', 'ရုုပ်', 'တစ်စ်', 'O\\ufeff', 'ဝင်္ဂ', 'ခိုင့်', 'ခြံု့', 'ကွဲ\\ufeff', 'ဂျွပ်', 'ချစ်ပ်', '>•', 'ကိုလ်ဗ်', 'တုပ္ပ', 'ဥာ', 'ဟုတ်\\ufeff', 'ရှင်\\ufeff', 'ဆိုင်\\ufeff', 'ဘာ\\ufeff', 'ဆက်စ်', 'ငြှိုး', 'စွန်း\\u200c', 'ချတ်', 'အား\\ufeff', 'ယော့ခ်', 'အ့', 'လပ်စ်', 'မျက်\\ufeff', 'ရေက်'}\n",
            "14962\n",
            "1649\n",
            "{'မြှ', 'ကျ်ား', 'ရှိက်', 'မတ်ခ်', 'ဏျ', 'ဒီ့', 'ဆွာ', 'မတ်စ်', 'ရပ်လ်', 'ဖေ့စ်', 'တှ', 'လိုက်ဖ်', '3', 'ဖတ်စ်', 'နှဇ်', 'ဥ်', '>မ္မ', 'ဟျူး', 'ဇုတ်', 'ဟောလ်', 'သှား', 'ပါ့ခ်စ်', 'လာ့', 'လိစ်', 'ညျး', 'ပြုုံး', 'ကမ္ဗော', 'ယပ်စ်', 'ခေံ', 'ပက်စ်', 'ဆွီ', 'ဆိုးလ်', 'မှော', 'ခီး', 'ဂျင့်', 'ရဇ္စျာယ်', 'နမ္မ', 'အဇ္စျတ္တ', 'ခန္ဒာ', 'အာြ', 'မုလ်', 'စျာန်', 'ညြျ', 'နွိုက်', 'ကရ်', 'စင်္ကာ', 'အာဘ်', 'ယက္ကန်း', 'ဘာ့', 'ဝေါလ်', 'ကိုစ္စ', 'ရိုးလ်', 'ဖော်ြ', 'မီးလ်', 'ဒက်စ်', 'ရပ်စ်', 'ကျူပ်', 'ဗီလ်', 'မောင့်', 'စမ်းါ', 'ရမ့်ပ်', 'ကုရ်', 'မမ်', 'ု', 'ဝီးလ်', 'ပွို', 'ဆော့ဖ်', 'ကွာြ', 'ဟွန်', 'အောက်စ်', 'ဘူလ်', 'ကျိုက္ခ', 'ဆေ', 'တျ', 'မန္တိ', 'ဒမ်', 'ပွေီ', 'မစ္စက်', 'ဖြ်', 'ကျုး', 'အက်စ်', 'ဘိဇ္စျာ', 'ဟဒ်', 'အော့ဖ်', 'နိခ်', 'ကောင့်', 'အဇ္စျ', 'ထားး', 'ပြံ', 'တျေ', 'သူဋ္ဌေး', 'ကြိုင်း', 'ငျ', 'ဂျော့', 'ခြာက်', 'ဗွာ', 'ဆယ်လ်', 'မယ်လ်', 'ကော့စ်', 'ဖေ့', 'ဖြစ်ည်', 'ဟောရ်', 'ရွှံ', 'ရင်္ဂ', 'ဘာ့ဂ်', 'ဂေါင်း', 'ဟယ်လ်', 'တေ့ာ်', 'ဥျ', 'မက္ကာ', 'ရြော', 'ဘစ်', 'အိက်', 'နေို', 'ရပ်စ်က်', 'အယ်လ်', 'လြာ', 'ပေါျ', 'ခှဲ', 'အိဘ်', 'ဆာဘ်', 'စုက္ကူ', 'ဆွန်', 'ဝဲလ်', 'ကော့စ်တ်', 'နျ', 'ညဏ်', 'ဝဇ်', 'ကို့စ်', 'ဂန္ဒီ', 'စျ', 'ဂေးလ်', 'လွမ်', 'ယဲဇ်', 'ré', 'ရေဲ', 'ဝေးလ်', 'လျံ', 'ပှဲ', 'မျး', 'ညိမ့်', 'ညဥ့်', 'ဂမ္ဘီ', 'ပတ်ခ်စ်', 'နက်စ်', '>́', 'မက်လ်', 'ပွု', 'ဓာရ်', 'ရေိ', 'ဧက္ကဂ္ဂ', '>ဉ်း', 'များး', 'မွိ', 'ကွယ့်', '2', 'အက်ဇ်', 'အိတ်ခ်စ်', 'ရဇ်', 'အိန်', 'မော့စ်', 'နျး', 'ဆူလ်', 'နျ့', 'ကှ', 'တက်ဒ်', 'ဗင့်', 'ပဲလ်', 'ပှား', 'ဘီလ်', 'ငွေး', 'နှု', 'အက်ဖ်', 'ဂြာ', '–', '>့', 'ဇာက်', 'အင်္သေ', 'ဆက်ဒ်', 'ပေါလ်', 'သည်–', 'ပြင့်', 'ပဲါ', 'ဂျွိုင်း', 'ကွှ', 'လီ့', 'ဘော့ဘ်', 'ကားလ်', 'အာန်', 'ပြာင်း', 'ဘွန်း', 'သောျ', 'လာြ', 'ဂယ်', 'တူစ်', 'ကျယ်\\u200cေ', 'ကွော', 'နစ္စ', 'tü', 'ခြီ', 'ပွူ', '>း', 'ဂိုလ်', 'သှ', 'တောျ', 'ရော့စ်', 'ငျ့', 'ဂေါလ်', 'တြီး', 'ဂျုး', 'ဇေး', 'လော့ခ်', '1', 'ရုဒ်', 'ဝေါ့', 'ဖှ', 'ကပ်စ်', 'ပေ့စ်', 'ကျိုတ်', 'သာြ', 'သန္ဒိဋ္ဌာန်', 'မျ', 'ခြု', 'ကြှ', 'မေးလ်', 'မာ့ဖ်', 'အတ္ထု', 'ဆစ်ခ်', 'မစ်ဇ်', 'ကြောျ', 'မွော', 'မန္တာန်', 'တက်ခ်', 'ညျ့', 'ပို့စ်', 'မစ္ဆား', 'ဝက်စ်', 'တိုင်းမ်', 'ဆစ််', 'သူး', 'ဒီု', 'နီဒ်', 'ဒါ့သ်', 'မှာြ', 'မော့်', 'ပြောြ', 'ဗိတ်', 'စှ', 'အူး', 'ဇိုင်', 'တမ္ဘာ', 'ခား', 'ဿ်', 'စှာ', 'နော့ခ်', 'ရှပ်စ်', 'ဝုဒ်', 'ဘဂ္ဂ', 'လှှ', 'စာတ်', 'ညျ', 'ရုပ်စ်', 'မီလ်', 'ချပ်စ်', 'တွစ်', 'ရွိုက်', 'လော့ဒ်', 'ဝက်ဘ်', 'ကျ်', '“ါ', 'ဂျယ်လ်', 'တော့ခ်', '`', 'ဂျူုး', 'ဘွန်', 'ကှဲ', 'ဂယ်လ်', 'ဆဗ္ဗ', 'န့်စ်က်', 'ဘာလ်', 'အဇ္ဇု', 'ဖို့ဒ်', 'နိန္ဒ', 'မော့တ်', 'ဘားလ်', 'ခြင်္သ့', 'နှယ့်', 'ဒေ့', 'ပိုးလ်', 'ချ်', 'ဂန္ဌ', 'မာ့က်စ်', 'ဟော့ဘ်စ်', 'ငျ်', 'မမ်း', 'သျှဒ်', 'ရွိုက်ဒ်', 'ဝတ္တု', 'ကီးလ်', 'ရှ်', 'မျိူး', '>က္ကု', 'အေးစ်', 'တင်းစ်', 'စြျ', 'မွှု', 'လော့ဂ်', 'ကာ့', 'ဗာရ်', 'ဗို့စ်', '>•', 'ဟိတ္တိ', 'ယုဒ္ဓ', 'ပို့စ်ထ်', 'အလ္လာ့', 'တာဗ္ဗစ်', 'လေ့စ်', 'ဇိဒ်', 'ဆေးလ်', 'ယက်ဗ်', 'ဇန်း', 'သတ္ထု', 'ယော့ခ်', 'ဂြူး', 'ထာရ်', 'ငျး', 'နြေ', 'ဥ်း'}\n",
            "1300\n",
            "328\n"
          ]
        }
      ],
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG[\"context_length\"],\n",
        "    stride=GPT_CONFIG[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG[\"context_length\"],\n",
        "    stride=GPT_CONFIG[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b85f99d6-bea5-4b1a-892d-ae690195dacf",
      "metadata": {
        "id": "b85f99d6-bea5-4b1a-892d-ae690195dacf",
        "outputId": "0a152395-3b4a-466a-f227-53c6239ceb4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 8.36904888132595\n",
            "Validation loss: 8.367079822629767\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fd4cf09-5871-4254-ac54-8e4945900985",
      "metadata": {
        "id": "3fd4cf09-5871-4254-ac54-8e4945900985",
        "outputId": "253016dd-1f9d-4a03-db60-4a106f31f795",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 7.195, Val loss 7.366\n",
            "Ep 1 (Step 000025): Train loss 5.413, Val loss 5.650\n",
            "Ep 1 (Step 000050): Train loss 5.075, Val loss 5.381\n",
            "Ep 1 (Step 000075): Train loss 5.141, Val loss 5.243\n",
            "Ep 1 (Step 000100): Train loss 4.819, Val loss 5.120\n",
            "Ep 1 (Step 000125): Train loss 4.919, Val loss 5.037\n",
            "Ep 1 (Step 000150): Train loss 4.670, Val loss 5.043\n",
            "Ep 1 (Step 000175): Train loss 4.735, Val loss 5.001\n",
            "Ep 1 (Step 000200): Train loss 4.488, Val loss 4.951\n",
            "Ep 1 (Step 000225): Train loss 4.614, Val loss 4.915\n",
            "Ep 1 (Step 000250): Train loss 4.600, Val loss 4.906\n",
            "Ep 1 (Step 000275): Train loss 4.599, Val loss 4.834\n",
            "Ep 1 (Step 000300): Train loss 4.392, Val loss 4.819\n",
            "Ep 1 (Step 000325): Train loss 4.379, Val loss 4.838\n",
            "Ep 1 (Step 000350): Train loss 4.476, Val loss 4.847\n",
            "Ep 1 (Step 000375): Train loss 4.268, Val loss 4.826\n",
            "Ep 1 (Step 000400): Train loss 4.370, Val loss 4.747\n",
            "Ep 1 (Step 000425): Train loss 4.068, Val loss 4.709\n",
            "Ep 1 (Step 000450): Train loss 4.383, Val loss 4.713\n",
            "Ep 1 (Step 000475): Train loss 4.169, Val loss 4.673\n",
            "Ep 1 (Step 000500): Train loss 4.244, Val loss 4.698\n",
            "Ep 1 (Step 000525): Train loss 4.251, Val loss 4.669\n",
            "Ep 1 (Step 000550): Train loss 4.539, Val loss 4.673\n",
            "Ep 1 (Step 000575): Train loss 4.261, Val loss 4.627\n",
            "Ep 1 (Step 000600): Train loss 4.097, Val loss 4.638\n",
            "Ep 1 (Step 000625): Train loss 4.264, Val loss 4.610\n",
            "Ep 1 (Step 000650): Train loss 4.204, Val loss 4.612\n",
            "Ep 1 (Step 000675): Train loss 4.247, Val loss 4.581\n",
            "Ep 1 (Step 000700): Train loss 4.010, Val loss 4.540\n",
            "Ep 1 (Step 000725): Train loss 3.992, Val loss 4.577\n",
            "Ep 1 (Step 000750): Train loss 4.318, Val loss 4.482\n",
            "Ep 1 (Step 000775): Train loss 4.171, Val loss 4.536\n",
            "Ep 1 (Step 000800): Train loss 4.258, Val loss 4.462\n",
            "Ep 1 (Step 000825): Train loss 4.204, Val loss 4.500\n",
            "Ep 1 (Step 000850): Train loss 4.104, Val loss 4.519\n",
            "Ep 1 (Step 000875): Train loss 4.097, Val loss 4.466\n",
            "Ep 1 (Step 000900): Train loss 3.997, Val loss 4.502\n",
            "Ep 1 (Step 000925): Train loss 4.022, Val loss 4.413\n",
            "Ep 1 (Step 000950): Train loss 4.064, Val loss 4.478\n",
            "Ep 1 (Step 000975): Train loss 4.167, Val loss 4.418\n",
            "Ep 1 (Step 001000): Train loss 3.928, Val loss 4.390\n",
            "Ep 1 (Step 001025): Train loss 3.998, Val loss 4.321\n",
            "Ep 1 (Step 001050): Train loss 3.928, Val loss 4.342\n",
            "Ep 1 (Step 001075): Train loss 3.933, Val loss 4.379\n",
            "Ep 1 (Step 001100): Train loss 4.110, Val loss 4.357\n",
            "Ep 1 (Step 001125): Train loss 4.128, Val loss 4.341\n",
            "Ep 1 (Step 001150): Train loss 3.858, Val loss 4.363\n",
            "Ep 1 (Step 001175): Train loss 3.913, Val loss 4.362\n",
            "Ep 1 (Step 001200): Train loss 4.040, Val loss 4.350\n",
            "Ep 1 (Step 001225): Train loss 4.018, Val loss 4.365\n",
            "Ep 1 (Step 001250): Train loss 4.202, Val loss 4.340\n",
            "Ep 1 (Step 001275): Train loss 3.871, Val loss 4.374\n",
            "Ep 1 (Step 001300): Train loss 3.969, Val loss 4.381\n",
            "Ep 1 (Step 001325): Train loss 4.024, Val loss 4.348\n",
            "Ep 1 (Step 001350): Train loss 4.044, Val loss 4.313\n",
            "Ep 1 (Step 001375): Train loss 3.879, Val loss 4.337\n",
            "Ep 1 (Step 001400): Train loss 4.244, Val loss 4.343\n",
            "Ep 1 (Step 001425): Train loss 4.087, Val loss 4.366\n",
            "Ep 1 (Step 001450): Train loss 4.130, Val loss 4.354\n",
            "Ep 1 (Step 001475): Train loss 4.153, Val loss 4.353\n",
            "Ep 1 (Step 001500): Train loss 3.637, Val loss 4.371\n",
            "Ep 1 (Step 001525): Train loss 3.801, Val loss 4.351\n",
            "Ep 1 (Step 001550): Train loss 4.036, Val loss 4.351\n",
            "Ep 1 (Step 001575): Train loss 3.875, Val loss 4.357\n",
            "Ep 1 (Step 001600): Train loss 3.946, Val loss 4.325\n",
            "Ep 1 (Step 001625): Train loss 4.053, Val loss 4.293\n",
            "Ep 1 (Step 001650): Train loss 3.754, Val loss 4.225\n",
            "Ep 1 (Step 001675): Train loss 3.879, Val loss 4.196\n",
            "Ep 1 (Step 001700): Train loss 4.186, Val loss 4.177\n",
            "Ep 1 (Step 001725): Train loss 3.920, Val loss 4.191\n",
            "Ep 1 (Step 001750): Train loss 3.791, Val loss 4.177\n",
            "Ep 1 (Step 001775): Train loss 3.930, Val loss 4.209\n",
            "Ep 1 (Step 001800): Train loss 3.954, Val loss 4.214\n",
            "Ep 1 (Step 001825): Train loss 3.571, Val loss 4.218\n",
            "Ep 1 (Step 001850): Train loss 3.623, Val loss 4.206\n",
            "Ep 1 (Step 001875): Train loss 3.730, Val loss 4.224\n",
            "Ep 1 (Step 001900): Train loss 3.705, Val loss 4.175\n",
            "Ep 1 (Step 001925): Train loss 4.027, Val loss 4.160\n",
            "Ep 1 (Step 001950): Train loss 3.743, Val loss 4.199\n",
            "Ep 1 (Step 001975): Train loss 3.817, Val loss 4.203\n",
            "Ep 1 (Step 002000): Train loss 3.824, Val loss 4.193\n",
            "Ep 1 (Step 002025): Train loss 3.919, Val loss 4.184\n",
            "Ep 1 (Step 002050): Train loss 3.707, Val loss 4.218\n",
            "Ep 1 (Step 002075): Train loss 3.750, Val loss 4.164\n",
            "Ep 1 (Step 002100): Train loss 3.650, Val loss 4.090\n",
            "Ep 1 (Step 002125): Train loss 3.703, Val loss 4.155\n",
            "Ep 1 (Step 002150): Train loss 3.950, Val loss 4.145\n",
            "Ep 1 (Step 002175): Train loss 3.814, Val loss 4.105\n",
            "Ep 1 (Step 002200): Train loss 3.383, Val loss 4.094\n",
            "Ep 1 (Step 002225): Train loss 3.606, Val loss 4.088\n",
            "Ep 1 (Step 002250): Train loss 3.597, Val loss 4.115\n",
            "Ep 1 (Step 002275): Train loss 3.671, Val loss 4.051\n",
            "Ep 1 (Step 002300): Train loss 3.601, Val loss 4.056\n",
            "Ep 1 (Step 002325): Train loss 3.831, Val loss 4.017\n",
            "Ep 1 (Step 002350): Train loss 3.743, Val loss 4.002\n",
            "Ep 1 (Step 002375): Train loss 3.946, Val loss 3.985\n",
            "Ep 1 (Step 002400): Train loss 3.737, Val loss 3.950\n",
            "Ep 1 (Step 002425): Train loss 3.886, Val loss 4.038\n",
            "Ep 1 (Step 002450): Train loss 4.082, Val loss 4.003\n",
            "Ep 1 (Step 002475): Train loss 3.715, Val loss 3.924\n",
            "Ep 1 (Step 002500): Train loss 3.701, Val loss 3.924\n",
            "Ep 1 (Step 002525): Train loss 3.658, Val loss 3.950\n",
            "Ep 1 (Step 002550): Train loss 3.666, Val loss 3.916\n",
            "Ep 1 (Step 002575): Train loss 3.784, Val loss 3.939\n",
            "Ep 1 (Step 002600): Train loss 3.558, Val loss 3.931\n",
            "Ep 1 (Step 002625): Train loss 3.438, Val loss 3.957\n",
            "Ep 1 (Step 002650): Train loss 3.956, Val loss 3.906\n",
            "Ep 1 (Step 002675): Train loss 3.677, Val loss 3.860\n",
            "Ep 1 (Step 002700): Train loss 3.566, Val loss 3.879\n",
            "Ep 1 (Step 002725): Train loss 3.682, Val loss 3.883\n",
            "Ep 1 (Step 002750): Train loss 4.000, Val loss 3.861\n",
            "Ep 1 (Step 002775): Train loss 3.799, Val loss 3.924\n",
            "Ep 1 (Step 002800): Train loss 3.854, Val loss 3.874\n",
            "Ep 1 (Step 002825): Train loss 3.576, Val loss 3.905\n",
            "Ep 1 (Step 002850): Train loss 3.668, Val loss 3.956\n",
            "Ep 1 (Step 002875): Train loss 3.820, Val loss 3.894\n",
            "Ep 1 (Step 002900): Train loss 3.629, Val loss 3.894\n",
            "Ep 1 (Step 002925): Train loss 3.605, Val loss 3.885\n",
            "Ep 1 (Step 002950): Train loss 3.608, Val loss 3.898\n",
            "Ep 1 (Step 002975): Train loss 3.427, Val loss 3.897\n",
            "Ep 1 (Step 003000): Train loss 3.642, Val loss 3.832\n",
            "Ep 1 (Step 003025): Train loss 3.607, Val loss 3.870\n",
            "Ep 1 (Step 003050): Train loss 3.660, Val loss 3.862\n",
            "Ep 1 (Step 003075): Train loss 3.484, Val loss 3.878\n",
            "Ep 1 (Step 003100): Train loss 3.677, Val loss 3.917\n",
            "Ep 1 (Step 003125): Train loss 3.558, Val loss 3.913\n",
            "Ep 1 (Step 003150): Train loss 3.548, Val loss 3.885\n",
            "Ep 1 (Step 003175): Train loss 3.494, Val loss 3.843\n",
            "Ep 1 (Step 003200): Train loss 3.693, Val loss 3.823\n",
            "Ep 1 (Step 003225): Train loss 3.808, Val loss 3.830\n",
            "Ep 1 (Step 003250): Train loss 3.575, Val loss 3.863\n",
            "Ep 1 (Step 003275): Train loss 3.848, Val loss 3.868\n",
            "Ep 1 (Step 003300): Train loss 3.463, Val loss 3.846\n",
            "Ep 1 (Step 003325): Train loss 3.422, Val loss 3.793\n",
            "Ep 1 (Step 003350): Train loss 3.442, Val loss 3.823\n",
            "Ep 1 (Step 003375): Train loss 3.518, Val loss 3.773\n",
            "Ep 1 (Step 003400): Train loss 3.415, Val loss 3.784\n",
            "Ep 1 (Step 003425): Train loss 3.470, Val loss 3.802\n",
            "Ep 1 (Step 003450): Train loss 3.552, Val loss 3.769\n",
            "Ep 1 (Step 003475): Train loss 3.548, Val loss 3.768\n",
            "Ep 1 (Step 003500): Train loss 3.739, Val loss 3.801\n",
            "Ep 1 (Step 003525): Train loss 3.599, Val loss 3.729\n",
            "Ep 1 (Step 003550): Train loss 3.621, Val loss 3.762\n",
            "Ep 1 (Step 003575): Train loss 3.776, Val loss 3.770\n",
            "Ep 1 (Step 003600): Train loss 3.741, Val loss 3.808\n",
            "Ep 1 (Step 003625): Train loss 3.520, Val loss 3.783\n",
            "Ep 1 (Step 003650): Train loss 3.483, Val loss 3.732\n",
            "Ep 1 (Step 003675): Train loss 3.240, Val loss 3.776\n",
            "Ep 1 (Step 003700): Train loss 3.642, Val loss 3.763\n",
            "Ep 1 (Step 003725): Train loss 3.525, Val loss 3.799\n",
            "Ep 1 (Step 003750): Train loss 3.567, Val loss 3.798\n",
            "Ep 1 (Step 003775): Train loss 3.582, Val loss 3.786\n",
            "Ep 1 (Step 003800): Train loss 3.789, Val loss 3.817\n",
            "Ep 1 (Step 003825): Train loss 3.527, Val loss 3.764\n",
            "Ep 1 (Step 003850): Train loss 3.482, Val loss 3.749\n",
            "Ep 1 (Step 003875): Train loss 3.589, Val loss 3.767\n",
            "Ep 1 (Step 003900): Train loss 3.529, Val loss 3.764\n",
            "Ep 1 (Step 003925): Train loss 3.511, Val loss 3.756\n",
            "Ep 1 (Step 003950): Train loss 3.367, Val loss 3.756\n",
            "Ep 1 (Step 003975): Train loss 3.409, Val loss 3.761\n",
            "Ep 1 (Step 004000): Train loss 3.589, Val loss 3.784\n",
            "Ep 1 (Step 004025): Train loss 3.658, Val loss 3.800\n",
            "Ep 1 (Step 004050): Train loss 3.509, Val loss 3.774\n",
            "Ep 1 (Step 004075): Train loss 3.338, Val loss 3.767\n",
            "Ep 1 (Step 004100): Train loss 3.482, Val loss 3.809\n",
            "Ep 1 (Step 004125): Train loss 3.511, Val loss 3.783\n",
            "Ep 1 (Step 004150): Train loss 3.632, Val loss 3.788\n",
            "Ep 1 (Step 004175): Train loss 3.626, Val loss 3.787\n",
            "Ep 1 (Step 004200): Train loss 3.380, Val loss 3.818\n",
            "Ep 1 (Step 004225): Train loss 3.556, Val loss 3.815\n",
            "Ep 1 (Step 004250): Train loss 3.392, Val loss 3.811\n",
            "Ep 1 (Step 004275): Train loss 3.474, Val loss 3.824\n",
            "Ep 1 (Step 004300): Train loss 3.519, Val loss 3.783\n",
            "Ep 1 (Step 004325): Train loss 3.662, Val loss 3.775\n",
            "Ep 1 (Step 004350): Train loss 3.578, Val loss 3.759\n",
            "Ep 1 (Step 004375): Train loss 3.562, Val loss 3.775\n",
            "Ep 1 (Step 004400): Train loss 3.595, Val loss 3.790\n",
            "Ep 1 (Step 004425): Train loss 3.506, Val loss 3.778\n",
            "Ep 1 (Step 004450): Train loss 3.488, Val loss 3.788\n",
            "Ep 1 (Step 004475): Train loss 3.662, Val loss 3.788\n",
            "Ep 1 (Step 004500): Train loss 3.577, Val loss 3.808\n",
            "Ep 1 (Step 004525): Train loss 3.501, Val loss 3.849\n",
            "Ep 1 (Step 004550): Train loss 3.569, Val loss 3.749\n",
            "Ep 1 (Step 004575): Train loss 3.667, Val loss 3.744\n",
            "Ep 1 (Step 004600): Train loss 3.766, Val loss 3.743\n",
            "Ep 1 (Step 004625): Train loss 3.256, Val loss 3.756\n",
            "Ep 1 (Step 004650): Train loss 3.472, Val loss 3.762\n",
            "Ep 1 (Step 004675): Train loss 3.704, Val loss 3.769\n",
            "Ep 1 (Step 004700): Train loss 3.276, Val loss 3.785\n",
            "Ep 1 (Step 004725): Train loss 3.883, Val loss 3.801\n",
            "Ep 1 (Step 004750): Train loss 3.220, Val loss 3.688\n",
            "Ep 1 (Step 004775): Train loss 3.521, Val loss 3.666\n",
            "Ep 1 (Step 004800): Train loss 3.519, Val loss 3.717\n",
            "Ep 1 (Step 004825): Train loss 3.507, Val loss 3.711\n",
            "Ep 1 (Step 004850): Train loss 3.491, Val loss 3.724\n",
            "Ep 1 (Step 004875): Train loss 3.354, Val loss 3.700\n",
            "Ep 1 (Step 004900): Train loss 3.588, Val loss 3.725\n",
            "Ep 1 (Step 004925): Train loss 3.564, Val loss 3.731\n",
            "Ep 1 (Step 004950): Train loss 3.440, Val loss 3.666\n",
            "Ep 1 (Step 004975): Train loss 3.394, Val loss 3.687\n",
            "Ep 1 (Step 005000): Train loss 3.291, Val loss 3.685\n",
            "Ep 1 (Step 005025): Train loss 3.249, Val loss 3.675\n",
            "Ep 1 (Step 005050): Train loss 3.263, Val loss 3.694\n",
            "Ep 1 (Step 005075): Train loss 3.442, Val loss 3.626\n",
            "Ep 1 (Step 005100): Train loss 3.449, Val loss 3.696\n",
            "Ep 1 (Step 005125): Train loss 3.521, Val loss 3.687\n",
            "Ep 1 (Step 005150): Train loss 3.716, Val loss 3.647\n",
            "Ep 1 (Step 005175): Train loss 3.279, Val loss 3.690\n",
            "Ep 1 (Step 005200): Train loss 3.375, Val loss 3.647\n",
            "Ep 1 (Step 005225): Train loss 3.494, Val loss 3.702\n",
            "Ep 1 (Step 005250): Train loss 3.279, Val loss 3.659\n",
            "Ep 1 (Step 005275): Train loss 3.165, Val loss 3.707\n",
            "Ep 1 (Step 005300): Train loss 3.145, Val loss 3.728\n",
            "Ep 1 (Step 005325): Train loss 3.496, Val loss 3.697\n",
            "Ep 1 (Step 005350): Train loss 3.143, Val loss 3.673\n",
            "Ep 1 (Step 005375): Train loss 3.501, Val loss 3.710\n",
            "Ep 1 (Step 005400): Train loss 3.378, Val loss 3.725\n",
            "Ep 1 (Step 005425): Train loss 3.374, Val loss 3.725\n",
            "Ep 1 (Step 005450): Train loss 3.361, Val loss 3.678\n",
            "Ep 1 (Step 005475): Train loss 3.527, Val loss 3.663\n",
            "Ep 1 (Step 005500): Train loss 3.377, Val loss 3.661\n",
            "Ep 1 (Step 005525): Train loss 3.271, Val loss 3.645\n",
            "Ep 1 (Step 005550): Train loss 3.276, Val loss 3.671\n",
            "Ep 1 (Step 005575): Train loss 3.428, Val loss 3.648\n",
            "Ep 1 (Step 005600): Train loss 3.203, Val loss 3.656\n",
            "Ep 1 (Step 005625): Train loss 3.373, Val loss 3.638\n",
            "Ep 1 (Step 005650): Train loss 3.336, Val loss 3.678\n",
            "Ep 1 (Step 005675): Train loss 3.252, Val loss 3.648\n",
            "Ep 1 (Step 005700): Train loss 3.383, Val loss 3.643\n",
            "Ep 1 (Step 005725): Train loss 3.360, Val loss 3.598\n",
            "Ep 1 (Step 005750): Train loss 3.495, Val loss 3.617\n",
            "Ep 1 (Step 005775): Train loss 3.290, Val loss 3.599\n",
            "Ep 1 (Step 005800): Train loss 3.490, Val loss 3.620\n",
            "Ep 1 (Step 005825): Train loss 3.186, Val loss 3.681\n",
            "Ep 1 (Step 005850): Train loss 3.478, Val loss 3.652\n",
            "Ep 1 (Step 005875): Train loss 3.327, Val loss 3.664\n",
            "Ep 1 (Step 005900): Train loss 3.556, Val loss 3.645\n",
            "Ep 1 (Step 005925): Train loss 3.476, Val loss 3.648\n",
            "Ep 1 (Step 005950): Train loss 3.388, Val loss 3.657\n",
            "Ep 1 (Step 005975): Train loss 3.287, Val loss 3.675\n",
            "Ep 1 (Step 006000): Train loss 3.515, Val loss 3.618\n",
            "Ep 1 (Step 006025): Train loss 3.398, Val loss 3.593\n",
            "Ep 1 (Step 006050): Train loss 3.273, Val loss 3.570\n",
            "Ep 1 (Step 006075): Train loss 2.963, Val loss 3.612\n",
            "Ep 1 (Step 006100): Train loss 3.230, Val loss 3.585\n",
            "Ep 1 (Step 006125): Train loss 3.140, Val loss 3.603\n",
            "Ep 1 (Step 006150): Train loss 3.284, Val loss 3.612\n",
            "Ep 1 (Step 006175): Train loss 3.581, Val loss 3.596\n",
            "Ep 1 (Step 006200): Train loss 3.423, Val loss 3.613\n",
            "Ep 1 (Step 006225): Train loss 3.396, Val loss 3.603\n",
            "Ep 1 (Step 006250): Train loss 3.193, Val loss 3.602\n",
            "Ep 1 (Step 006275): Train loss 3.321, Val loss 3.624\n",
            "Ep 1 (Step 006300): Train loss 3.393, Val loss 3.611\n",
            "Ep 1 (Step 006325): Train loss 3.391, Val loss 3.624\n",
            "Ep 1 (Step 006350): Train loss 3.178, Val loss 3.611\n",
            "Ep 1 (Step 006375): Train loss 3.480, Val loss 3.567\n",
            "Ep 1 (Step 006400): Train loss 3.313, Val loss 3.508\n",
            "Ep 1 (Step 006425): Train loss 3.506, Val loss 3.538\n",
            "Ep 1 (Step 006450): Train loss 3.448, Val loss 3.532\n",
            "Ep 1 (Step 006475): Train loss 3.481, Val loss 3.575\n",
            "Ep 1 (Step 006500): Train loss 3.168, Val loss 3.516\n",
            "Ep 1 (Step 006525): Train loss 3.409, Val loss 3.537\n",
            "Ep 1 (Step 006550): Train loss 3.257, Val loss 3.501\n",
            "Ep 1 (Step 006575): Train loss 3.385, Val loss 3.579\n",
            "Ep 1 (Step 006600): Train loss 3.332, Val loss 3.519\n",
            "Ep 1 (Step 006625): Train loss 3.265, Val loss 3.548\n",
            "Ep 1 (Step 006650): Train loss 3.352, Val loss 3.502\n",
            "Ep 1 (Step 006675): Train loss 3.240, Val loss 3.571\n",
            "Ep 1 (Step 006700): Train loss 3.487, Val loss 3.587\n",
            "Ep 1 (Step 006725): Train loss 3.401, Val loss 3.545\n",
            "Ep 1 (Step 006750): Train loss 3.171, Val loss 3.580\n",
            "Ep 1 (Step 006775): Train loss 3.586, Val loss 3.563\n",
            "Ep 1 (Step 006800): Train loss 3.246, Val loss 3.556\n",
            "Ep 1 (Step 006825): Train loss 3.198, Val loss 3.602\n",
            "Ep 1 (Step 006850): Train loss 3.587, Val loss 3.550\n",
            "Ep 1 (Step 006875): Train loss 3.398, Val loss 3.511\n",
            "Ep 1 (Step 006900): Train loss 3.156, Val loss 3.477\n",
            "Ep 1 (Step 006925): Train loss 3.157, Val loss 3.490\n",
            "Ep 1 (Step 006950): Train loss 3.291, Val loss 3.518\n",
            "Ep 1 (Step 006975): Train loss 3.553, Val loss 3.507\n",
            "Ep 1 (Step 007000): Train loss 3.224, Val loss 3.535\n",
            "Ep 1 (Step 007025): Train loss 3.224, Val loss 3.567\n",
            "Ep 1 (Step 007050): Train loss 3.367, Val loss 3.524\n",
            "Ep 1 (Step 007075): Train loss 3.253, Val loss 3.527\n",
            "Ep 1 (Step 007100): Train loss 3.294, Val loss 3.520\n",
            "Ep 1 (Step 007125): Train loss 3.071, Val loss 3.548\n",
            "Ep 1 (Step 007150): Train loss 3.306, Val loss 3.567\n",
            "Ep 1 (Step 007175): Train loss 3.235, Val loss 3.474\n",
            "Ep 1 (Step 007200): Train loss 3.265, Val loss 3.494\n",
            "Ep 1 (Step 007225): Train loss 3.307, Val loss 3.524\n",
            "Ep 1 (Step 007250): Train loss 3.219, Val loss 3.513\n",
            "Ep 1 (Step 007275): Train loss 3.483, Val loss 3.549\n",
            "Ep 1 (Step 007300): Train loss 3.221, Val loss 3.542\n",
            "Ep 1 (Step 007325): Train loss 3.483, Val loss 3.533\n",
            "Ep 1 (Step 007350): Train loss 3.098, Val loss 3.559\n",
            "Ep 1 (Step 007375): Train loss 3.070, Val loss 3.568\n",
            "Ep 1 (Step 007400): Train loss 3.350, Val loss 3.522\n",
            "Ep 1 (Step 007425): Train loss 3.293, Val loss 3.534\n",
            "Ep 1 (Step 007450): Train loss 3.175, Val loss 3.580\n",
            "Ep 1 (Step 007475): Train loss 3.130, Val loss 3.555\n",
            "Ep 1 (Step 007500): Train loss 3.489, Val loss 3.542\n",
            "Ep 1 (Step 007525): Train loss 3.233, Val loss 3.561\n",
            "Ep 1 (Step 007550): Train loss 3.420, Val loss 3.528\n",
            "Ep 1 (Step 007575): Train loss 3.299, Val loss 3.564\n",
            "Ep 1 (Step 007600): Train loss 3.160, Val loss 3.545\n",
            "Ep 1 (Step 007625): Train loss 3.179, Val loss 3.513\n",
            "Ep 1 (Step 007650): Train loss 3.399, Val loss 3.544\n",
            "Ep 1 (Step 007675): Train loss 3.012, Val loss 3.575\n",
            "Ep 1 (Step 007700): Train loss 3.143, Val loss 3.553\n",
            "Ep 1 (Step 007725): Train loss 3.234, Val loss 3.561\n",
            "Ep 1 (Step 007750): Train loss 3.222, Val loss 3.560\n",
            "Ep 1 (Step 007775): Train loss 3.304, Val loss 3.577\n",
            "Ep 1 (Step 007800): Train loss 3.275, Val loss 3.567\n",
            "Ep 1 (Step 007825): Train loss 3.168, Val loss 3.518\n",
            "Ep 1 (Step 007850): Train loss 3.258, Val loss 3.559\n",
            "Ep 1 (Step 007875): Train loss 3.199, Val loss 3.532\n",
            "Ep 1 (Step 007900): Train loss 3.105, Val loss 3.525\n",
            "Ep 1 (Step 007925): Train loss 3.073, Val loss 3.578\n",
            "Ep 1 (Step 007950): Train loss 3.301, Val loss 3.519\n",
            "Ep 1 (Step 007975): Train loss 3.290, Val loss 3.545\n",
            "Ep 1 (Step 008000): Train loss 3.222, Val loss 3.502\n",
            "Ep 1 (Step 008025): Train loss 3.115, Val loss 3.539\n",
            "Ep 1 (Step 008050): Train loss 3.314, Val loss 3.487\n",
            "Ep 1 (Step 008075): Train loss 3.148, Val loss 3.464\n",
            "Ep 1 (Step 008100): Train loss 3.404, Val loss 3.528\n",
            "Ep 1 (Step 008125): Train loss 3.312, Val loss 3.505\n",
            "Ep 1 (Step 008150): Train loss 3.274, Val loss 3.510\n",
            "Ep 1 (Step 008175): Train loss 3.552, Val loss 3.455\n",
            "Ep 1 (Step 008200): Train loss 3.236, Val loss 3.475\n",
            "Ep 1 (Step 008225): Train loss 2.904, Val loss 3.494\n",
            "Ep 1 (Step 008250): Train loss 3.145, Val loss 3.461\n",
            "Ep 1 (Step 008275): Train loss 3.275, Val loss 3.468\n",
            "Ep 1 (Step 008300): Train loss 3.025, Val loss 3.472\n",
            "Ep 1 (Step 008325): Train loss 3.205, Val loss 3.435\n",
            "Ep 1 (Step 008350): Train loss 3.247, Val loss 3.463\n",
            "Ep 1 (Step 008375): Train loss 3.183, Val loss 3.428\n",
            "Ep 1 (Step 008400): Train loss 3.134, Val loss 3.423\n",
            "Ep 1 (Step 008425): Train loss 3.191, Val loss 3.437\n",
            "Ep 1 (Step 008450): Train loss 3.273, Val loss 3.407\n",
            "Ep 1 (Step 008475): Train loss 3.196, Val loss 3.414\n",
            "Ep 1 (Step 008500): Train loss 3.226, Val loss 3.341\n",
            "Ep 1 (Step 008525): Train loss 2.967, Val loss 3.370\n",
            "Ep 1 (Step 008550): Train loss 3.215, Val loss 3.391\n",
            "Ep 1 (Step 008575): Train loss 3.355, Val loss 3.428\n",
            "Ep 1 (Step 008600): Train loss 3.203, Val loss 3.405\n",
            "Ep 1 (Step 008625): Train loss 3.168, Val loss 3.411\n",
            "Ep 1 (Step 008650): Train loss 3.357, Val loss 3.429\n",
            "Ep 1 (Step 008675): Train loss 3.340, Val loss 3.467\n",
            "Ep 1 (Step 008700): Train loss 3.239, Val loss 3.519\n",
            "Ep 1 (Step 008725): Train loss 3.311, Val loss 3.497\n",
            "Ep 1 (Step 008750): Train loss 3.116, Val loss 3.477\n",
            "Ep 1 (Step 008775): Train loss 3.331, Val loss 3.423\n",
            "Ep 1 (Step 008800): Train loss 3.183, Val loss 3.416\n",
            "Ep 1 (Step 008825): Train loss 3.238, Val loss 3.442\n",
            "Ep 1 (Step 008850): Train loss 3.151, Val loss 3.410\n",
            "Ep 1 (Step 008875): Train loss 3.159, Val loss 3.439\n",
            "Ep 1 (Step 008900): Train loss 2.878, Val loss 3.410\n",
            "Ep 1 (Step 008925): Train loss 3.113, Val loss 3.505\n",
            "Ep 1 (Step 008950): Train loss 3.032, Val loss 3.488\n",
            "Ep 1 (Step 008975): Train loss 2.988, Val loss 3.428\n",
            "Ep 1 (Step 009000): Train loss 3.175, Val loss 3.464\n",
            "Ep 1 (Step 009025): Train loss 2.951, Val loss 3.487\n",
            "Ep 1 (Step 009050): Train loss 3.035, Val loss 3.448\n",
            "Ep 1 (Step 009075): Train loss 2.982, Val loss 3.495\n",
            "Ep 1 (Step 009100): Train loss 3.383, Val loss 3.499\n",
            "Ep 1 (Step 009125): Train loss 3.197, Val loss 3.473\n",
            "Ep 1 (Step 009150): Train loss 3.062, Val loss 3.475\n",
            "Ep 1 (Step 009175): Train loss 3.221, Val loss 3.423\n",
            "Ep 1 (Step 009200): Train loss 2.969, Val loss 3.407\n",
            "Ep 1 (Step 009225): Train loss 3.373, Val loss 3.483\n",
            "Ep 1 (Step 009250): Train loss 2.935, Val loss 3.484\n",
            "Ep 1 (Step 009275): Train loss 2.909, Val loss 3.443\n",
            "Ep 1 (Step 009300): Train loss 3.065, Val loss 3.445\n",
            "Ep 1 (Step 009325): Train loss 3.368, Val loss 3.464\n",
            "Ep 1 (Step 009350): Train loss 3.244, Val loss 3.412\n",
            "Ep 1 (Step 009375): Train loss 3.241, Val loss 3.451\n",
            "Ep 1 (Step 009400): Train loss 3.287, Val loss 3.413\n",
            "Ep 1 (Step 009425): Train loss 3.030, Val loss 3.444\n",
            "Ep 1 (Step 009450): Train loss 3.282, Val loss 3.428\n",
            "Ep 1 (Step 009475): Train loss 2.905, Val loss 3.442\n",
            "Ep 1 (Step 009500): Train loss 2.763, Val loss 3.452\n",
            "Ep 1 (Step 009525): Train loss 3.251, Val loss 3.402\n",
            "Ep 1 (Step 009550): Train loss 2.925, Val loss 3.410\n",
            "Ep 1 (Step 009575): Train loss 3.176, Val loss 3.454\n",
            "Ep 1 (Step 009600): Train loss 3.109, Val loss 3.530\n",
            "Ep 1 (Step 009625): Train loss 3.378, Val loss 3.393\n",
            "Ep 1 (Step 009650): Train loss 3.290, Val loss 3.387\n",
            "Ep 1 (Step 009675): Train loss 3.068, Val loss 3.420\n",
            "Ep 1 (Step 009700): Train loss 3.303, Val loss 3.379\n",
            "Ep 1 (Step 009725): Train loss 2.959, Val loss 3.367\n",
            "Ep 1 (Step 009750): Train loss 3.023, Val loss 3.365\n",
            "Ep 1 (Step 009775): Train loss 2.817, Val loss 3.432\n",
            "Ep 1 (Step 009800): Train loss 3.200, Val loss 3.395\n",
            "Ep 1 (Step 009825): Train loss 3.171, Val loss 3.394\n",
            "Ep 1 (Step 009850): Train loss 3.200, Val loss 3.337\n",
            "Ep 1 (Step 009875): Train loss 3.136, Val loss 3.366\n",
            "Ep 1 (Step 009900): Train loss 3.151, Val loss 3.380\n",
            "Ep 1 (Step 009925): Train loss 3.190, Val loss 3.359\n",
            "Ep 1 (Step 009950): Train loss 2.835, Val loss 3.397\n",
            "Ep 1 (Step 009975): Train loss 3.527, Val loss 3.340\n",
            "Ep 1 (Step 010000): Train loss 2.955, Val loss 3.396\n",
            "Ep 1 (Step 010025): Train loss 3.123, Val loss 3.379\n",
            "Ep 1 (Step 010050): Train loss 2.975, Val loss 3.387\n",
            "Ep 1 (Step 010075): Train loss 3.008, Val loss 3.393\n",
            "Ep 1 (Step 010100): Train loss 3.199, Val loss 3.369\n",
            "Ep 1 (Step 010125): Train loss 3.155, Val loss 3.377\n",
            "Ep 1 (Step 010150): Train loss 3.122, Val loss 3.334\n",
            "Ep 1 (Step 010175): Train loss 2.945, Val loss 3.375\n",
            "Ep 1 (Step 010200): Train loss 3.233, Val loss 3.381\n",
            "Ep 1 (Step 010225): Train loss 3.127, Val loss 3.354\n",
            "set()\n",
            "0\n",
            "0\n",
            "ဘာမှ မဖြစ်နိုင်တော့။ သူသည် သူ၏ အသံကို မသိမသာ ဖြစ်နေသည်။ သူသည် သူ၏ အသံကို အကဲခတ်ကြည့်သည်။ သူသည် သူသည်\n",
            "Ep 2 (Step 010250): Train loss 2.935, Val loss 3.403\n",
            "Ep 2 (Step 010275): Train loss 3.123, Val loss 3.391\n",
            "Ep 2 (Step 010300): Train loss 3.163, Val loss 3.344\n",
            "Ep 2 (Step 010325): Train loss 2.999, Val loss 3.450\n",
            "Ep 2 (Step 010350): Train loss 3.265, Val loss 3.376\n",
            "Ep 2 (Step 010375): Train loss 3.024, Val loss 3.367\n",
            "Ep 2 (Step 010400): Train loss 3.117, Val loss 3.332\n",
            "Ep 2 (Step 010425): Train loss 3.098, Val loss 3.344\n",
            "Ep 2 (Step 010450): Train loss 3.305, Val loss 3.323\n",
            "Ep 2 (Step 010475): Train loss 3.189, Val loss 3.337\n",
            "Ep 2 (Step 010500): Train loss 3.056, Val loss 3.362\n",
            "Ep 2 (Step 010525): Train loss 3.065, Val loss 3.318\n",
            "Ep 2 (Step 010550): Train loss 3.028, Val loss 3.336\n",
            "Ep 2 (Step 010575): Train loss 3.348, Val loss 3.374\n",
            "Ep 2 (Step 010600): Train loss 3.228, Val loss 3.360\n",
            "Ep 2 (Step 010625): Train loss 3.300, Val loss 3.396\n",
            "Ep 2 (Step 010650): Train loss 3.173, Val loss 3.291\n",
            "Ep 2 (Step 010675): Train loss 3.391, Val loss 3.340\n",
            "Ep 2 (Step 010700): Train loss 3.082, Val loss 3.285\n",
            "Ep 2 (Step 010725): Train loss 3.155, Val loss 3.313\n",
            "Ep 2 (Step 010750): Train loss 3.296, Val loss 3.303\n",
            "Ep 2 (Step 010775): Train loss 3.381, Val loss 3.290\n",
            "Ep 2 (Step 010800): Train loss 3.029, Val loss 3.292\n",
            "Ep 2 (Step 010825): Train loss 2.986, Val loss 3.346\n",
            "Ep 2 (Step 010850): Train loss 3.203, Val loss 3.345\n",
            "Ep 2 (Step 010875): Train loss 3.097, Val loss 3.323\n",
            "Ep 2 (Step 010900): Train loss 3.010, Val loss 3.337\n",
            "Ep 2 (Step 010925): Train loss 3.025, Val loss 3.324\n",
            "Ep 2 (Step 010950): Train loss 2.991, Val loss 3.333\n",
            "Ep 2 (Step 010975): Train loss 2.946, Val loss 3.377\n",
            "Ep 2 (Step 011000): Train loss 3.156, Val loss 3.362\n",
            "Ep 2 (Step 011025): Train loss 3.014, Val loss 3.379\n",
            "Ep 2 (Step 011050): Train loss 2.958, Val loss 3.335\n",
            "Ep 2 (Step 011075): Train loss 3.074, Val loss 3.325\n",
            "Ep 2 (Step 011100): Train loss 2.949, Val loss 3.322\n",
            "Ep 2 (Step 011125): Train loss 3.368, Val loss 3.371\n",
            "Ep 2 (Step 011150): Train loss 2.986, Val loss 3.374\n",
            "Ep 2 (Step 011175): Train loss 3.226, Val loss 3.329\n",
            "Ep 2 (Step 011200): Train loss 3.034, Val loss 3.322\n",
            "Ep 2 (Step 011225): Train loss 3.069, Val loss 3.339\n",
            "Ep 2 (Step 011250): Train loss 3.048, Val loss 3.344\n",
            "Ep 2 (Step 011275): Train loss 2.842, Val loss 3.354\n",
            "Ep 2 (Step 011300): Train loss 3.215, Val loss 3.371\n",
            "Ep 2 (Step 011325): Train loss 2.952, Val loss 3.338\n",
            "Ep 2 (Step 011350): Train loss 3.041, Val loss 3.360\n",
            "Ep 2 (Step 011375): Train loss 3.149, Val loss 3.374\n",
            "Ep 2 (Step 011400): Train loss 3.130, Val loss 3.336\n",
            "Ep 2 (Step 011425): Train loss 3.178, Val loss 3.331\n",
            "Ep 2 (Step 011450): Train loss 3.127, Val loss 3.292\n",
            "Ep 2 (Step 011475): Train loss 3.149, Val loss 3.325\n",
            "Ep 2 (Step 011500): Train loss 2.936, Val loss 3.291\n",
            "Ep 2 (Step 011525): Train loss 3.164, Val loss 3.292\n",
            "Ep 2 (Step 011550): Train loss 2.864, Val loss 3.317\n",
            "Ep 2 (Step 011575): Train loss 3.247, Val loss 3.266\n",
            "Ep 2 (Step 011600): Train loss 3.152, Val loss 3.326\n",
            "Ep 2 (Step 011625): Train loss 2.867, Val loss 3.377\n",
            "Ep 2 (Step 011650): Train loss 3.191, Val loss 3.332\n",
            "Ep 2 (Step 011675): Train loss 2.890, Val loss 3.342\n",
            "Ep 2 (Step 011700): Train loss 3.363, Val loss 3.370\n",
            "Ep 2 (Step 011725): Train loss 3.015, Val loss 3.325\n",
            "Ep 2 (Step 011750): Train loss 2.836, Val loss 3.334\n",
            "Ep 2 (Step 011775): Train loss 2.828, Val loss 3.322\n",
            "Ep 2 (Step 011800): Train loss 3.201, Val loss 3.365\n",
            "Ep 2 (Step 011825): Train loss 3.103, Val loss 3.370\n",
            "Ep 2 (Step 011850): Train loss 3.125, Val loss 3.356\n",
            "Ep 2 (Step 011875): Train loss 3.177, Val loss 3.317\n",
            "Ep 2 (Step 011900): Train loss 2.831, Val loss 3.333\n",
            "Ep 2 (Step 011925): Train loss 2.926, Val loss 3.259\n",
            "Ep 2 (Step 011950): Train loss 2.908, Val loss 3.303\n",
            "Ep 2 (Step 011975): Train loss 3.101, Val loss 3.340\n",
            "Ep 2 (Step 012000): Train loss 3.276, Val loss 3.268\n",
            "Ep 2 (Step 012025): Train loss 3.079, Val loss 3.266\n",
            "Ep 2 (Step 012050): Train loss 3.274, Val loss 3.270\n",
            "Ep 2 (Step 012075): Train loss 2.862, Val loss 3.315\n",
            "Ep 2 (Step 012100): Train loss 3.172, Val loss 3.285\n",
            "Ep 2 (Step 012125): Train loss 3.092, Val loss 3.281\n",
            "Ep 2 (Step 012150): Train loss 3.176, Val loss 3.311\n",
            "Ep 2 (Step 012175): Train loss 3.048, Val loss 3.320\n",
            "Ep 2 (Step 012200): Train loss 3.220, Val loss 3.322\n",
            "Ep 2 (Step 012225): Train loss 3.241, Val loss 3.318\n",
            "Ep 2 (Step 012250): Train loss 3.138, Val loss 3.286\n",
            "Ep 2 (Step 012275): Train loss 3.227, Val loss 3.291\n",
            "Ep 2 (Step 012300): Train loss 3.133, Val loss 3.265\n",
            "Ep 2 (Step 012325): Train loss 2.901, Val loss 3.285\n",
            "Ep 2 (Step 012350): Train loss 3.134, Val loss 3.309\n",
            "Ep 2 (Step 012375): Train loss 2.579, Val loss 3.264\n",
            "Ep 2 (Step 012400): Train loss 3.359, Val loss 3.280\n",
            "Ep 2 (Step 012425): Train loss 3.233, Val loss 3.272\n",
            "Ep 2 (Step 012450): Train loss 3.023, Val loss 3.272\n",
            "Ep 2 (Step 012475): Train loss 2.985, Val loss 3.332\n",
            "Ep 2 (Step 012500): Train loss 2.986, Val loss 3.306\n",
            "Ep 2 (Step 012525): Train loss 3.176, Val loss 3.288\n",
            "Ep 2 (Step 012550): Train loss 2.902, Val loss 3.289\n",
            "Ep 2 (Step 012575): Train loss 2.965, Val loss 3.325\n",
            "Ep 2 (Step 012600): Train loss 3.042, Val loss 3.238\n",
            "Ep 2 (Step 012625): Train loss 2.952, Val loss 3.322\n",
            "Ep 2 (Step 012650): Train loss 3.191, Val loss 3.286\n",
            "Ep 2 (Step 012675): Train loss 3.006, Val loss 3.301\n",
            "Ep 2 (Step 012700): Train loss 2.998, Val loss 3.275\n",
            "Ep 2 (Step 012725): Train loss 2.949, Val loss 3.305\n",
            "Ep 2 (Step 012750): Train loss 3.222, Val loss 3.270\n",
            "Ep 2 (Step 012775): Train loss 2.999, Val loss 3.276\n",
            "Ep 2 (Step 012800): Train loss 3.288, Val loss 3.285\n",
            "Ep 2 (Step 012825): Train loss 2.967, Val loss 3.318\n",
            "Ep 2 (Step 012850): Train loss 3.032, Val loss 3.239\n",
            "Ep 2 (Step 012875): Train loss 3.186, Val loss 3.244\n",
            "Ep 2 (Step 012900): Train loss 3.174, Val loss 3.224\n",
            "Ep 2 (Step 012925): Train loss 2.926, Val loss 3.242\n",
            "Ep 2 (Step 012950): Train loss 2.993, Val loss 3.236\n",
            "Ep 2 (Step 012975): Train loss 3.206, Val loss 3.223\n",
            "Ep 2 (Step 013000): Train loss 3.219, Val loss 3.211\n",
            "Ep 2 (Step 013025): Train loss 2.822, Val loss 3.251\n",
            "Ep 2 (Step 013050): Train loss 2.909, Val loss 3.264\n",
            "Ep 2 (Step 013075): Train loss 3.070, Val loss 3.256\n",
            "Ep 2 (Step 013100): Train loss 2.870, Val loss 3.224\n",
            "Ep 2 (Step 013125): Train loss 2.963, Val loss 3.274\n",
            "Ep 2 (Step 013150): Train loss 2.883, Val loss 3.281\n",
            "Ep 2 (Step 013175): Train loss 3.136, Val loss 3.265\n",
            "Ep 2 (Step 013200): Train loss 2.928, Val loss 3.245\n",
            "Ep 2 (Step 013225): Train loss 3.109, Val loss 3.283\n",
            "Ep 2 (Step 013250): Train loss 3.069, Val loss 3.281\n",
            "Ep 2 (Step 013275): Train loss 3.147, Val loss 3.249\n",
            "Ep 2 (Step 013300): Train loss 3.316, Val loss 3.293\n",
            "Ep 2 (Step 013325): Train loss 2.927, Val loss 3.256\n",
            "Ep 2 (Step 013350): Train loss 2.860, Val loss 3.235\n",
            "Ep 2 (Step 013375): Train loss 3.044, Val loss 3.244\n",
            "Ep 2 (Step 013400): Train loss 2.967, Val loss 3.238\n",
            "Ep 2 (Step 013425): Train loss 2.906, Val loss 3.237\n",
            "Ep 2 (Step 013450): Train loss 2.939, Val loss 3.212\n",
            "Ep 2 (Step 013475): Train loss 2.703, Val loss 3.225\n",
            "Ep 2 (Step 013500): Train loss 2.876, Val loss 3.218\n",
            "Ep 2 (Step 013525): Train loss 2.971, Val loss 3.264\n",
            "Ep 2 (Step 013550): Train loss 3.094, Val loss 3.244\n",
            "Ep 2 (Step 013575): Train loss 3.010, Val loss 3.334\n",
            "Ep 2 (Step 013600): Train loss 3.089, Val loss 3.346\n",
            "Ep 2 (Step 013625): Train loss 3.015, Val loss 3.261\n",
            "Ep 2 (Step 013650): Train loss 2.780, Val loss 3.235\n",
            "Ep 2 (Step 013675): Train loss 3.067, Val loss 3.255\n",
            "Ep 2 (Step 013700): Train loss 3.071, Val loss 3.259\n",
            "Ep 2 (Step 013725): Train loss 2.990, Val loss 3.300\n",
            "Ep 2 (Step 013750): Train loss 2.882, Val loss 3.246\n",
            "Ep 2 (Step 013775): Train loss 2.823, Val loss 3.265\n",
            "Ep 2 (Step 013800): Train loss 2.824, Val loss 3.238\n",
            "Ep 2 (Step 013825): Train loss 3.008, Val loss 3.265\n",
            "Ep 2 (Step 013850): Train loss 3.093, Val loss 3.239\n",
            "Ep 2 (Step 013875): Train loss 2.777, Val loss 3.206\n",
            "Ep 2 (Step 013900): Train loss 3.276, Val loss 3.229\n",
            "Ep 2 (Step 013925): Train loss 3.086, Val loss 3.256\n",
            "Ep 2 (Step 013950): Train loss 2.691, Val loss 3.204\n",
            "Ep 2 (Step 013975): Train loss 2.820, Val loss 3.258\n",
            "Ep 2 (Step 014000): Train loss 3.072, Val loss 3.222\n",
            "Ep 2 (Step 014025): Train loss 2.959, Val loss 3.194\n",
            "Ep 2 (Step 014050): Train loss 2.913, Val loss 3.215\n",
            "Ep 2 (Step 014075): Train loss 2.941, Val loss 3.224\n",
            "Ep 2 (Step 014100): Train loss 3.011, Val loss 3.267\n",
            "Ep 2 (Step 014125): Train loss 3.213, Val loss 3.285\n",
            "Ep 2 (Step 014150): Train loss 2.771, Val loss 3.244\n",
            "Ep 2 (Step 014175): Train loss 2.917, Val loss 3.202\n",
            "Ep 2 (Step 014200): Train loss 3.060, Val loss 3.261\n",
            "Ep 2 (Step 014225): Train loss 2.942, Val loss 3.226\n",
            "Ep 2 (Step 014250): Train loss 3.056, Val loss 3.281\n",
            "Ep 2 (Step 014275): Train loss 3.051, Val loss 3.249\n",
            "Ep 2 (Step 014300): Train loss 3.106, Val loss 3.290\n",
            "Ep 2 (Step 014325): Train loss 2.867, Val loss 3.223\n",
            "Ep 2 (Step 014350): Train loss 3.063, Val loss 3.266\n",
            "Ep 2 (Step 014375): Train loss 2.981, Val loss 3.269\n",
            "Ep 2 (Step 014400): Train loss 3.007, Val loss 3.231\n",
            "Ep 2 (Step 014425): Train loss 3.150, Val loss 3.240\n",
            "Ep 2 (Step 014450): Train loss 2.988, Val loss 3.224\n",
            "Ep 2 (Step 014475): Train loss 3.246, Val loss 3.216\n",
            "Ep 2 (Step 014500): Train loss 2.873, Val loss 3.241\n",
            "Ep 2 (Step 014525): Train loss 3.007, Val loss 3.172\n",
            "Ep 2 (Step 014550): Train loss 3.000, Val loss 3.205\n",
            "Ep 2 (Step 014575): Train loss 2.872, Val loss 3.182\n",
            "Ep 2 (Step 014600): Train loss 3.078, Val loss 3.185\n",
            "Ep 2 (Step 014625): Train loss 2.991, Val loss 3.221\n",
            "Ep 2 (Step 014650): Train loss 2.948, Val loss 3.178\n",
            "Ep 2 (Step 014675): Train loss 2.781, Val loss 3.152\n",
            "Ep 2 (Step 014700): Train loss 2.914, Val loss 3.175\n",
            "Ep 2 (Step 014725): Train loss 2.953, Val loss 3.201\n",
            "Ep 2 (Step 014750): Train loss 3.120, Val loss 3.153\n",
            "Ep 2 (Step 014775): Train loss 3.025, Val loss 3.172\n",
            "Ep 2 (Step 014800): Train loss 2.846, Val loss 3.161\n",
            "Ep 2 (Step 014825): Train loss 2.922, Val loss 3.168\n",
            "Ep 2 (Step 014850): Train loss 2.897, Val loss 3.206\n",
            "Ep 2 (Step 014875): Train loss 3.160, Val loss 3.224\n",
            "Ep 2 (Step 014900): Train loss 3.003, Val loss 3.207\n",
            "Ep 2 (Step 014925): Train loss 3.233, Val loss 3.207\n",
            "Ep 2 (Step 014950): Train loss 3.074, Val loss 3.231\n",
            "Ep 2 (Step 014975): Train loss 3.093, Val loss 3.259\n",
            "Ep 2 (Step 015000): Train loss 3.039, Val loss 3.183\n",
            "Ep 2 (Step 015025): Train loss 2.914, Val loss 3.162\n",
            "Ep 2 (Step 015050): Train loss 2.837, Val loss 3.172\n",
            "Ep 2 (Step 015075): Train loss 2.938, Val loss 3.154\n",
            "Ep 2 (Step 015100): Train loss 2.826, Val loss 3.168\n",
            "Ep 2 (Step 015125): Train loss 2.873, Val loss 3.195\n",
            "Ep 2 (Step 015150): Train loss 3.014, Val loss 3.125\n",
            "Ep 2 (Step 015175): Train loss 3.095, Val loss 3.138\n",
            "Ep 2 (Step 015200): Train loss 3.085, Val loss 3.168\n",
            "Ep 2 (Step 015225): Train loss 3.032, Val loss 3.167\n",
            "Ep 2 (Step 015250): Train loss 2.898, Val loss 3.197\n",
            "Ep 2 (Step 015275): Train loss 3.021, Val loss 3.144\n",
            "Ep 2 (Step 015300): Train loss 2.847, Val loss 3.149\n",
            "Ep 2 (Step 015325): Train loss 2.952, Val loss 3.206\n",
            "Ep 2 (Step 015350): Train loss 3.010, Val loss 3.141\n",
            "Ep 2 (Step 015375): Train loss 2.894, Val loss 3.121\n",
            "Ep 2 (Step 015400): Train loss 3.234, Val loss 3.148\n",
            "Ep 2 (Step 015425): Train loss 2.988, Val loss 3.145\n",
            "Ep 2 (Step 015450): Train loss 2.909, Val loss 3.148\n",
            "Ep 2 (Step 015475): Train loss 3.271, Val loss 3.164\n",
            "Ep 2 (Step 015500): Train loss 3.080, Val loss 3.139\n",
            "Ep 2 (Step 015525): Train loss 3.000, Val loss 3.171\n",
            "Ep 2 (Step 015550): Train loss 3.108, Val loss 3.218\n",
            "Ep 2 (Step 015575): Train loss 3.364, Val loss 3.238\n",
            "Ep 2 (Step 015600): Train loss 2.878, Val loss 3.198\n",
            "Ep 2 (Step 015625): Train loss 2.731, Val loss 3.217\n",
            "Ep 2 (Step 015650): Train loss 2.924, Val loss 3.254\n",
            "Ep 2 (Step 015675): Train loss 2.991, Val loss 3.209\n",
            "Ep 2 (Step 015700): Train loss 2.823, Val loss 3.191\n",
            "Ep 2 (Step 015725): Train loss 2.742, Val loss 3.202\n",
            "Ep 2 (Step 015750): Train loss 2.962, Val loss 3.147\n",
            "Ep 2 (Step 015775): Train loss 3.107, Val loss 3.165\n",
            "Ep 2 (Step 015800): Train loss 3.021, Val loss 3.204\n",
            "Ep 2 (Step 015825): Train loss 2.968, Val loss 3.137\n",
            "Ep 2 (Step 015850): Train loss 3.051, Val loss 3.143\n",
            "Ep 2 (Step 015875): Train loss 2.854, Val loss 3.172\n",
            "Ep 2 (Step 015900): Train loss 2.862, Val loss 3.195\n",
            "Ep 2 (Step 015925): Train loss 2.650, Val loss 3.217\n",
            "Ep 2 (Step 015950): Train loss 2.881, Val loss 3.153\n",
            "Ep 2 (Step 015975): Train loss 3.149, Val loss 3.245\n",
            "Ep 2 (Step 016000): Train loss 2.889, Val loss 3.167\n",
            "Ep 2 (Step 016025): Train loss 2.930, Val loss 3.204\n",
            "Ep 2 (Step 016050): Train loss 2.819, Val loss 3.247\n",
            "Ep 2 (Step 016075): Train loss 3.020, Val loss 3.262\n",
            "Ep 2 (Step 016100): Train loss 2.962, Val loss 3.207\n",
            "Ep 2 (Step 016125): Train loss 3.004, Val loss 3.207\n",
            "Ep 2 (Step 016150): Train loss 2.933, Val loss 3.262\n",
            "Ep 2 (Step 016175): Train loss 2.842, Val loss 3.191\n",
            "Ep 2 (Step 016200): Train loss 2.999, Val loss 3.230\n",
            "Ep 2 (Step 016225): Train loss 3.068, Val loss 3.220\n",
            "Ep 2 (Step 016250): Train loss 3.050, Val loss 3.201\n",
            "Ep 2 (Step 016275): Train loss 2.920, Val loss 3.206\n",
            "Ep 2 (Step 016300): Train loss 2.944, Val loss 3.246\n",
            "Ep 2 (Step 016325): Train loss 2.964, Val loss 3.233\n",
            "Ep 2 (Step 016350): Train loss 2.857, Val loss 3.194\n",
            "Ep 2 (Step 016375): Train loss 2.676, Val loss 3.195\n",
            "Ep 2 (Step 016400): Train loss 3.126, Val loss 3.258\n",
            "Ep 2 (Step 016425): Train loss 2.808, Val loss 3.252\n",
            "Ep 2 (Step 016450): Train loss 2.821, Val loss 3.197\n",
            "Ep 2 (Step 016475): Train loss 2.936, Val loss 3.245\n",
            "Ep 2 (Step 016500): Train loss 3.020, Val loss 3.211\n",
            "Ep 2 (Step 016525): Train loss 2.783, Val loss 3.193\n",
            "Ep 2 (Step 016550): Train loss 2.855, Val loss 3.201\n",
            "Ep 2 (Step 016575): Train loss 2.748, Val loss 3.203\n",
            "Ep 2 (Step 016600): Train loss 2.769, Val loss 3.221\n",
            "Ep 2 (Step 016625): Train loss 2.952, Val loss 3.204\n",
            "Ep 2 (Step 016650): Train loss 2.884, Val loss 3.203\n",
            "Ep 2 (Step 016675): Train loss 3.038, Val loss 3.261\n",
            "Ep 2 (Step 016700): Train loss 3.052, Val loss 3.219\n",
            "Ep 2 (Step 016725): Train loss 2.833, Val loss 3.195\n",
            "Ep 2 (Step 016750): Train loss 3.128, Val loss 3.196\n",
            "Ep 2 (Step 016775): Train loss 3.046, Val loss 3.230\n",
            "Ep 2 (Step 016800): Train loss 2.957, Val loss 3.199\n",
            "Ep 2 (Step 016825): Train loss 2.884, Val loss 3.199\n",
            "Ep 2 (Step 016850): Train loss 2.928, Val loss 3.253\n",
            "Ep 2 (Step 016875): Train loss 2.985, Val loss 3.243\n",
            "Ep 2 (Step 016900): Train loss 3.145, Val loss 3.217\n",
            "Ep 2 (Step 016925): Train loss 2.968, Val loss 3.203\n",
            "Ep 2 (Step 016950): Train loss 2.988, Val loss 3.202\n",
            "Ep 2 (Step 016975): Train loss 3.173, Val loss 3.213\n",
            "Ep 2 (Step 017000): Train loss 2.960, Val loss 3.219\n",
            "Ep 2 (Step 017025): Train loss 2.839, Val loss 3.201\n",
            "Ep 2 (Step 017050): Train loss 2.996, Val loss 3.203\n",
            "Ep 2 (Step 017075): Train loss 2.957, Val loss 3.195\n",
            "Ep 2 (Step 017100): Train loss 3.039, Val loss 3.229\n",
            "Ep 2 (Step 017125): Train loss 2.762, Val loss 3.218\n",
            "Ep 2 (Step 017150): Train loss 3.003, Val loss 3.235\n",
            "Ep 2 (Step 017175): Train loss 2.821, Val loss 3.177\n",
            "Ep 2 (Step 017200): Train loss 3.142, Val loss 3.225\n",
            "Ep 2 (Step 017225): Train loss 3.062, Val loss 3.174\n",
            "Ep 2 (Step 017250): Train loss 2.860, Val loss 3.204\n",
            "Ep 2 (Step 017275): Train loss 2.752, Val loss 3.199\n",
            "Ep 2 (Step 017300): Train loss 2.973, Val loss 3.179\n",
            "Ep 2 (Step 017325): Train loss 2.805, Val loss 3.174\n",
            "Ep 2 (Step 017350): Train loss 2.849, Val loss 3.186\n",
            "Ep 2 (Step 017375): Train loss 2.783, Val loss 3.145\n",
            "Ep 2 (Step 017400): Train loss 2.759, Val loss 3.153\n",
            "Ep 2 (Step 017425): Train loss 3.120, Val loss 3.156\n",
            "Ep 2 (Step 017450): Train loss 2.949, Val loss 3.186\n",
            "Ep 2 (Step 017475): Train loss 2.789, Val loss 3.186\n",
            "Ep 2 (Step 017500): Train loss 2.819, Val loss 3.177\n",
            "Ep 2 (Step 017525): Train loss 2.727, Val loss 3.128\n",
            "Ep 2 (Step 017550): Train loss 2.842, Val loss 3.154\n",
            "Ep 2 (Step 017575): Train loss 2.947, Val loss 3.145\n",
            "Ep 2 (Step 017600): Train loss 2.882, Val loss 3.176\n",
            "Ep 2 (Step 017625): Train loss 2.785, Val loss 3.184\n",
            "Ep 2 (Step 017650): Train loss 2.920, Val loss 3.169\n",
            "Ep 2 (Step 017675): Train loss 3.029, Val loss 3.180\n",
            "Ep 2 (Step 017700): Train loss 2.765, Val loss 3.137\n",
            "Ep 2 (Step 017725): Train loss 2.615, Val loss 3.171\n",
            "Ep 2 (Step 017750): Train loss 2.955, Val loss 3.141\n",
            "Ep 2 (Step 017775): Train loss 2.754, Val loss 3.142\n",
            "Ep 2 (Step 017800): Train loss 2.896, Val loss 3.172\n",
            "Ep 2 (Step 017825): Train loss 2.671, Val loss 3.185\n",
            "Ep 2 (Step 017850): Train loss 2.978, Val loss 3.160\n",
            "Ep 2 (Step 017875): Train loss 2.788, Val loss 3.168\n",
            "Ep 2 (Step 017900): Train loss 3.023, Val loss 3.212\n",
            "Ep 2 (Step 017925): Train loss 3.216, Val loss 3.156\n",
            "Ep 2 (Step 017950): Train loss 2.758, Val loss 3.172\n",
            "Ep 2 (Step 017975): Train loss 2.737, Val loss 3.155\n",
            "Ep 2 (Step 018000): Train loss 2.980, Val loss 3.147\n",
            "Ep 2 (Step 018025): Train loss 2.915, Val loss 3.178\n",
            "Ep 2 (Step 018050): Train loss 3.034, Val loss 3.179\n",
            "Ep 2 (Step 018075): Train loss 2.954, Val loss 3.151\n",
            "Ep 2 (Step 018100): Train loss 3.007, Val loss 3.191\n",
            "Ep 2 (Step 018125): Train loss 2.813, Val loss 3.181\n",
            "Ep 2 (Step 018150): Train loss 3.159, Val loss 3.164\n",
            "Ep 2 (Step 018175): Train loss 3.092, Val loss 3.185\n",
            "Ep 2 (Step 018200): Train loss 3.066, Val loss 3.203\n",
            "Ep 2 (Step 018225): Train loss 3.084, Val loss 3.172\n",
            "Ep 2 (Step 018250): Train loss 2.755, Val loss 3.234\n",
            "Ep 2 (Step 018275): Train loss 2.837, Val loss 3.186\n",
            "Ep 2 (Step 018300): Train loss 2.899, Val loss 3.207\n",
            "Ep 2 (Step 018325): Train loss 2.842, Val loss 3.184\n",
            "Ep 2 (Step 018350): Train loss 3.033, Val loss 3.228\n",
            "Ep 2 (Step 018375): Train loss 2.879, Val loss 3.192\n",
            "Ep 2 (Step 018400): Train loss 2.963, Val loss 3.192\n",
            "Ep 2 (Step 018425): Train loss 2.957, Val loss 3.214\n",
            "Ep 2 (Step 018450): Train loss 2.918, Val loss 3.199\n",
            "Ep 2 (Step 018475): Train loss 3.070, Val loss 3.163\n",
            "Ep 2 (Step 018500): Train loss 3.062, Val loss 3.190\n",
            "Ep 2 (Step 018525): Train loss 2.761, Val loss 3.165\n",
            "Ep 2 (Step 018550): Train loss 2.540, Val loss 3.183\n",
            "Ep 2 (Step 018575): Train loss 3.184, Val loss 3.188\n",
            "Ep 2 (Step 018600): Train loss 2.914, Val loss 3.212\n",
            "Ep 2 (Step 018625): Train loss 2.986, Val loss 3.212\n",
            "Ep 2 (Step 018650): Train loss 2.990, Val loss 3.215\n",
            "Ep 2 (Step 018675): Train loss 2.636, Val loss 3.210\n",
            "Ep 2 (Step 018700): Train loss 2.723, Val loss 3.182\n",
            "Ep 2 (Step 018725): Train loss 2.787, Val loss 3.215\n",
            "Ep 2 (Step 018750): Train loss 2.875, Val loss 3.156\n",
            "Ep 2 (Step 018775): Train loss 2.869, Val loss 3.186\n",
            "Ep 2 (Step 018800): Train loss 2.894, Val loss 3.181\n",
            "Ep 2 (Step 018825): Train loss 2.926, Val loss 3.188\n",
            "Ep 2 (Step 018850): Train loss 2.910, Val loss 3.230\n",
            "Ep 2 (Step 018875): Train loss 2.944, Val loss 3.262\n",
            "Ep 2 (Step 018900): Train loss 2.727, Val loss 3.181\n",
            "Ep 2 (Step 018925): Train loss 3.115, Val loss 3.172\n",
            "Ep 2 (Step 018950): Train loss 2.690, Val loss 3.235\n",
            "Ep 2 (Step 018975): Train loss 3.014, Val loss 3.134\n",
            "Ep 2 (Step 019000): Train loss 2.892, Val loss 3.135\n",
            "Ep 2 (Step 019025): Train loss 2.831, Val loss 3.150\n",
            "Ep 2 (Step 019050): Train loss 2.833, Val loss 3.119\n",
            "Ep 2 (Step 019075): Train loss 2.856, Val loss 3.143\n",
            "Ep 2 (Step 019100): Train loss 2.927, Val loss 3.172\n",
            "Ep 2 (Step 019125): Train loss 2.982, Val loss 3.123\n",
            "Ep 2 (Step 019150): Train loss 3.129, Val loss 3.094\n",
            "Ep 2 (Step 019175): Train loss 2.822, Val loss 3.105\n",
            "Ep 2 (Step 019200): Train loss 3.006, Val loss 3.075\n",
            "Ep 2 (Step 019225): Train loss 2.672, Val loss 3.145\n",
            "Ep 2 (Step 019250): Train loss 3.008, Val loss 3.125\n",
            "Ep 2 (Step 019275): Train loss 2.827, Val loss 3.120\n",
            "Ep 2 (Step 019300): Train loss 2.636, Val loss 3.115\n",
            "Ep 2 (Step 019325): Train loss 2.943, Val loss 3.108\n",
            "Ep 2 (Step 019350): Train loss 2.955, Val loss 3.145\n",
            "Ep 2 (Step 019375): Train loss 2.896, Val loss 3.135\n",
            "Ep 2 (Step 019400): Train loss 2.786, Val loss 3.144\n",
            "Ep 2 (Step 019425): Train loss 2.841, Val loss 3.138\n",
            "Ep 2 (Step 019450): Train loss 2.825, Val loss 3.155\n",
            "Ep 2 (Step 019475): Train loss 2.912, Val loss 3.200\n",
            "Ep 2 (Step 019500): Train loss 2.987, Val loss 3.237\n",
            "Ep 2 (Step 019525): Train loss 2.831, Val loss 3.206\n",
            "Ep 2 (Step 019550): Train loss 2.855, Val loss 3.165\n",
            "Ep 2 (Step 019575): Train loss 2.911, Val loss 3.156\n",
            "Ep 2 (Step 019600): Train loss 2.733, Val loss 3.136\n",
            "Ep 2 (Step 019625): Train loss 3.047, Val loss 3.160\n",
            "Ep 2 (Step 019650): Train loss 2.763, Val loss 3.128\n",
            "Ep 2 (Step 019675): Train loss 2.806, Val loss 3.154\n",
            "Ep 2 (Step 019700): Train loss 2.715, Val loss 3.149\n",
            "Ep 2 (Step 019725): Train loss 2.716, Val loss 3.128\n",
            "Ep 2 (Step 019750): Train loss 2.677, Val loss 3.143\n",
            "Ep 2 (Step 019775): Train loss 2.705, Val loss 3.131\n",
            "Ep 2 (Step 019800): Train loss 2.722, Val loss 3.153\n",
            "Ep 2 (Step 019825): Train loss 3.044, Val loss 3.135\n",
            "Ep 2 (Step 019850): Train loss 2.902, Val loss 3.187\n",
            "Ep 2 (Step 019875): Train loss 3.074, Val loss 3.148\n",
            "Ep 2 (Step 019900): Train loss 2.911, Val loss 3.179\n",
            "Ep 2 (Step 019925): Train loss 2.929, Val loss 3.137\n",
            "Ep 2 (Step 019950): Train loss 2.665, Val loss 3.159\n",
            "Ep 2 (Step 019975): Train loss 3.015, Val loss 3.100\n",
            "Ep 2 (Step 020000): Train loss 2.980, Val loss 3.123\n",
            "Ep 2 (Step 020025): Train loss 2.944, Val loss 3.119\n",
            "Ep 2 (Step 020050): Train loss 2.905, Val loss 3.124\n",
            "Ep 2 (Step 020075): Train loss 2.720, Val loss 3.151\n",
            "Ep 2 (Step 020100): Train loss 2.858, Val loss 3.158\n",
            "Ep 2 (Step 020125): Train loss 2.981, Val loss 3.179\n",
            "Ep 2 (Step 020150): Train loss 2.946, Val loss 3.198\n",
            "Ep 2 (Step 020175): Train loss 2.789, Val loss 3.162\n",
            "Ep 2 (Step 020200): Train loss 2.933, Val loss 3.201\n",
            "Ep 2 (Step 020225): Train loss 2.797, Val loss 3.165\n",
            "Ep 2 (Step 020250): Train loss 3.012, Val loss 3.148\n",
            "Ep 2 (Step 020275): Train loss 2.974, Val loss 3.145\n",
            "Ep 2 (Step 020300): Train loss 2.892, Val loss 3.128\n",
            "Ep 2 (Step 020325): Train loss 2.849, Val loss 3.140\n",
            "Ep 2 (Step 020350): Train loss 2.967, Val loss 3.156\n",
            "Ep 2 (Step 020375): Train loss 2.879, Val loss 3.137\n",
            "Ep 2 (Step 020400): Train loss 2.884, Val loss 3.182\n",
            "Ep 2 (Step 020425): Train loss 2.638, Val loss 3.155\n",
            "Ep 2 (Step 020450): Train loss 3.035, Val loss 3.171\n",
            "Ep 2 (Step 020475): Train loss 2.877, Val loss 3.202\n",
            "set()\n",
            "0\n",
            "0\n",
            "ဘာလုပ်သလဲ”“ဒါပေမဲ့ ဒီလို အလုပ်လုပ်ရမယ်၊ ဒီလို အလုပ်လုပ်ရမယ်၊ ဒီလို အလုပ်လုပ်ရမယ်၊ ဒီလို အလုပ်လုပ်ရမယ်၊ အ\n",
            "Ep 3 (Step 020500): Train loss 2.704, Val loss 3.179\n",
            "Ep 3 (Step 020525): Train loss 2.754, Val loss 3.179\n",
            "Ep 3 (Step 020550): Train loss 2.918, Val loss 3.115\n",
            "Ep 3 (Step 020575): Train loss 2.830, Val loss 3.132\n",
            "Ep 3 (Step 020600): Train loss 2.839, Val loss 3.158\n",
            "Ep 3 (Step 020625): Train loss 2.930, Val loss 3.202\n",
            "Ep 3 (Step 020650): Train loss 2.966, Val loss 3.172\n",
            "Ep 3 (Step 020675): Train loss 2.841, Val loss 3.188\n",
            "Ep 3 (Step 020700): Train loss 2.907, Val loss 3.131\n",
            "Ep 3 (Step 020725): Train loss 2.873, Val loss 3.204\n",
            "Ep 3 (Step 020750): Train loss 2.921, Val loss 3.179\n",
            "Ep 3 (Step 020775): Train loss 3.023, Val loss 3.168\n",
            "Ep 3 (Step 020800): Train loss 2.899, Val loss 3.177\n",
            "Ep 3 (Step 020825): Train loss 2.801, Val loss 3.155\n",
            "Ep 3 (Step 020850): Train loss 2.785, Val loss 3.151\n",
            "Ep 3 (Step 020875): Train loss 2.819, Val loss 3.155\n",
            "Ep 3 (Step 020900): Train loss 3.090, Val loss 3.140\n",
            "Ep 3 (Step 020925): Train loss 2.938, Val loss 3.159\n",
            "Ep 3 (Step 020950): Train loss 2.936, Val loss 3.128\n",
            "Ep 3 (Step 020975): Train loss 2.914, Val loss 3.162\n",
            "Ep 3 (Step 021000): Train loss 2.716, Val loss 3.153\n",
            "Ep 3 (Step 021025): Train loss 2.883, Val loss 3.160\n",
            "Ep 3 (Step 021050): Train loss 2.832, Val loss 3.143\n",
            "Ep 3 (Step 021075): Train loss 3.077, Val loss 3.184\n",
            "Ep 3 (Step 021100): Train loss 2.744, Val loss 3.177\n",
            "Ep 3 (Step 021125): Train loss 2.914, Val loss 3.141\n",
            "Ep 3 (Step 021150): Train loss 2.796, Val loss 3.163\n",
            "Ep 3 (Step 021175): Train loss 2.864, Val loss 3.128\n",
            "Ep 3 (Step 021200): Train loss 2.966, Val loss 3.155\n",
            "Ep 3 (Step 021225): Train loss 2.771, Val loss 3.144\n",
            "Ep 3 (Step 021250): Train loss 2.788, Val loss 3.153\n",
            "Ep 3 (Step 021275): Train loss 3.150, Val loss 3.161\n",
            "Ep 3 (Step 021300): Train loss 2.770, Val loss 3.139\n",
            "Ep 3 (Step 021325): Train loss 2.929, Val loss 3.154\n",
            "Ep 3 (Step 021350): Train loss 2.923, Val loss 3.147\n",
            "Ep 3 (Step 021375): Train loss 2.744, Val loss 3.143\n",
            "Ep 3 (Step 021400): Train loss 2.567, Val loss 3.137\n",
            "Ep 3 (Step 021425): Train loss 2.735, Val loss 3.170\n",
            "Ep 3 (Step 021450): Train loss 2.797, Val loss 3.164\n",
            "Ep 3 (Step 021475): Train loss 3.084, Val loss 3.134\n",
            "Ep 3 (Step 021500): Train loss 2.889, Val loss 3.187\n",
            "Ep 3 (Step 021525): Train loss 3.107, Val loss 3.188\n",
            "Ep 3 (Step 021550): Train loss 2.842, Val loss 3.184\n",
            "Ep 3 (Step 021575): Train loss 2.860, Val loss 3.172\n",
            "Ep 3 (Step 021600): Train loss 2.971, Val loss 3.181\n",
            "Ep 3 (Step 021625): Train loss 2.716, Val loss 3.183\n",
            "Ep 3 (Step 021650): Train loss 2.691, Val loss 3.208\n",
            "Ep 3 (Step 021675): Train loss 2.865, Val loss 3.204\n",
            "Ep 3 (Step 021700): Train loss 2.910, Val loss 3.162\n",
            "Ep 3 (Step 021725): Train loss 2.972, Val loss 3.165\n",
            "Ep 3 (Step 021750): Train loss 2.768, Val loss 3.190\n",
            "Ep 3 (Step 021775): Train loss 2.745, Val loss 3.209\n",
            "Ep 3 (Step 021800): Train loss 2.934, Val loss 3.193\n",
            "Ep 3 (Step 021825): Train loss 2.941, Val loss 3.217\n",
            "Ep 3 (Step 021850): Train loss 2.786, Val loss 3.176\n",
            "Ep 3 (Step 021875): Train loss 2.878, Val loss 3.185\n",
            "Ep 3 (Step 021900): Train loss 2.859, Val loss 3.171\n",
            "Ep 3 (Step 021925): Train loss 2.574, Val loss 3.177\n",
            "Ep 3 (Step 021950): Train loss 2.736, Val loss 3.132\n",
            "Ep 3 (Step 021975): Train loss 3.054, Val loss 3.129\n",
            "Ep 3 (Step 022000): Train loss 2.731, Val loss 3.160\n",
            "Ep 3 (Step 022025): Train loss 2.830, Val loss 3.156\n",
            "Ep 3 (Step 022050): Train loss 2.906, Val loss 3.141\n",
            "Ep 3 (Step 022075): Train loss 2.721, Val loss 3.164\n",
            "Ep 3 (Step 022100): Train loss 2.793, Val loss 3.173\n",
            "Ep 3 (Step 022125): Train loss 2.939, Val loss 3.184\n",
            "Ep 3 (Step 022150): Train loss 2.961, Val loss 3.172\n",
            "Ep 3 (Step 022175): Train loss 2.655, Val loss 3.160\n",
            "Ep 3 (Step 022200): Train loss 2.813, Val loss 3.177\n",
            "Ep 3 (Step 022225): Train loss 2.851, Val loss 3.141\n",
            "Ep 3 (Step 022250): Train loss 2.827, Val loss 3.147\n",
            "Ep 3 (Step 022275): Train loss 2.987, Val loss 3.121\n",
            "Ep 3 (Step 022300): Train loss 2.812, Val loss 3.155\n",
            "Ep 3 (Step 022325): Train loss 2.896, Val loss 3.130\n",
            "Ep 3 (Step 022350): Train loss 2.822, Val loss 3.133\n",
            "Ep 3 (Step 022375): Train loss 2.917, Val loss 3.120\n",
            "Ep 3 (Step 022400): Train loss 2.956, Val loss 3.139\n",
            "Ep 3 (Step 022425): Train loss 2.999, Val loss 3.147\n",
            "Ep 3 (Step 022450): Train loss 2.732, Val loss 3.144\n",
            "Ep 3 (Step 022475): Train loss 2.823, Val loss 3.124\n",
            "Ep 3 (Step 022500): Train loss 3.021, Val loss 3.119\n",
            "Ep 3 (Step 022525): Train loss 2.794, Val loss 3.131\n",
            "Ep 3 (Step 022550): Train loss 2.776, Val loss 3.115\n",
            "Ep 3 (Step 022575): Train loss 2.622, Val loss 3.140\n",
            "Ep 3 (Step 022600): Train loss 2.799, Val loss 3.194\n",
            "Ep 3 (Step 022625): Train loss 2.884, Val loss 3.130\n",
            "Ep 3 (Step 022650): Train loss 3.115, Val loss 3.138\n",
            "Ep 3 (Step 022675): Train loss 2.838, Val loss 3.157\n",
            "Ep 3 (Step 022700): Train loss 2.911, Val loss 3.138\n",
            "Ep 3 (Step 022725): Train loss 2.736, Val loss 3.143\n",
            "Ep 3 (Step 022750): Train loss 2.678, Val loss 3.124\n",
            "Ep 3 (Step 022775): Train loss 2.753, Val loss 3.080\n",
            "Ep 3 (Step 022800): Train loss 2.846, Val loss 3.138\n",
            "Ep 3 (Step 022825): Train loss 2.628, Val loss 3.116\n",
            "Ep 3 (Step 022850): Train loss 2.892, Val loss 3.129\n",
            "Ep 3 (Step 022875): Train loss 2.812, Val loss 3.134\n",
            "Ep 3 (Step 022900): Train loss 2.963, Val loss 3.142\n",
            "Ep 3 (Step 022925): Train loss 2.816, Val loss 3.106\n",
            "Ep 3 (Step 022950): Train loss 2.759, Val loss 3.133\n",
            "Ep 3 (Step 022975): Train loss 2.729, Val loss 3.099\n",
            "Ep 3 (Step 023000): Train loss 2.607, Val loss 3.122\n",
            "Ep 3 (Step 023025): Train loss 3.143, Val loss 3.084\n",
            "Ep 3 (Step 023050): Train loss 2.768, Val loss 3.101\n",
            "Ep 3 (Step 023075): Train loss 2.953, Val loss 3.096\n",
            "Ep 3 (Step 023100): Train loss 2.847, Val loss 3.179\n",
            "Ep 3 (Step 023125): Train loss 2.966, Val loss 3.140\n",
            "Ep 3 (Step 023150): Train loss 3.011, Val loss 3.117\n",
            "Ep 3 (Step 023175): Train loss 3.070, Val loss 3.127\n",
            "Ep 3 (Step 023200): Train loss 2.806, Val loss 3.086\n",
            "Ep 3 (Step 023225): Train loss 2.760, Val loss 3.100\n",
            "Ep 3 (Step 023250): Train loss 2.757, Val loss 3.106\n",
            "Ep 3 (Step 023275): Train loss 2.629, Val loss 3.088\n",
            "Ep 3 (Step 023300): Train loss 2.875, Val loss 3.051\n",
            "Ep 3 (Step 023325): Train loss 2.604, Val loss 3.106\n",
            "Ep 3 (Step 023350): Train loss 2.635, Val loss 3.098\n",
            "Ep 3 (Step 023375): Train loss 2.660, Val loss 3.107\n",
            "Ep 3 (Step 023400): Train loss 2.783, Val loss 3.093\n",
            "Ep 3 (Step 023425): Train loss 2.909, Val loss 3.058\n",
            "Ep 3 (Step 023450): Train loss 2.602, Val loss 3.086\n",
            "Ep 3 (Step 023475): Train loss 2.598, Val loss 3.096\n",
            "Ep 3 (Step 023500): Train loss 3.001, Val loss 3.094\n",
            "Ep 3 (Step 023525): Train loss 2.954, Val loss 3.114\n",
            "Ep 3 (Step 023550): Train loss 2.747, Val loss 3.055\n",
            "Ep 3 (Step 023575): Train loss 2.899, Val loss 3.110\n",
            "Ep 3 (Step 023600): Train loss 2.743, Val loss 3.139\n",
            "Ep 3 (Step 023625): Train loss 2.866, Val loss 3.109\n",
            "Ep 3 (Step 023650): Train loss 3.026, Val loss 3.140\n",
            "Ep 3 (Step 023675): Train loss 2.753, Val loss 3.146\n",
            "Ep 3 (Step 023700): Train loss 2.755, Val loss 3.084\n",
            "Ep 3 (Step 023725): Train loss 2.765, Val loss 3.077\n",
            "Ep 3 (Step 023750): Train loss 2.598, Val loss 3.096\n",
            "Ep 3 (Step 023775): Train loss 3.086, Val loss 3.102\n",
            "Ep 3 (Step 023800): Train loss 2.797, Val loss 3.136\n",
            "Ep 3 (Step 023825): Train loss 2.739, Val loss 3.116\n",
            "Ep 3 (Step 023850): Train loss 2.969, Val loss 3.088\n",
            "Ep 3 (Step 023875): Train loss 2.689, Val loss 3.069\n",
            "Ep 3 (Step 023900): Train loss 2.840, Val loss 3.049\n",
            "Ep 3 (Step 023925): Train loss 3.080, Val loss 3.081\n",
            "Ep 3 (Step 023950): Train loss 3.072, Val loss 3.101\n",
            "Ep 3 (Step 023975): Train loss 2.794, Val loss 3.102\n",
            "Ep 3 (Step 024000): Train loss 2.669, Val loss 3.075\n",
            "Ep 3 (Step 024025): Train loss 2.882, Val loss 3.127\n",
            "Ep 3 (Step 024050): Train loss 2.786, Val loss 3.141\n",
            "Ep 3 (Step 024075): Train loss 2.815, Val loss 3.087\n",
            "Ep 3 (Step 024100): Train loss 2.892, Val loss 3.085\n",
            "Ep 3 (Step 024125): Train loss 2.870, Val loss 3.062\n",
            "Ep 3 (Step 024150): Train loss 2.943, Val loss 3.042\n",
            "Ep 3 (Step 024175): Train loss 2.930, Val loss 3.061\n",
            "Ep 3 (Step 024200): Train loss 3.002, Val loss 3.054\n",
            "Ep 3 (Step 024225): Train loss 2.881, Val loss 3.085\n",
            "Ep 3 (Step 024250): Train loss 2.900, Val loss 3.061\n",
            "Ep 3 (Step 024275): Train loss 3.006, Val loss 3.048\n",
            "Ep 3 (Step 024300): Train loss 2.710, Val loss 3.049\n",
            "Ep 3 (Step 024325): Train loss 2.944, Val loss 3.067\n",
            "Ep 3 (Step 024350): Train loss 2.743, Val loss 3.105\n",
            "Ep 3 (Step 024375): Train loss 2.798, Val loss 3.091\n",
            "Ep 3 (Step 024400): Train loss 2.931, Val loss 3.107\n",
            "Ep 3 (Step 024425): Train loss 2.625, Val loss 3.138\n",
            "Ep 3 (Step 024450): Train loss 2.857, Val loss 3.077\n",
            "Ep 3 (Step 024475): Train loss 2.539, Val loss 3.119\n",
            "Ep 3 (Step 024500): Train loss 2.817, Val loss 3.150\n",
            "Ep 3 (Step 024525): Train loss 2.788, Val loss 3.114\n",
            "Ep 3 (Step 024550): Train loss 2.830, Val loss 3.087\n",
            "Ep 3 (Step 024575): Train loss 2.783, Val loss 3.108\n",
            "Ep 3 (Step 024600): Train loss 2.849, Val loss 3.089\n",
            "Ep 3 (Step 024625): Train loss 2.738, Val loss 3.110\n",
            "Ep 3 (Step 024650): Train loss 2.866, Val loss 3.115\n",
            "Ep 3 (Step 024675): Train loss 2.842, Val loss 3.086\n",
            "Ep 3 (Step 024700): Train loss 3.013, Val loss 3.137\n",
            "Ep 3 (Step 024725): Train loss 2.783, Val loss 3.087\n",
            "Ep 3 (Step 024750): Train loss 2.933, Val loss 3.131\n",
            "Ep 3 (Step 024775): Train loss 2.791, Val loss 3.077\n",
            "Ep 3 (Step 024800): Train loss 2.644, Val loss 3.075\n",
            "Ep 3 (Step 024825): Train loss 2.547, Val loss 3.087\n",
            "Ep 3 (Step 024850): Train loss 2.919, Val loss 3.078\n",
            "Ep 3 (Step 024875): Train loss 2.797, Val loss 3.154\n",
            "Ep 3 (Step 024900): Train loss 2.895, Val loss 3.120\n",
            "Ep 3 (Step 024925): Train loss 2.752, Val loss 3.144\n",
            "Ep 3 (Step 024950): Train loss 2.906, Val loss 3.070\n",
            "Ep 3 (Step 024975): Train loss 2.691, Val loss 3.083\n",
            "Ep 3 (Step 025000): Train loss 3.036, Val loss 3.116\n",
            "Ep 3 (Step 025025): Train loss 2.897, Val loss 3.107\n",
            "Ep 3 (Step 025050): Train loss 2.601, Val loss 3.162\n",
            "Ep 3 (Step 025075): Train loss 3.015, Val loss 3.093\n",
            "Ep 3 (Step 025100): Train loss 3.016, Val loss 3.078\n",
            "Ep 3 (Step 025125): Train loss 2.633, Val loss 3.070\n",
            "Ep 3 (Step 025150): Train loss 2.942, Val loss 3.033\n",
            "Ep 3 (Step 025175): Train loss 2.918, Val loss 3.053\n",
            "Ep 3 (Step 025200): Train loss 2.827, Val loss 3.083\n",
            "Ep 3 (Step 025225): Train loss 2.803, Val loss 3.080\n",
            "Ep 3 (Step 025250): Train loss 2.902, Val loss 3.074\n",
            "Ep 3 (Step 025275): Train loss 2.864, Val loss 3.057\n",
            "Ep 3 (Step 025300): Train loss 2.787, Val loss 3.079\n",
            "Ep 3 (Step 025325): Train loss 2.846, Val loss 3.086\n",
            "Ep 3 (Step 025350): Train loss 2.810, Val loss 3.098\n",
            "Ep 3 (Step 025375): Train loss 2.734, Val loss 3.146\n",
            "Ep 3 (Step 025400): Train loss 2.966, Val loss 3.146\n",
            "Ep 3 (Step 025425): Train loss 3.027, Val loss 3.082\n",
            "Ep 3 (Step 025450): Train loss 2.624, Val loss 3.067\n",
            "Ep 3 (Step 025475): Train loss 2.629, Val loss 3.064\n",
            "Ep 3 (Step 025500): Train loss 2.730, Val loss 3.058\n",
            "Ep 3 (Step 025525): Train loss 2.644, Val loss 3.067\n",
            "Ep 3 (Step 025550): Train loss 2.706, Val loss 3.063\n",
            "Ep 3 (Step 025575): Train loss 2.897, Val loss 3.068\n",
            "Ep 3 (Step 025600): Train loss 2.745, Val loss 3.068\n",
            "Ep 3 (Step 025625): Train loss 3.005, Val loss 3.098\n",
            "Ep 3 (Step 025650): Train loss 2.934, Val loss 3.122\n",
            "Ep 3 (Step 025675): Train loss 3.107, Val loss 3.097\n",
            "Ep 3 (Step 025700): Train loss 2.927, Val loss 3.094\n",
            "Ep 3 (Step 025725): Train loss 2.761, Val loss 3.117\n",
            "Ep 3 (Step 025750): Train loss 2.751, Val loss 3.146\n",
            "Ep 3 (Step 025775): Train loss 2.849, Val loss 3.116\n",
            "Ep 3 (Step 025800): Train loss 2.536, Val loss 3.075\n",
            "Ep 3 (Step 025825): Train loss 3.121, Val loss 3.099\n",
            "Ep 3 (Step 025850): Train loss 2.863, Val loss 3.105\n",
            "Ep 3 (Step 025875): Train loss 2.610, Val loss 3.110\n",
            "Ep 3 (Step 025900): Train loss 3.147, Val loss 3.148\n",
            "Ep 3 (Step 025925): Train loss 2.930, Val loss 3.130\n",
            "Ep 3 (Step 025950): Train loss 2.856, Val loss 3.064\n",
            "Ep 3 (Step 025975): Train loss 2.786, Val loss 3.094\n",
            "Ep 3 (Step 026000): Train loss 2.978, Val loss 3.088\n",
            "Ep 3 (Step 026025): Train loss 2.670, Val loss 3.053\n",
            "Ep 3 (Step 026050): Train loss 2.912, Val loss 3.056\n",
            "Ep 3 (Step 026075): Train loss 2.662, Val loss 3.080\n",
            "Ep 3 (Step 026100): Train loss 2.814, Val loss 3.089\n",
            "Ep 3 (Step 026125): Train loss 2.769, Val loss 3.093\n",
            "Ep 3 (Step 026150): Train loss 2.922, Val loss 3.089\n",
            "Ep 3 (Step 026175): Train loss 2.964, Val loss 3.114\n",
            "Ep 3 (Step 026200): Train loss 2.647, Val loss 3.064\n",
            "Ep 3 (Step 026225): Train loss 2.943, Val loss 3.073\n",
            "Ep 3 (Step 026250): Train loss 2.807, Val loss 3.110\n",
            "Ep 3 (Step 026275): Train loss 2.996, Val loss 3.056\n",
            "Ep 3 (Step 026300): Train loss 2.646, Val loss 3.048\n",
            "Ep 3 (Step 026325): Train loss 2.906, Val loss 3.089\n",
            "Ep 3 (Step 026350): Train loss 2.743, Val loss 3.071\n",
            "Ep 3 (Step 026375): Train loss 2.660, Val loss 3.072\n",
            "Ep 3 (Step 026400): Train loss 2.912, Val loss 3.089\n",
            "Ep 3 (Step 026425): Train loss 2.643, Val loss 3.096\n",
            "Ep 3 (Step 026450): Train loss 2.876, Val loss 3.089\n",
            "Ep 3 (Step 026475): Train loss 2.991, Val loss 3.128\n",
            "Ep 3 (Step 026500): Train loss 2.761, Val loss 3.096\n",
            "Ep 3 (Step 026525): Train loss 3.143, Val loss 3.111\n",
            "Ep 3 (Step 026550): Train loss 2.767, Val loss 3.114\n",
            "Ep 3 (Step 026575): Train loss 2.731, Val loss 3.116\n",
            "Ep 3 (Step 026600): Train loss 2.814, Val loss 3.083\n",
            "Ep 3 (Step 026625): Train loss 2.772, Val loss 3.130\n",
            "Ep 3 (Step 026650): Train loss 2.795, Val loss 3.103\n",
            "Ep 3 (Step 026675): Train loss 2.876, Val loss 3.054\n",
            "Ep 3 (Step 026700): Train loss 2.990, Val loss 3.016\n",
            "Ep 3 (Step 026725): Train loss 2.629, Val loss 3.054\n",
            "Ep 3 (Step 026750): Train loss 3.003, Val loss 3.081\n",
            "Ep 3 (Step 026775): Train loss 2.805, Val loss 3.041\n",
            "Ep 3 (Step 026800): Train loss 2.786, Val loss 3.073\n",
            "Ep 3 (Step 026825): Train loss 3.178, Val loss 3.039\n",
            "Ep 3 (Step 026850): Train loss 2.754, Val loss 3.052\n",
            "Ep 3 (Step 026875): Train loss 2.856, Val loss 3.072\n",
            "Ep 3 (Step 026900): Train loss 2.733, Val loss 3.072\n",
            "Ep 3 (Step 026925): Train loss 2.842, Val loss 3.065\n",
            "Ep 3 (Step 026950): Train loss 2.551, Val loss 3.092\n",
            "Ep 3 (Step 026975): Train loss 2.890, Val loss 3.075\n",
            "Ep 3 (Step 027000): Train loss 2.721, Val loss 3.041\n",
            "Ep 3 (Step 027025): Train loss 2.652, Val loss 3.097\n",
            "Ep 3 (Step 027050): Train loss 2.894, Val loss 3.074\n",
            "Ep 3 (Step 027075): Train loss 2.910, Val loss 3.080\n",
            "Ep 3 (Step 027100): Train loss 2.742, Val loss 3.061\n",
            "Ep 3 (Step 027125): Train loss 2.401, Val loss 3.078\n",
            "Ep 3 (Step 027150): Train loss 2.769, Val loss 3.095\n",
            "Ep 3 (Step 027175): Train loss 2.610, Val loss 3.063\n",
            "Ep 3 (Step 027200): Train loss 2.742, Val loss 3.108\n",
            "Ep 3 (Step 027225): Train loss 2.698, Val loss 3.128\n",
            "Ep 3 (Step 027250): Train loss 2.742, Val loss 3.070\n",
            "Ep 3 (Step 027275): Train loss 2.795, Val loss 3.075\n",
            "Ep 3 (Step 027300): Train loss 2.696, Val loss 3.089\n",
            "Ep 3 (Step 027325): Train loss 2.905, Val loss 3.105\n",
            "Ep 3 (Step 027350): Train loss 2.805, Val loss 3.119\n",
            "Ep 3 (Step 027375): Train loss 2.762, Val loss 3.087\n",
            "Ep 3 (Step 027400): Train loss 2.715, Val loss 3.086\n",
            "Ep 3 (Step 027425): Train loss 2.805, Val loss 3.086\n",
            "Ep 3 (Step 027450): Train loss 2.476, Val loss 3.053\n",
            "Ep 3 (Step 027475): Train loss 2.748, Val loss 3.036\n",
            "Ep 3 (Step 027500): Train loss 3.017, Val loss 3.097\n",
            "Ep 3 (Step 027525): Train loss 2.778, Val loss 3.075\n",
            "Ep 3 (Step 027550): Train loss 2.755, Val loss 3.073\n",
            "Ep 3 (Step 027575): Train loss 2.801, Val loss 3.032\n",
            "Ep 3 (Step 027600): Train loss 2.785, Val loss 3.091\n",
            "Ep 3 (Step 027625): Train loss 2.779, Val loss 3.090\n",
            "Ep 3 (Step 027650): Train loss 2.894, Val loss 3.075\n",
            "Ep 3 (Step 027675): Train loss 2.747, Val loss 3.080\n",
            "Ep 3 (Step 027700): Train loss 2.740, Val loss 3.085\n",
            "Ep 3 (Step 027725): Train loss 2.844, Val loss 3.064\n",
            "Ep 3 (Step 027750): Train loss 2.638, Val loss 3.068\n",
            "Ep 3 (Step 027775): Train loss 2.683, Val loss 3.077\n",
            "Ep 3 (Step 027800): Train loss 2.792, Val loss 3.099\n",
            "Ep 3 (Step 027825): Train loss 2.903, Val loss 3.079\n",
            "Ep 3 (Step 027850): Train loss 2.818, Val loss 3.062\n",
            "Ep 3 (Step 027875): Train loss 2.669, Val loss 3.079\n",
            "Ep 3 (Step 027900): Train loss 2.694, Val loss 3.077\n",
            "Ep 3 (Step 027925): Train loss 2.959, Val loss 3.083\n",
            "Ep 3 (Step 027950): Train loss 2.557, Val loss 3.127\n",
            "Ep 3 (Step 027975): Train loss 2.656, Val loss 3.060\n",
            "Ep 3 (Step 028000): Train loss 2.642, Val loss 3.064\n",
            "Ep 3 (Step 028025): Train loss 2.787, Val loss 3.032\n",
            "Ep 3 (Step 028050): Train loss 2.711, Val loss 3.047\n",
            "Ep 3 (Step 028075): Train loss 2.926, Val loss 3.017\n",
            "Ep 3 (Step 028100): Train loss 2.651, Val loss 3.027\n",
            "Ep 3 (Step 028125): Train loss 2.879, Val loss 3.051\n",
            "Ep 3 (Step 028150): Train loss 2.950, Val loss 3.056\n",
            "Ep 3 (Step 028175): Train loss 2.487, Val loss 3.042\n",
            "Ep 3 (Step 028200): Train loss 2.744, Val loss 3.078\n",
            "Ep 3 (Step 028225): Train loss 2.650, Val loss 3.099\n",
            "Ep 3 (Step 028250): Train loss 2.790, Val loss 3.090\n",
            "Ep 3 (Step 028275): Train loss 2.668, Val loss 3.073\n",
            "Ep 3 (Step 028300): Train loss 2.955, Val loss 3.036\n",
            "Ep 3 (Step 028325): Train loss 2.798, Val loss 3.038\n",
            "Ep 3 (Step 028350): Train loss 2.723, Val loss 3.111\n",
            "Ep 3 (Step 028375): Train loss 2.888, Val loss 3.116\n",
            "Ep 3 (Step 028400): Train loss 2.986, Val loss 3.095\n",
            "Ep 3 (Step 028425): Train loss 2.898, Val loss 3.065\n",
            "Ep 3 (Step 028450): Train loss 2.703, Val loss 3.060\n",
            "Ep 3 (Step 028475): Train loss 2.621, Val loss 3.092\n",
            "Ep 3 (Step 028500): Train loss 2.916, Val loss 3.025\n",
            "Ep 3 (Step 028525): Train loss 2.889, Val loss 3.084\n",
            "Ep 3 (Step 028550): Train loss 2.844, Val loss 3.070\n",
            "Ep 3 (Step 028575): Train loss 2.833, Val loss 3.095\n",
            "Ep 3 (Step 028600): Train loss 2.936, Val loss 3.081\n",
            "Ep 3 (Step 028625): Train loss 2.575, Val loss 3.085\n",
            "Ep 3 (Step 028650): Train loss 2.724, Val loss 3.103\n",
            "Ep 3 (Step 028675): Train loss 2.692, Val loss 3.054\n",
            "Ep 3 (Step 028700): Train loss 2.734, Val loss 3.064\n",
            "Ep 3 (Step 028725): Train loss 2.705, Val loss 3.097\n",
            "Ep 3 (Step 028750): Train loss 2.616, Val loss 3.046\n",
            "Ep 3 (Step 028775): Train loss 2.489, Val loss 3.066\n",
            "Ep 3 (Step 028800): Train loss 2.861, Val loss 3.064\n",
            "Ep 3 (Step 028825): Train loss 2.911, Val loss 3.066\n",
            "Ep 3 (Step 028850): Train loss 2.819, Val loss 3.067\n",
            "Ep 3 (Step 028875): Train loss 2.747, Val loss 3.066\n",
            "Ep 3 (Step 028900): Train loss 2.654, Val loss 3.082\n",
            "Ep 3 (Step 028925): Train loss 2.868, Val loss 3.079\n",
            "Ep 3 (Step 028950): Train loss 2.792, Val loss 3.033\n",
            "Ep 3 (Step 028975): Train loss 2.751, Val loss 3.030\n",
            "Ep 3 (Step 029000): Train loss 2.749, Val loss 3.003\n",
            "Ep 3 (Step 029025): Train loss 2.806, Val loss 3.056\n",
            "Ep 3 (Step 029050): Train loss 2.743, Val loss 3.026\n",
            "Ep 3 (Step 029075): Train loss 2.912, Val loss 3.040\n",
            "Ep 3 (Step 029100): Train loss 2.658, Val loss 3.047\n",
            "Ep 3 (Step 029125): Train loss 2.584, Val loss 3.125\n",
            "Ep 3 (Step 029150): Train loss 2.766, Val loss 3.071\n",
            "Ep 3 (Step 029175): Train loss 2.811, Val loss 3.071\n",
            "Ep 3 (Step 029200): Train loss 2.823, Val loss 3.070\n",
            "Ep 3 (Step 029225): Train loss 2.492, Val loss 3.100\n",
            "Ep 3 (Step 029250): Train loss 2.679, Val loss 3.074\n",
            "Ep 3 (Step 029275): Train loss 2.791, Val loss 3.078\n",
            "Ep 3 (Step 029300): Train loss 2.852, Val loss 3.041\n",
            "Ep 3 (Step 029325): Train loss 2.800, Val loss 3.044\n",
            "Ep 3 (Step 029350): Train loss 2.827, Val loss 3.010\n",
            "Ep 3 (Step 029375): Train loss 2.866, Val loss 3.040\n",
            "Ep 3 (Step 029400): Train loss 2.731, Val loss 3.032\n",
            "Ep 3 (Step 029425): Train loss 2.880, Val loss 3.030\n",
            "Ep 3 (Step 029450): Train loss 2.728, Val loss 3.026\n",
            "Ep 3 (Step 029475): Train loss 2.820, Val loss 3.056\n",
            "Ep 3 (Step 029500): Train loss 2.720, Val loss 3.023\n",
            "Ep 3 (Step 029525): Train loss 2.839, Val loss 3.075\n",
            "Ep 3 (Step 029550): Train loss 2.802, Val loss 3.035\n",
            "Ep 3 (Step 029575): Train loss 2.958, Val loss 3.059\n",
            "Ep 3 (Step 029600): Train loss 2.778, Val loss 3.062\n",
            "Ep 3 (Step 029625): Train loss 2.945, Val loss 3.096\n",
            "Ep 3 (Step 029650): Train loss 2.879, Val loss 3.095\n",
            "Ep 3 (Step 029675): Train loss 2.798, Val loss 3.158\n",
            "Ep 3 (Step 029700): Train loss 2.703, Val loss 3.102\n",
            "Ep 3 (Step 029725): Train loss 2.764, Val loss 3.033\n",
            "Ep 3 (Step 029750): Train loss 2.808, Val loss 3.068\n",
            "Ep 3 (Step 029775): Train loss 2.799, Val loss 3.064\n",
            "Ep 3 (Step 029800): Train loss 2.862, Val loss 3.081\n",
            "Ep 3 (Step 029825): Train loss 2.608, Val loss 3.067\n",
            "Ep 3 (Step 029850): Train loss 2.730, Val loss 3.058\n",
            "Ep 3 (Step 029875): Train loss 2.552, Val loss 3.058\n",
            "Ep 3 (Step 029900): Train loss 2.893, Val loss 3.054\n",
            "Ep 3 (Step 029925): Train loss 2.760, Val loss 3.064\n",
            "Ep 3 (Step 029950): Train loss 2.655, Val loss 3.070\n",
            "Ep 3 (Step 029975): Train loss 2.642, Val loss 3.069\n",
            "Ep 3 (Step 030000): Train loss 2.771, Val loss 3.066\n",
            "Ep 3 (Step 030025): Train loss 2.814, Val loss 3.081\n",
            "Ep 3 (Step 030050): Train loss 2.672, Val loss 3.037\n",
            "Ep 3 (Step 030075): Train loss 2.606, Val loss 3.048\n",
            "Ep 3 (Step 030100): Train loss 2.620, Val loss 3.060\n",
            "Ep 3 (Step 030125): Train loss 2.821, Val loss 3.035\n",
            "Ep 3 (Step 030150): Train loss 2.578, Val loss 3.010\n",
            "Ep 3 (Step 030175): Train loss 2.723, Val loss 3.056\n",
            "Ep 3 (Step 030200): Train loss 2.514, Val loss 3.020\n",
            "Ep 3 (Step 030225): Train loss 2.491, Val loss 3.017\n",
            "Ep 3 (Step 030250): Train loss 2.839, Val loss 3.025\n",
            "Ep 3 (Step 030275): Train loss 2.894, Val loss 3.045\n",
            "Ep 3 (Step 030300): Train loss 2.935, Val loss 3.044\n",
            "Ep 3 (Step 030325): Train loss 2.851, Val loss 3.059\n",
            "Ep 3 (Step 030350): Train loss 2.714, Val loss 3.061\n",
            "Ep 3 (Step 030375): Train loss 2.782, Val loss 3.093\n",
            "Ep 3 (Step 030400): Train loss 2.746, Val loss 3.024\n",
            "Ep 3 (Step 030425): Train loss 2.991, Val loss 3.096\n",
            "Ep 3 (Step 030450): Train loss 2.695, Val loss 3.044\n",
            "Ep 3 (Step 030475): Train loss 2.546, Val loss 3.064\n",
            "Ep 3 (Step 030500): Train loss 2.755, Val loss 3.060\n",
            "Ep 3 (Step 030525): Train loss 3.024, Val loss 3.040\n",
            "Ep 3 (Step 030550): Train loss 2.987, Val loss 3.031\n",
            "Ep 3 (Step 030575): Train loss 2.688, Val loss 3.065\n",
            "Ep 3 (Step 030600): Train loss 2.760, Val loss 2.984\n",
            "Ep 3 (Step 030625): Train loss 2.805, Val loss 3.010\n",
            "Ep 3 (Step 030650): Train loss 2.816, Val loss 2.990\n",
            "Ep 3 (Step 030675): Train loss 2.774, Val loss 2.986\n",
            "Ep 3 (Step 030700): Train loss 2.709, Val loss 3.018\n",
            "Ep 3 (Step 030725): Train loss 2.743, Val loss 3.032\n",
            "set()\n",
            "0\n",
            "0\n",
            "ဘာမျှ မသိရပေ။ သူမက သူမ၏လက်ကို အနည်းငယ် အနည်းငယ် အသက်ရှူနေမိသည်။ သူမက သူမ၏လက်ကို အနည်းငယ် အနည်း\n",
            "Ep 4 (Step 030750): Train loss 2.795, Val loss 3.044\n",
            "Ep 4 (Step 030775): Train loss 2.832, Val loss 3.076\n",
            "Ep 4 (Step 030800): Train loss 2.743, Val loss 3.033\n",
            "Ep 4 (Step 030825): Train loss 2.875, Val loss 3.045\n",
            "Ep 4 (Step 030850): Train loss 2.755, Val loss 3.071\n",
            "Ep 4 (Step 030875): Train loss 2.990, Val loss 3.057\n",
            "Ep 4 (Step 030900): Train loss 2.791, Val loss 3.020\n",
            "Ep 4 (Step 030925): Train loss 2.769, Val loss 2.989\n",
            "Ep 4 (Step 030950): Train loss 2.875, Val loss 3.016\n",
            "Ep 4 (Step 030975): Train loss 2.857, Val loss 3.010\n",
            "Ep 4 (Step 031000): Train loss 2.820, Val loss 3.067\n",
            "Ep 4 (Step 031025): Train loss 2.708, Val loss 3.064\n",
            "Ep 4 (Step 031050): Train loss 2.875, Val loss 3.030\n",
            "Ep 4 (Step 031075): Train loss 2.796, Val loss 3.028\n",
            "Ep 4 (Step 031100): Train loss 2.589, Val loss 3.003\n",
            "Ep 4 (Step 031125): Train loss 2.641, Val loss 3.062\n",
            "Ep 4 (Step 031150): Train loss 2.788, Val loss 3.070\n",
            "Ep 4 (Step 031175): Train loss 2.983, Val loss 3.082\n",
            "Ep 4 (Step 031200): Train loss 2.628, Val loss 3.069\n",
            "Ep 4 (Step 031225): Train loss 2.687, Val loss 3.035\n",
            "Ep 4 (Step 031250): Train loss 2.759, Val loss 3.064\n",
            "Ep 4 (Step 031275): Train loss 2.787, Val loss 3.081\n",
            "Ep 4 (Step 031300): Train loss 2.692, Val loss 3.069\n",
            "Ep 4 (Step 031325): Train loss 2.446, Val loss 3.040\n",
            "Ep 4 (Step 031350): Train loss 2.598, Val loss 3.033\n",
            "Ep 4 (Step 031375): Train loss 2.664, Val loss 3.086\n",
            "Ep 4 (Step 031400): Train loss 3.008, Val loss 3.045\n",
            "Ep 4 (Step 031425): Train loss 2.603, Val loss 3.038\n",
            "Ep 4 (Step 031450): Train loss 2.924, Val loss 3.017\n",
            "Ep 4 (Step 031475): Train loss 2.779, Val loss 3.025\n",
            "Ep 4 (Step 031500): Train loss 2.815, Val loss 3.059\n",
            "Ep 4 (Step 031525): Train loss 2.697, Val loss 3.057\n",
            "Ep 4 (Step 031550): Train loss 2.545, Val loss 3.051\n",
            "Ep 4 (Step 031575): Train loss 2.758, Val loss 3.084\n",
            "Ep 4 (Step 031600): Train loss 2.781, Val loss 3.032\n",
            "Ep 4 (Step 031625): Train loss 2.670, Val loss 3.085\n",
            "Ep 4 (Step 031650): Train loss 2.795, Val loss 3.015\n",
            "Ep 4 (Step 031675): Train loss 2.907, Val loss 3.037\n",
            "Ep 4 (Step 031700): Train loss 2.788, Val loss 3.015\n",
            "Ep 4 (Step 031725): Train loss 2.576, Val loss 3.031\n",
            "Ep 4 (Step 031750): Train loss 2.710, Val loss 3.006\n",
            "Ep 4 (Step 031775): Train loss 2.676, Val loss 3.017\n",
            "Ep 4 (Step 031800): Train loss 2.746, Val loss 3.031\n",
            "Ep 4 (Step 031825): Train loss 2.627, Val loss 3.069\n",
            "Ep 4 (Step 031850): Train loss 2.782, Val loss 3.054\n",
            "Ep 4 (Step 031875): Train loss 2.586, Val loss 3.053\n",
            "Ep 4 (Step 031900): Train loss 2.816, Val loss 3.110\n",
            "Ep 4 (Step 031925): Train loss 2.536, Val loss 3.068\n",
            "Ep 4 (Step 031950): Train loss 2.610, Val loss 3.074\n",
            "Ep 4 (Step 031975): Train loss 2.780, Val loss 3.082\n",
            "Ep 4 (Step 032000): Train loss 2.629, Val loss 3.021\n",
            "Ep 4 (Step 032025): Train loss 2.596, Val loss 3.050\n",
            "Ep 4 (Step 032050): Train loss 2.774, Val loss 3.072\n",
            "Ep 4 (Step 032075): Train loss 2.654, Val loss 3.094\n",
            "Ep 4 (Step 032100): Train loss 2.658, Val loss 3.092\n",
            "Ep 4 (Step 032125): Train loss 2.831, Val loss 3.063\n",
            "Ep 4 (Step 032150): Train loss 2.776, Val loss 3.051\n",
            "Ep 4 (Step 032175): Train loss 2.773, Val loss 3.075\n",
            "Ep 4 (Step 032200): Train loss 2.606, Val loss 3.074\n",
            "Ep 4 (Step 032225): Train loss 2.603, Val loss 3.064\n",
            "Ep 4 (Step 032250): Train loss 2.948, Val loss 3.075\n",
            "Ep 4 (Step 032275): Train loss 2.604, Val loss 3.052\n",
            "Ep 4 (Step 032300): Train loss 2.664, Val loss 3.066\n",
            "Ep 4 (Step 032325): Train loss 2.889, Val loss 3.032\n",
            "Ep 4 (Step 032350): Train loss 2.810, Val loss 3.083\n",
            "Ep 4 (Step 032375): Train loss 2.689, Val loss 3.073\n",
            "Ep 4 (Step 032400): Train loss 2.533, Val loss 3.048\n",
            "Ep 4 (Step 032425): Train loss 2.936, Val loss 3.070\n",
            "Ep 4 (Step 032450): Train loss 2.780, Val loss 3.067\n",
            "Ep 4 (Step 032475): Train loss 2.822, Val loss 3.052\n",
            "Ep 4 (Step 032500): Train loss 2.632, Val loss 3.078\n",
            "Ep 4 (Step 032525): Train loss 2.973, Val loss 3.041\n",
            "Ep 4 (Step 032550): Train loss 2.697, Val loss 3.046\n",
            "Ep 4 (Step 032575): Train loss 2.738, Val loss 3.053\n",
            "Ep 4 (Step 032600): Train loss 2.619, Val loss 3.055\n",
            "Ep 4 (Step 032625): Train loss 2.582, Val loss 3.050\n",
            "Ep 4 (Step 032650): Train loss 2.939, Val loss 3.034\n",
            "Ep 4 (Step 032675): Train loss 2.728, Val loss 3.066\n",
            "Ep 4 (Step 032700): Train loss 2.788, Val loss 3.062\n",
            "Ep 4 (Step 032725): Train loss 2.801, Val loss 3.061\n",
            "Ep 4 (Step 032750): Train loss 2.710, Val loss 3.034\n",
            "Ep 4 (Step 032775): Train loss 2.454, Val loss 3.057\n",
            "Ep 4 (Step 032800): Train loss 2.675, Val loss 3.038\n",
            "Ep 4 (Step 032825): Train loss 2.860, Val loss 3.067\n",
            "Ep 4 (Step 032850): Train loss 2.973, Val loss 3.023\n",
            "Ep 4 (Step 032875): Train loss 2.965, Val loss 3.013\n",
            "Ep 4 (Step 032900): Train loss 2.881, Val loss 3.011\n",
            "Ep 4 (Step 032925): Train loss 2.693, Val loss 3.035\n",
            "Ep 4 (Step 032950): Train loss 2.665, Val loss 3.012\n",
            "Ep 4 (Step 032975): Train loss 2.681, Val loss 3.054\n",
            "Ep 4 (Step 033000): Train loss 2.995, Val loss 3.049\n",
            "Ep 4 (Step 033025): Train loss 2.505, Val loss 3.038\n",
            "Ep 4 (Step 033050): Train loss 2.759, Val loss 3.029\n",
            "Ep 4 (Step 033075): Train loss 2.591, Val loss 3.052\n",
            "Ep 4 (Step 033100): Train loss 2.770, Val loss 3.036\n",
            "Ep 4 (Step 033125): Train loss 2.661, Val loss 2.986\n",
            "Ep 4 (Step 033150): Train loss 2.576, Val loss 3.036\n",
            "Ep 4 (Step 033175): Train loss 3.031, Val loss 3.034\n",
            "Ep 4 (Step 033200): Train loss 2.933, Val loss 3.074\n",
            "Ep 4 (Step 033225): Train loss 2.738, Val loss 2.988\n",
            "Ep 4 (Step 033250): Train loss 2.510, Val loss 3.017\n",
            "Ep 4 (Step 033275): Train loss 2.773, Val loss 3.040\n",
            "Ep 4 (Step 033300): Train loss 2.567, Val loss 3.009\n",
            "Ep 4 (Step 033325): Train loss 2.691, Val loss 3.044\n",
            "Ep 4 (Step 033350): Train loss 2.665, Val loss 3.038\n",
            "Ep 4 (Step 033375): Train loss 2.866, Val loss 3.024\n",
            "Ep 4 (Step 033400): Train loss 2.573, Val loss 3.042\n",
            "Ep 4 (Step 033425): Train loss 2.847, Val loss 3.059\n",
            "Ep 4 (Step 033450): Train loss 2.614, Val loss 3.036\n",
            "Ep 4 (Step 033475): Train loss 2.913, Val loss 3.041\n",
            "Ep 4 (Step 033500): Train loss 2.583, Val loss 3.072\n",
            "Ep 4 (Step 033525): Train loss 2.822, Val loss 3.042\n",
            "Ep 4 (Step 033550): Train loss 2.772, Val loss 3.003\n",
            "Ep 4 (Step 033575): Train loss 2.674, Val loss 3.010\n",
            "Ep 4 (Step 033600): Train loss 2.760, Val loss 3.020\n",
            "Ep 4 (Step 033625): Train loss 2.473, Val loss 3.023\n",
            "Ep 4 (Step 033650): Train loss 2.743, Val loss 3.019\n",
            "Ep 4 (Step 033675): Train loss 2.735, Val loss 2.997\n",
            "Ep 4 (Step 033700): Train loss 2.768, Val loss 3.031\n",
            "Ep 4 (Step 033725): Train loss 2.832, Val loss 3.009\n",
            "Ep 4 (Step 033750): Train loss 2.784, Val loss 2.994\n",
            "Ep 4 (Step 033775): Train loss 2.720, Val loss 3.002\n",
            "Ep 4 (Step 033800): Train loss 2.649, Val loss 3.025\n",
            "Ep 4 (Step 033825): Train loss 2.615, Val loss 3.042\n",
            "Ep 4 (Step 033850): Train loss 2.718, Val loss 3.036\n",
            "Ep 4 (Step 033875): Train loss 2.521, Val loss 3.025\n",
            "Ep 4 (Step 033900): Train loss 2.769, Val loss 3.023\n",
            "Ep 4 (Step 033925): Train loss 2.810, Val loss 3.003\n",
            "Ep 4 (Step 033950): Train loss 2.696, Val loss 3.017\n",
            "Ep 4 (Step 033975): Train loss 2.824, Val loss 3.040\n",
            "Ep 4 (Step 034000): Train loss 2.534, Val loss 2.984\n",
            "Ep 4 (Step 034025): Train loss 2.731, Val loss 3.019\n",
            "Ep 4 (Step 034050): Train loss 2.889, Val loss 3.039\n",
            "Ep 4 (Step 034075): Train loss 2.693, Val loss 3.006\n",
            "Ep 4 (Step 034100): Train loss 2.699, Val loss 3.010\n",
            "Ep 4 (Step 034125): Train loss 2.694, Val loss 2.994\n",
            "Ep 4 (Step 034150): Train loss 2.709, Val loss 3.007\n",
            "Ep 4 (Step 034175): Train loss 2.786, Val loss 3.042\n",
            "Ep 4 (Step 034200): Train loss 2.650, Val loss 3.026\n",
            "Ep 4 (Step 034225): Train loss 2.829, Val loss 3.037\n",
            "Ep 4 (Step 034250): Train loss 2.679, Val loss 3.032\n",
            "Ep 4 (Step 034275): Train loss 2.877, Val loss 3.045\n",
            "Ep 4 (Step 034300): Train loss 2.811, Val loss 3.019\n",
            "Ep 4 (Step 034325): Train loss 2.842, Val loss 3.035\n",
            "Ep 4 (Step 034350): Train loss 2.759, Val loss 3.043\n",
            "Ep 4 (Step 034375): Train loss 2.708, Val loss 3.011\n",
            "Ep 4 (Step 034400): Train loss 2.770, Val loss 3.020\n",
            "Ep 4 (Step 034425): Train loss 2.625, Val loss 3.030\n",
            "Ep 4 (Step 034450): Train loss 2.541, Val loss 3.039\n",
            "Ep 4 (Step 034475): Train loss 2.684, Val loss 3.107\n",
            "Ep 4 (Step 034500): Train loss 2.687, Val loss 3.047\n",
            "Ep 4 (Step 034525): Train loss 2.803, Val loss 3.082\n",
            "Ep 4 (Step 034550): Train loss 2.731, Val loss 3.036\n",
            "Ep 4 (Step 034575): Train loss 2.664, Val loss 3.035\n",
            "Ep 4 (Step 034600): Train loss 2.770, Val loss 3.040\n",
            "Ep 4 (Step 034625): Train loss 2.600, Val loss 3.054\n",
            "Ep 4 (Step 034650): Train loss 2.761, Val loss 3.014\n",
            "Ep 4 (Step 034675): Train loss 2.594, Val loss 3.023\n",
            "Ep 4 (Step 034700): Train loss 2.675, Val loss 3.061\n",
            "Ep 4 (Step 034725): Train loss 2.701, Val loss 3.067\n",
            "Ep 4 (Step 034750): Train loss 2.530, Val loss 3.065\n",
            "Ep 4 (Step 034775): Train loss 2.788, Val loss 3.089\n",
            "Ep 4 (Step 034800): Train loss 2.764, Val loss 3.017\n",
            "Ep 4 (Step 034825): Train loss 2.624, Val loss 3.028\n",
            "Ep 4 (Step 034850): Train loss 2.772, Val loss 3.023\n",
            "Ep 4 (Step 034875): Train loss 2.742, Val loss 3.029\n",
            "Ep 4 (Step 034900): Train loss 3.092, Val loss 3.076\n",
            "Ep 4 (Step 034925): Train loss 2.583, Val loss 3.033\n",
            "Ep 4 (Step 034950): Train loss 2.700, Val loss 3.085\n",
            "Ep 4 (Step 034975): Train loss 2.680, Val loss 3.074\n",
            "Ep 4 (Step 035000): Train loss 2.543, Val loss 3.084\n",
            "Ep 4 (Step 035025): Train loss 2.645, Val loss 3.079\n",
            "Ep 4 (Step 035050): Train loss 2.857, Val loss 3.026\n",
            "Ep 4 (Step 035075): Train loss 2.633, Val loss 3.067\n",
            "Ep 4 (Step 035100): Train loss 2.615, Val loss 3.091\n",
            "Ep 4 (Step 035125): Train loss 2.566, Val loss 3.044\n",
            "Ep 4 (Step 035150): Train loss 2.428, Val loss 3.075\n",
            "Ep 4 (Step 035175): Train loss 2.506, Val loss 3.064\n",
            "Ep 4 (Step 035200): Train loss 2.888, Val loss 3.100\n",
            "Ep 4 (Step 035225): Train loss 2.613, Val loss 3.071\n",
            "Ep 4 (Step 035250): Train loss 2.680, Val loss 3.102\n",
            "Ep 4 (Step 035275): Train loss 2.625, Val loss 3.000\n",
            "Ep 4 (Step 035300): Train loss 2.701, Val loss 3.035\n",
            "Ep 4 (Step 035325): Train loss 2.352, Val loss 3.022\n",
            "Ep 4 (Step 035350): Train loss 2.658, Val loss 3.003\n",
            "Ep 4 (Step 035375): Train loss 2.742, Val loss 2.978\n",
            "Ep 4 (Step 035400): Train loss 2.993, Val loss 3.027\n",
            "Ep 4 (Step 035425): Train loss 2.819, Val loss 2.996\n",
            "Ep 4 (Step 035450): Train loss 2.742, Val loss 3.028\n",
            "Ep 4 (Step 035475): Train loss 2.756, Val loss 3.032\n",
            "Ep 4 (Step 035500): Train loss 2.648, Val loss 3.053\n",
            "Ep 4 (Step 035525): Train loss 2.795, Val loss 3.044\n",
            "Ep 4 (Step 035550): Train loss 2.720, Val loss 3.024\n",
            "Ep 4 (Step 035575): Train loss 2.765, Val loss 2.991\n",
            "Ep 4 (Step 035600): Train loss 2.689, Val loss 3.015\n",
            "Ep 4 (Step 035625): Train loss 2.854, Val loss 2.983\n",
            "Ep 4 (Step 035650): Train loss 2.667, Val loss 3.017\n",
            "Ep 4 (Step 035675): Train loss 2.749, Val loss 3.052\n",
            "Ep 4 (Step 035700): Train loss 2.678, Val loss 3.027\n",
            "Ep 4 (Step 035725): Train loss 2.664, Val loss 3.015\n",
            "Ep 4 (Step 035750): Train loss 2.693, Val loss 2.979\n",
            "Ep 4 (Step 035775): Train loss 2.617, Val loss 3.066\n",
            "Ep 4 (Step 035800): Train loss 2.604, Val loss 3.012\n",
            "Ep 4 (Step 035825): Train loss 2.694, Val loss 2.977\n",
            "Ep 4 (Step 035850): Train loss 2.636, Val loss 3.006\n",
            "Ep 4 (Step 035875): Train loss 2.603, Val loss 2.999\n",
            "Ep 4 (Step 035900): Train loss 2.581, Val loss 3.023\n",
            "Ep 4 (Step 035925): Train loss 2.616, Val loss 3.041\n",
            "Ep 4 (Step 035950): Train loss 2.683, Val loss 3.011\n",
            "Ep 4 (Step 035975): Train loss 3.054, Val loss 3.024\n",
            "Ep 4 (Step 036000): Train loss 2.820, Val loss 3.004\n",
            "Ep 4 (Step 036025): Train loss 2.880, Val loss 2.974\n",
            "Ep 4 (Step 036050): Train loss 2.871, Val loss 3.014\n",
            "Ep 4 (Step 036075): Train loss 2.622, Val loss 3.009\n",
            "Ep 4 (Step 036100): Train loss 2.523, Val loss 3.021\n",
            "Ep 4 (Step 036125): Train loss 2.600, Val loss 2.979\n",
            "Ep 4 (Step 036150): Train loss 2.582, Val loss 3.020\n",
            "Ep 4 (Step 036175): Train loss 2.618, Val loss 2.955\n",
            "Ep 4 (Step 036200): Train loss 2.797, Val loss 2.997\n",
            "Ep 4 (Step 036225): Train loss 2.567, Val loss 2.952\n",
            "Ep 4 (Step 036250): Train loss 2.874, Val loss 2.998\n",
            "Ep 4 (Step 036275): Train loss 2.805, Val loss 2.963\n",
            "Ep 4 (Step 036300): Train loss 2.570, Val loss 2.941\n",
            "Ep 4 (Step 036325): Train loss 2.628, Val loss 2.940\n",
            "Ep 4 (Step 036350): Train loss 2.826, Val loss 2.963\n",
            "Ep 4 (Step 036375): Train loss 2.758, Val loss 2.957\n",
            "Ep 4 (Step 036400): Train loss 2.697, Val loss 2.938\n",
            "Ep 4 (Step 036425): Train loss 2.655, Val loss 2.953\n",
            "Ep 4 (Step 036450): Train loss 2.553, Val loss 2.964\n",
            "Ep 4 (Step 036475): Train loss 2.396, Val loss 2.986\n",
            "Ep 4 (Step 036500): Train loss 2.734, Val loss 2.979\n",
            "Ep 4 (Step 036525): Train loss 2.515, Val loss 2.950\n",
            "Ep 4 (Step 036550): Train loss 2.724, Val loss 2.971\n",
            "Ep 4 (Step 036575): Train loss 2.890, Val loss 2.964\n",
            "Ep 4 (Step 036600): Train loss 2.372, Val loss 2.980\n",
            "Ep 4 (Step 036625): Train loss 2.956, Val loss 3.017\n",
            "Ep 4 (Step 036650): Train loss 2.760, Val loss 3.038\n",
            "Ep 4 (Step 036675): Train loss 2.668, Val loss 3.062\n",
            "Ep 4 (Step 036700): Train loss 2.775, Val loss 2.977\n",
            "Ep 4 (Step 036725): Train loss 2.576, Val loss 3.010\n",
            "Ep 4 (Step 036750): Train loss 2.632, Val loss 3.036\n",
            "Ep 4 (Step 036775): Train loss 2.616, Val loss 3.053\n",
            "Ep 4 (Step 036800): Train loss 2.864, Val loss 3.049\n",
            "Ep 4 (Step 036825): Train loss 2.950, Val loss 3.043\n",
            "Ep 4 (Step 036850): Train loss 2.727, Val loss 2.992\n",
            "Ep 4 (Step 036875): Train loss 2.939, Val loss 3.000\n",
            "Ep 4 (Step 036900): Train loss 2.949, Val loss 2.996\n",
            "Ep 4 (Step 036925): Train loss 3.055, Val loss 2.962\n",
            "Ep 4 (Step 036950): Train loss 2.869, Val loss 3.013\n",
            "Ep 4 (Step 036975): Train loss 2.787, Val loss 2.999\n",
            "Ep 4 (Step 037000): Train loss 2.751, Val loss 3.004\n",
            "Ep 4 (Step 037025): Train loss 2.541, Val loss 3.013\n",
            "Ep 4 (Step 037050): Train loss 2.648, Val loss 2.955\n",
            "Ep 4 (Step 037075): Train loss 2.810, Val loss 2.949\n",
            "Ep 4 (Step 037100): Train loss 2.760, Val loss 2.983\n",
            "Ep 4 (Step 037125): Train loss 2.695, Val loss 2.986\n",
            "Ep 4 (Step 037150): Train loss 2.989, Val loss 3.026\n",
            "Ep 4 (Step 037175): Train loss 2.717, Val loss 2.977\n",
            "Ep 4 (Step 037200): Train loss 2.754, Val loss 2.960\n",
            "Ep 4 (Step 037225): Train loss 2.788, Val loss 2.950\n",
            "Ep 4 (Step 037250): Train loss 2.843, Val loss 2.998\n",
            "Ep 4 (Step 037275): Train loss 2.443, Val loss 3.009\n",
            "Ep 4 (Step 037300): Train loss 2.643, Val loss 3.036\n",
            "Ep 4 (Step 037325): Train loss 2.645, Val loss 3.001\n",
            "Ep 4 (Step 037350): Train loss 2.780, Val loss 2.977\n",
            "Ep 4 (Step 037375): Train loss 2.761, Val loss 3.011\n",
            "Ep 4 (Step 037400): Train loss 2.702, Val loss 2.971\n",
            "Ep 4 (Step 037425): Train loss 2.952, Val loss 2.962\n",
            "Ep 4 (Step 037450): Train loss 2.785, Val loss 2.919\n",
            "Ep 4 (Step 037475): Train loss 2.537, Val loss 2.948\n",
            "Ep 4 (Step 037500): Train loss 2.894, Val loss 2.972\n",
            "Ep 4 (Step 037525): Train loss 2.670, Val loss 2.937\n",
            "Ep 4 (Step 037550): Train loss 2.727, Val loss 2.936\n",
            "Ep 4 (Step 037575): Train loss 2.784, Val loss 2.954\n",
            "Ep 4 (Step 037600): Train loss 2.517, Val loss 2.967\n",
            "Ep 4 (Step 037625): Train loss 2.404, Val loss 2.974\n",
            "Ep 4 (Step 037650): Train loss 2.767, Val loss 2.994\n",
            "Ep 4 (Step 037675): Train loss 2.780, Val loss 2.960\n",
            "Ep 4 (Step 037700): Train loss 2.618, Val loss 3.004\n",
            "Ep 4 (Step 037725): Train loss 2.729, Val loss 2.981\n",
            "Ep 4 (Step 037750): Train loss 2.608, Val loss 2.966\n",
            "Ep 4 (Step 037775): Train loss 2.626, Val loss 2.996\n",
            "Ep 4 (Step 037800): Train loss 2.743, Val loss 2.950\n",
            "Ep 4 (Step 037825): Train loss 2.730, Val loss 2.993\n",
            "Ep 4 (Step 037850): Train loss 2.669, Val loss 3.013\n",
            "Ep 4 (Step 037875): Train loss 2.599, Val loss 2.996\n",
            "Ep 4 (Step 037900): Train loss 2.676, Val loss 2.985\n",
            "Ep 4 (Step 037925): Train loss 2.586, Val loss 2.981\n",
            "Ep 4 (Step 037950): Train loss 2.600, Val loss 2.963\n",
            "Ep 4 (Step 037975): Train loss 2.440, Val loss 2.964\n",
            "Ep 4 (Step 038000): Train loss 2.628, Val loss 2.994\n",
            "Ep 4 (Step 038025): Train loss 2.677, Val loss 2.957\n",
            "Ep 4 (Step 038050): Train loss 2.709, Val loss 2.974\n",
            "Ep 4 (Step 038075): Train loss 2.949, Val loss 2.966\n",
            "Ep 4 (Step 038100): Train loss 2.784, Val loss 2.981\n",
            "Ep 4 (Step 038125): Train loss 2.775, Val loss 3.042\n",
            "Ep 4 (Step 038150): Train loss 2.727, Val loss 3.002\n",
            "Ep 4 (Step 038175): Train loss 2.851, Val loss 2.978\n",
            "Ep 4 (Step 038200): Train loss 2.797, Val loss 2.983\n",
            "Ep 4 (Step 038225): Train loss 2.565, Val loss 2.980\n",
            "Ep 4 (Step 038250): Train loss 2.884, Val loss 2.963\n",
            "Ep 4 (Step 038275): Train loss 2.595, Val loss 2.974\n",
            "Ep 4 (Step 038300): Train loss 2.834, Val loss 2.992\n",
            "Ep 4 (Step 038325): Train loss 2.703, Val loss 2.962\n",
            "Ep 4 (Step 038350): Train loss 2.766, Val loss 2.996\n",
            "Ep 4 (Step 038375): Train loss 2.772, Val loss 2.977\n",
            "Ep 4 (Step 038400): Train loss 2.731, Val loss 3.028\n",
            "Ep 4 (Step 038425): Train loss 2.723, Val loss 3.005\n",
            "Ep 4 (Step 038450): Train loss 2.881, Val loss 3.021\n",
            "Ep 4 (Step 038475): Train loss 2.833, Val loss 2.998\n",
            "Ep 4 (Step 038500): Train loss 2.782, Val loss 3.039\n",
            "Ep 4 (Step 038525): Train loss 2.577, Val loss 3.040\n",
            "Ep 4 (Step 038550): Train loss 2.478, Val loss 3.004\n",
            "Ep 4 (Step 038575): Train loss 2.602, Val loss 2.999\n",
            "Ep 4 (Step 038600): Train loss 2.613, Val loss 2.992\n",
            "Ep 4 (Step 038625): Train loss 2.531, Val loss 3.094\n",
            "Ep 4 (Step 038650): Train loss 2.585, Val loss 3.008\n",
            "Ep 4 (Step 038675): Train loss 2.557, Val loss 3.034\n",
            "Ep 4 (Step 038700): Train loss 2.905, Val loss 3.006\n",
            "Ep 4 (Step 038725): Train loss 2.721, Val loss 3.014\n",
            "Ep 4 (Step 038750): Train loss 2.724, Val loss 3.001\n",
            "Ep 4 (Step 038775): Train loss 2.756, Val loss 3.079\n",
            "Ep 4 (Step 038800): Train loss 2.764, Val loss 3.039\n",
            "Ep 4 (Step 038825): Train loss 2.811, Val loss 3.055\n",
            "Ep 4 (Step 038850): Train loss 2.716, Val loss 3.045\n",
            "Ep 4 (Step 038875): Train loss 2.741, Val loss 3.025\n",
            "Ep 4 (Step 038900): Train loss 2.676, Val loss 3.015\n",
            "Ep 4 (Step 038925): Train loss 2.772, Val loss 3.008\n",
            "Ep 4 (Step 038950): Train loss 2.853, Val loss 3.019\n",
            "Ep 4 (Step 038975): Train loss 2.563, Val loss 2.997\n",
            "Ep 4 (Step 039000): Train loss 2.624, Val loss 3.038\n",
            "Ep 4 (Step 039025): Train loss 2.757, Val loss 2.994\n",
            "Ep 4 (Step 039050): Train loss 2.677, Val loss 2.960\n",
            "Ep 4 (Step 039075): Train loss 2.513, Val loss 2.931\n",
            "Ep 4 (Step 039100): Train loss 2.974, Val loss 2.966\n",
            "Ep 4 (Step 039125): Train loss 2.483, Val loss 2.960\n",
            "Ep 4 (Step 039150): Train loss 2.547, Val loss 2.983\n",
            "Ep 4 (Step 039175): Train loss 2.527, Val loss 2.942\n",
            "Ep 4 (Step 039200): Train loss 2.633, Val loss 2.938\n",
            "Ep 4 (Step 039225): Train loss 2.761, Val loss 3.016\n",
            "Ep 4 (Step 039250): Train loss 2.480, Val loss 2.978\n",
            "Ep 4 (Step 039275): Train loss 2.448, Val loss 2.984\n",
            "Ep 4 (Step 039300): Train loss 2.789, Val loss 3.028\n",
            "Ep 4 (Step 039325): Train loss 2.783, Val loss 3.007\n",
            "Ep 4 (Step 039350): Train loss 2.648, Val loss 2.972\n",
            "Ep 4 (Step 039375): Train loss 3.007, Val loss 3.005\n",
            "Ep 4 (Step 039400): Train loss 2.665, Val loss 3.033\n",
            "Ep 4 (Step 039425): Train loss 2.480, Val loss 2.992\n",
            "Ep 4 (Step 039450): Train loss 2.695, Val loss 3.000\n",
            "Ep 4 (Step 039475): Train loss 2.904, Val loss 2.942\n",
            "Ep 4 (Step 039500): Train loss 2.797, Val loss 2.957\n",
            "Ep 4 (Step 039525): Train loss 2.752, Val loss 2.971\n",
            "Ep 4 (Step 039550): Train loss 2.590, Val loss 2.960\n",
            "Ep 4 (Step 039575): Train loss 2.511, Val loss 2.984\n",
            "Ep 4 (Step 039600): Train loss 2.629, Val loss 2.982\n",
            "Ep 4 (Step 039625): Train loss 2.669, Val loss 2.959\n",
            "Ep 4 (Step 039650): Train loss 2.624, Val loss 2.967\n",
            "Ep 4 (Step 039675): Train loss 2.709, Val loss 3.010\n",
            "Ep 4 (Step 039700): Train loss 2.421, Val loss 2.971\n",
            "Ep 4 (Step 039725): Train loss 3.013, Val loss 2.987\n",
            "Ep 4 (Step 039750): Train loss 2.393, Val loss 3.002\n",
            "Ep 4 (Step 039775): Train loss 2.686, Val loss 2.993\n",
            "Ep 4 (Step 039800): Train loss 2.828, Val loss 2.971\n",
            "Ep 4 (Step 039825): Train loss 2.717, Val loss 2.971\n",
            "Ep 4 (Step 039850): Train loss 2.691, Val loss 2.955\n",
            "Ep 4 (Step 039875): Train loss 2.537, Val loss 2.957\n",
            "Ep 4 (Step 039900): Train loss 2.738, Val loss 2.944\n",
            "Ep 4 (Step 039925): Train loss 2.741, Val loss 2.958\n",
            "Ep 4 (Step 039950): Train loss 2.677, Val loss 2.998\n",
            "Ep 4 (Step 039975): Train loss 2.718, Val loss 2.969\n",
            "Ep 4 (Step 040000): Train loss 2.551, Val loss 2.952\n",
            "Ep 4 (Step 040025): Train loss 2.535, Val loss 2.935\n",
            "Ep 4 (Step 040050): Train loss 2.611, Val loss 2.970\n",
            "Ep 4 (Step 040075): Train loss 2.690, Val loss 2.939\n",
            "Ep 4 (Step 040100): Train loss 2.721, Val loss 2.944\n",
            "Ep 4 (Step 040125): Train loss 2.648, Val loss 2.927\n",
            "Ep 4 (Step 040150): Train loss 2.646, Val loss 2.918\n",
            "Ep 4 (Step 040175): Train loss 2.584, Val loss 2.948\n",
            "Ep 4 (Step 040200): Train loss 2.637, Val loss 2.906\n",
            "Ep 4 (Step 040225): Train loss 2.694, Val loss 2.971\n",
            "Ep 4 (Step 040250): Train loss 2.917, Val loss 2.982\n",
            "Ep 4 (Step 040275): Train loss 2.638, Val loss 2.986\n",
            "Ep 4 (Step 040300): Train loss 2.419, Val loss 2.960\n",
            "Ep 4 (Step 040325): Train loss 2.980, Val loss 2.995\n",
            "Ep 4 (Step 040350): Train loss 2.646, Val loss 2.970\n",
            "Ep 4 (Step 040375): Train loss 2.706, Val loss 3.021\n",
            "Ep 4 (Step 040400): Train loss 2.586, Val loss 2.984\n",
            "Ep 4 (Step 040425): Train loss 2.659, Val loss 2.980\n",
            "Ep 4 (Step 040450): Train loss 2.472, Val loss 3.005\n",
            "Ep 4 (Step 040475): Train loss 2.940, Val loss 3.071\n",
            "Ep 4 (Step 040500): Train loss 2.695, Val loss 3.029\n",
            "Ep 4 (Step 040525): Train loss 2.585, Val loss 3.026\n",
            "Ep 4 (Step 040550): Train loss 2.846, Val loss 3.049\n",
            "Ep 4 (Step 040575): Train loss 2.655, Val loss 3.042\n",
            "Ep 4 (Step 040600): Train loss 2.538, Val loss 3.026\n",
            "Ep 4 (Step 040625): Train loss 2.846, Val loss 3.013\n",
            "Ep 4 (Step 040650): Train loss 2.699, Val loss 3.030\n",
            "Ep 4 (Step 040675): Train loss 2.740, Val loss 3.001\n",
            "Ep 4 (Step 040700): Train loss 2.481, Val loss 3.007\n",
            "Ep 4 (Step 040725): Train loss 2.844, Val loss 3.058\n",
            "Ep 4 (Step 040750): Train loss 2.925, Val loss 2.996\n",
            "Ep 4 (Step 040775): Train loss 2.535, Val loss 2.975\n",
            "Ep 4 (Step 040800): Train loss 2.597, Val loss 2.980\n",
            "Ep 4 (Step 040825): Train loss 2.760, Val loss 2.985\n",
            "Ep 4 (Step 040850): Train loss 2.811, Val loss 2.979\n",
            "Ep 4 (Step 040875): Train loss 2.629, Val loss 2.957\n",
            "Ep 4 (Step 040900): Train loss 2.793, Val loss 3.011\n",
            "Ep 4 (Step 040925): Train loss 2.599, Val loss 2.976\n",
            "Ep 4 (Step 040950): Train loss 2.647, Val loss 2.978\n",
            "Ep 4 (Step 040975): Train loss 2.490, Val loss 2.964\n",
            "set()\n",
            "0\n",
            "0\n",
            "ဘာလုပ်ရမှန်း မသိဘူး...” “ငါ အခု ငါ အသက်ရှင်နေတာပဲ...” “ငါ အသက်ရှင်နေတယ်.. ငါ အသက်ရှင်နေတယ်.\n",
            "Ep 5 (Step 041000): Train loss 2.953, Val loss 2.993\n",
            "Ep 5 (Step 041025): Train loss 2.671, Val loss 2.971\n",
            "Ep 5 (Step 041050): Train loss 2.503, Val loss 2.956\n",
            "Ep 5 (Step 041075): Train loss 2.730, Val loss 2.977\n",
            "Ep 5 (Step 041100): Train loss 2.681, Val loss 2.979\n",
            "Ep 5 (Step 041125): Train loss 2.589, Val loss 3.003\n",
            "Ep 5 (Step 041150): Train loss 2.783, Val loss 2.970\n",
            "Ep 5 (Step 041175): Train loss 3.089, Val loss 3.035\n",
            "Ep 5 (Step 041200): Train loss 2.890, Val loss 3.023\n",
            "Ep 5 (Step 041225): Train loss 2.694, Val loss 3.015\n",
            "Ep 5 (Step 041250): Train loss 2.607, Val loss 3.024\n",
            "Ep 5 (Step 041275): Train loss 2.609, Val loss 3.015\n",
            "Ep 5 (Step 041300): Train loss 2.748, Val loss 3.028\n",
            "Ep 5 (Step 041325): Train loss 2.649, Val loss 3.039\n",
            "Ep 5 (Step 041350): Train loss 2.943, Val loss 3.047\n",
            "Ep 5 (Step 041375): Train loss 2.620, Val loss 3.018\n",
            "Ep 5 (Step 041400): Train loss 2.690, Val loss 2.967\n",
            "Ep 5 (Step 041425): Train loss 2.787, Val loss 3.003\n",
            "Ep 5 (Step 041450): Train loss 2.612, Val loss 2.981\n",
            "Ep 5 (Step 041475): Train loss 2.662, Val loss 2.967\n",
            "Ep 5 (Step 041500): Train loss 2.656, Val loss 3.009\n",
            "Ep 5 (Step 041525): Train loss 2.872, Val loss 2.993\n",
            "Ep 5 (Step 041550): Train loss 2.659, Val loss 2.991\n",
            "Ep 5 (Step 041575): Train loss 2.726, Val loss 3.032\n",
            "Ep 5 (Step 041600): Train loss 2.657, Val loss 3.000\n",
            "Ep 5 (Step 041625): Train loss 2.642, Val loss 2.967\n",
            "Ep 5 (Step 041650): Train loss 2.735, Val loss 2.957\n",
            "Ep 5 (Step 041675): Train loss 2.771, Val loss 2.993\n",
            "Ep 5 (Step 041700): Train loss 2.706, Val loss 2.987\n",
            "Ep 5 (Step 041725): Train loss 2.804, Val loss 3.010\n",
            "Ep 5 (Step 041750): Train loss 2.751, Val loss 3.046\n",
            "Ep 5 (Step 041775): Train loss 2.644, Val loss 3.055\n",
            "Ep 5 (Step 041800): Train loss 2.640, Val loss 3.069\n",
            "Ep 5 (Step 041825): Train loss 2.475, Val loss 3.065\n",
            "Ep 5 (Step 041850): Train loss 2.660, Val loss 3.013\n",
            "Ep 5 (Step 041875): Train loss 2.725, Val loss 3.024\n",
            "Ep 5 (Step 041900): Train loss 2.710, Val loss 2.979\n",
            "Ep 5 (Step 041925): Train loss 2.823, Val loss 3.021\n",
            "Ep 5 (Step 041950): Train loss 2.755, Val loss 2.968\n",
            "Ep 5 (Step 041975): Train loss 2.641, Val loss 2.994\n",
            "Ep 5 (Step 042000): Train loss 2.935, Val loss 3.011\n",
            "Ep 5 (Step 042025): Train loss 2.602, Val loss 2.975\n",
            "Ep 5 (Step 042050): Train loss 2.516, Val loss 2.988\n",
            "Ep 5 (Step 042075): Train loss 2.753, Val loss 2.996\n",
            "Ep 5 (Step 042100): Train loss 2.637, Val loss 2.994\n",
            "Ep 5 (Step 042125): Train loss 2.813, Val loss 2.948\n",
            "Ep 5 (Step 042150): Train loss 2.699, Val loss 2.962\n",
            "Ep 5 (Step 042175): Train loss 2.477, Val loss 2.968\n",
            "Ep 5 (Step 042200): Train loss 2.718, Val loss 2.973\n",
            "Ep 5 (Step 042225): Train loss 2.712, Val loss 2.984\n",
            "Ep 5 (Step 042250): Train loss 2.271, Val loss 2.978\n",
            "Ep 5 (Step 042275): Train loss 2.666, Val loss 3.025\n",
            "Ep 5 (Step 042300): Train loss 2.757, Val loss 2.976\n",
            "Ep 5 (Step 042325): Train loss 2.595, Val loss 2.979\n",
            "Ep 5 (Step 042350): Train loss 2.772, Val loss 2.962\n",
            "Ep 5 (Step 042375): Train loss 2.675, Val loss 3.004\n",
            "Ep 5 (Step 042400): Train loss 2.761, Val loss 3.020\n",
            "Ep 5 (Step 042425): Train loss 2.688, Val loss 2.996\n",
            "Ep 5 (Step 042450): Train loss 2.897, Val loss 2.968\n",
            "Ep 5 (Step 042475): Train loss 2.524, Val loss 2.975\n",
            "Ep 5 (Step 042500): Train loss 2.770, Val loss 3.008\n",
            "Ep 5 (Step 042525): Train loss 2.542, Val loss 2.986\n",
            "Ep 5 (Step 042550): Train loss 2.428, Val loss 3.019\n",
            "Ep 5 (Step 042575): Train loss 2.690, Val loss 2.983\n",
            "Ep 5 (Step 042600): Train loss 2.777, Val loss 2.986\n",
            "Ep 5 (Step 042625): Train loss 2.471, Val loss 3.004\n",
            "Ep 5 (Step 042650): Train loss 2.665, Val loss 2.964\n",
            "Ep 5 (Step 042675): Train loss 2.690, Val loss 3.005\n",
            "Ep 5 (Step 042700): Train loss 2.904, Val loss 2.977\n",
            "Ep 5 (Step 042725): Train loss 2.597, Val loss 2.961\n",
            "Ep 5 (Step 042750): Train loss 2.758, Val loss 2.976\n",
            "Ep 5 (Step 042775): Train loss 2.625, Val loss 3.000\n",
            "Ep 5 (Step 042800): Train loss 2.590, Val loss 2.954\n",
            "Ep 5 (Step 042825): Train loss 2.735, Val loss 2.943\n",
            "Ep 5 (Step 042850): Train loss 2.745, Val loss 2.933\n",
            "Ep 5 (Step 042875): Train loss 2.448, Val loss 2.931\n",
            "Ep 5 (Step 042900): Train loss 2.541, Val loss 2.947\n",
            "Ep 5 (Step 042925): Train loss 2.668, Val loss 2.940\n",
            "Ep 5 (Step 042950): Train loss 2.687, Val loss 2.968\n",
            "Ep 5 (Step 042975): Train loss 2.527, Val loss 2.964\n",
            "Ep 5 (Step 043000): Train loss 2.490, Val loss 2.920\n",
            "Ep 5 (Step 043025): Train loss 2.556, Val loss 2.914\n",
            "Ep 5 (Step 043050): Train loss 2.640, Val loss 2.953\n",
            "Ep 5 (Step 043075): Train loss 2.834, Val loss 2.945\n",
            "Ep 5 (Step 043100): Train loss 2.570, Val loss 2.951\n",
            "Ep 5 (Step 043125): Train loss 2.634, Val loss 2.942\n",
            "Ep 5 (Step 043150): Train loss 2.766, Val loss 2.980\n",
            "Ep 5 (Step 043175): Train loss 2.763, Val loss 2.956\n",
            "Ep 5 (Step 043200): Train loss 2.688, Val loss 2.943\n",
            "Ep 5 (Step 043225): Train loss 2.499, Val loss 2.956\n",
            "Ep 5 (Step 043250): Train loss 2.690, Val loss 2.956\n",
            "Ep 5 (Step 043275): Train loss 2.645, Val loss 2.943\n",
            "Ep 5 (Step 043300): Train loss 2.594, Val loss 2.932\n",
            "Ep 5 (Step 043325): Train loss 2.796, Val loss 2.955\n",
            "Ep 5 (Step 043350): Train loss 2.806, Val loss 2.963\n",
            "Ep 5 (Step 043375): Train loss 2.698, Val loss 2.969\n",
            "Ep 5 (Step 043400): Train loss 2.688, Val loss 2.992\n",
            "Ep 5 (Step 043425): Train loss 2.666, Val loss 3.004\n",
            "Ep 5 (Step 043450): Train loss 2.624, Val loss 2.994\n",
            "Ep 5 (Step 043475): Train loss 2.528, Val loss 2.985\n",
            "Ep 5 (Step 043500): Train loss 2.806, Val loss 3.006\n",
            "Ep 5 (Step 043525): Train loss 2.767, Val loss 2.959\n",
            "Ep 5 (Step 043550): Train loss 2.425, Val loss 2.964\n",
            "Ep 5 (Step 043575): Train loss 2.602, Val loss 2.971\n",
            "Ep 5 (Step 043600): Train loss 2.674, Val loss 2.973\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "model = GPTModel(GPT_CONFIG)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 4\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"ဘာ\",\n",
        ")\n",
        "\n",
        "# Note:\n",
        "# Uncomment the following code to show the execution time\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d941400-2491-4aab-9592-38f4ec8b4d44",
      "metadata": {
        "id": "0d941400-2491-4aab-9592-38f4ec8b4d44"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d698b3b6-0215-4629-9794-ecfc4db45faf",
      "metadata": {
        "id": "d698b3b6-0215-4629-9794-ecfc4db45faf",
        "outputId": "6c35809e-4722-44aa-c331-99c298358634",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# Save the model's state_dict\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Model saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db9c97c-3ad7-40cb-b874-fff26ae5d182",
      "metadata": {
        "id": "4db9c97c-3ad7-40cb-b874-fff26ae5d182",
        "outputId": "e4fb131c-f2b1-4674-ab4d-857aee717c85"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'YourModel' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize your model (must match the saved model architecture)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYourModel\u001b[49m()  \u001b[38;5;66;03m# Replace with your actual model class\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load state_dict into model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "\u001b[1;31mNameError\u001b[0m: name 'YourModel' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize your model (must match the saved model architecture)\n",
        "model = YourModel()  # Replace with your actual model class\n",
        "\n",
        "# Load state_dict into model\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# Set model to evaluation mode (important for inference)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7161694a-3b9a-4f20-bbac-4790594e3cb4",
      "metadata": {
        "id": "7161694a-3b9a-4f20-bbac-4790594e3cb4",
        "outputId": "26fcc0c8-388f-4d93-8c86-9faa70e68fc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " ဟုတ်သည် ထိုစကား တစ်ရပ် ပြောနေဟန် ရှိ၏ ။သူသည် ရုတ်ချည်းပင်<UNK><UNK><UNK>ငါကလို\n"
          ]
        }
      ],
      "source": [
        "# Generate token IDs\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"ဟုတ်\").to(device),  # Move idx to the correct device\n",
        "    max_new_tokens=30,\n",
        "    context_size=GPT_CONFIG[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() #A\n",
        "#model = GPTModel(GPT_CONFIG)\n",
        "out = generate_text_simple(\n",
        "model=model,\n",
        "idx=encoded_tensor,\n",
        "max_new_tokens=6,\n",
        "context_size=GPT_CONFIG[\"context_length\"]\n",
        ")\n",
        "print(\"Output:\", out)\n",
        "print(\"Output length:\", len(out[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Pp0L2GV_G6S-",
        "outputId": "84665a84-dfca-4337-800f-dcf1937e7922"
      },
      "id": "Pp0L2GV_G6S-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'encoded_tensor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-a747037f85a2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m out = generate_text_simple(\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGPT_CONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoded_tensor' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}